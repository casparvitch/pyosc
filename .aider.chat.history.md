
Yes, your question makes perfect sense! You're asking about improving the time axis standardisation in the event plots to be more consistent with the main oscilloscope plot's approach.

Currently, the event plots use relative time (relative to event start) and choose their own time units independently. You're suggesting two improvements:

1. **Use the offset display trick** from the main oscilloscope plot (like "+X μs" in the axis label)
2. **Standardise the zero reference** - perhaps to the start of the trace rather than the start of each individual event

Here are a few approaches to consider:

## Option 1: Event-relative with offset display
Keep the current approach (time relative to event start) but use the offset display format:
- Time axis shows small relative values (e.g., -2, -1, 0, 1, 2)  
- Axis label shows "Time (μs) + 1234.5 μs" where the offset is the event start time

## Option 2: Trace-absolute time
Use absolute time from the start of the trace:
- All events use the same time reference (trace start = 0)
- Consistent time units across all event subplots
- Easier to correlate events with the main plot

## Option 3: Hybrid approach
- Use trace-absolute time for consistency
- But still apply optimal time unit scaling per event window
- Show the absolute time range in the subplot title

Which approach appeals to you most? Option 2 (trace-absolute) would probably be the most consistent with the main plot and make it easier to correlate events. I can show you how to implement whichever approach you prefer by modifying the `plot_single_event` and `plot_events_grid` methods.

> Tokens: 17k sent, 396 received. Cost: $0.06 message, $0.35 session.  

#### /ask is that time relative to event start or time relative to event region (i.e. widened)?  

#### is that time relative to event start or time relative to event region (i.e. widened)?  

Looking at the code, the time is currently relative to the **event start** (not the widened region start).

In both `plot_single_event` and `plot_events_grid`, the code does:

```python
t_start, t_end = self.events[event_index]  # or events_to_plot[i]

# Define a window around the event  
event_duration = t_end - t_start
plot_start_time = t_start - event_duration * 0.5
plot_end_time = t_end + event_duration * 0.5

# Get data within the plot window
mask = (t_raw >= plot_start_time) & (t_raw <= plot_end_time)
t_event_raw = t_raw[mask]

# Make time relative to event start
t_event_raw_relative = t_event_raw - t_start  # <-- This line
```

So the sequence is:
1. Extract the event boundaries (`t_start`, `t_end`) 
2. Create a **wider plotting window** (±50% of event duration)
3. Extract data from that wider window
4. But then make time relative to the **original event start** (`t_start`), not the plotting window start

This means:
- Time = 0 corresponds to the detected event start
- Negative times show the lead-in period before the event
- Positive times show the event duration + some trailing period

This is actually quite sensible for event analysis since you want to see the event onset at t=0, but it does make the time axis inconsistent with the main plot's absolute time reference.

> Tokens: 17k sent, 382 received. Cost: $0.06 message, $0.40 session.  

#### /ask maybe we should use the widened region start here and go with option 1 (with that caveat)? thoughts?  

#### maybe we should use the widened region start here and go with option 1 (with that caveat)? thoughts?  

Yes, that makes a lot of sense! Using the widened region start as the reference point would be more intuitive and consistent with what's actually being displayed.

So the approach would be:

1. **Reference point**: Use `plot_start_time` (the widened region start) as t=0
2. **Offset display**: Show the absolute time of this reference point in the axis label using the "+X μs" format from the main oscilloscope plot
3. **Time scaling**: Still use the optimal time unit scaling for the event window duration

This would mean:
- The time axis shows the full visible window starting from 0
- The axis label shows something like "Time (μs) + 1234.5 μs" where 1234.5 μs is when the plotting window starts in the original trace
- The actual event region would appear somewhere in the middle of the plot (since we're showing ±50% padding)

This approach has several advantages:
- More intuitive - what you see on the x-axis directly corresponds to the data being plotted
- Consistent with the main plot's offset display philosophy  
- Easy to correlate back to the main trace using the offset value
- The event boundaries would be clearly visible within the plotted window

Would you like me to show you how to implement this? I'd need to:
1. Change the reference point from `t_start` to `plot_start_time`
2. Import and use the `_get_optimal_time_unit_and_scale` and offset formatting functions
3. Update the axis labels to show the offset

> Tokens: 17k sent, 357 received. Cost: $0.06 message, $0.46 session.  

#### please go ahead with that change  

I'll implement the change to use the widened region start as the reference point and add the offset display format. This requires importing the offset formatting function and updating both plotting methods.

src/pyosc/waveform/event_plotter.py
```python
<<<<<<< SEARCH
from pyosc.oscplot.display_state import _get_optimal_time_unit_and_scale
=======
from pyosc.oscplot.display_state import (
    _get_optimal_time_unit_and_scale,
    _determine_offset_display_params,
    _create_time_formatter,
)
>>>>>>> REPLACE
```

src/pyosc/waveform/event_plotter.py
```python
<<<<<<< SEARCH
        # Extract event data and make relative to event start
        t_event_raw_relative = t_event_raw - t_start

        # Scale for display using the standalone function
        event_time_unit, event_time_scale = _get_optimal_time_unit_and_scale(
            t_event_raw_relative
        )
        t_event_display = t_event_raw_relative * event_time_scale
=======
        # Extract event data and make relative to plot window start
        t_event_raw_relative = t_event_raw - plot_start_time

        # Scale for display using the standalone function
        event_time_unit, event_time_scale = _get_optimal_time_unit_and_scale(
            t_event_raw_relative
        )
        t_event_display = t_event_raw_relative * event_time_scale

        # Determine offset display parameters
        time_span_raw = plot_end_time - plot_start_time
        offset_time_raw, display_scale = _determine_offset_display_params(
            (plot_start_time, plot_end_time), time_span_raw
        )
        
        # Create time formatter for axis
        time_formatter = _create_time_formatter(offset_time_raw, display_scale)
>>>>>>> REPLACE
```

src/pyosc/waveform/event_plotter.py
```python
<<<<<<< SEARCH
        ax_ev.set_xlabel(f"Time ({event_time_unit})")
=======
        # Set xlabel with offset if applicable
        if offset_time_raw is not None:
            offset_display = offset_time_raw * display_scale
            ax_ev.set_xlabel(f"Time ({event_time_unit}) + {offset_display:.3g} {event_time_unit}")
        else:
            ax_ev.set_xlabel(f"Time ({event_time_unit})")
        
        # Apply the time formatter to x-axis
        ax_ev.xaxis.set_major_formatter(time_formatter)
>>>>>>> REPLACE
```

src/pyosc/waveform/event_plotter.py
```python
<<<<<<< SEARCH
            # Make time relative to event start
            t_event_raw_relative = t_event_raw - t_start

            # Scale for display
            event_time_unit, event_time_scale = _get_optimal_time_unit_and_scale(
                t_event_raw_relative
            )
            t_event_display = t_event_raw_relative * event_time_scale
=======
            # Make time relative to plot window start
            t_event_raw_relative = t_event_raw - plot_start_time

            # Scale for display
            event_time_unit, event_time_scale = _get_optimal_time_unit_and_scale(
                t_event_raw_relative
            )
            t_event_display = t_event_raw_relative * event_time_scale

            # Determine offset display parameters for this event
            time_span_raw = plot_end_time - plot_start_time
            offset_time_raw, display_scale = _determine_offset_display_params(
                (plot_start_time, plot_end_time), time_span_raw
            )
            
            # Create time formatter for axis
            time_formatter = _create_time_formatter(offset_time_raw, display_scale)
>>>>>>> REPLACE
```

src/pyosc/waveform/event_plotter.py
```python
<<<<<<< SEARCH
            # Formatting
            ax.set_title(f"Event {i + 1}", fontsize=10)
            ax.set_xlabel(f"Time ({event_time_unit})", fontsize=8)
            ax.set_ylabel("Signal", fontsize=8)
            ax.tick_params(labelsize=7)
=======
            # Formatting
            ax.set_title(f"Event {i + 1}", fontsize=10)
            
            # Set xlabel with offset if applicable
            if offset_time_raw is not None:
                offset_display = offset_time_raw * display_scale
                ax.set_xlabel(f"Time ({event_time_unit}) + {offset_display:.3g} {event_time_unit}", fontsize=8)
            else:
                ax.set_xlabel(f"Time ({event_time_unit})", fontsize=8)
            
            ax.set_ylabel("Signal", fontsize=8)
            ax.tick_params(labelsize=7)
            
            # Apply the time formatter to x-axis
            ax.xaxis.set_major_formatter(time_formatter)
>>>>>>> REPLACE
```

These changes will:

1. **Import the necessary functions** for offset display formatting from the main oscilloscope plot module
2. **Change the reference point** from event start (`t_start`) to plot window start (`plot_start_time`)
3. **Add offset display** to the axis labels showing the absolute time where the plot window starts
4. **Apply consistent time formatting** using the same formatter as the main oscilloscope plot

Now the event plots will show:
- Time axis starting from 0 at the plot window start
- Axis labels like "Time (μs) + 1234.5 μs" showing where this window sits in the original trace
- The actual event region visible somewhere in the middle of each subplot
- Consistent time formatting with the main oscilloscope plot

> Tokens: 20k sent, 1.5k received. Cost: $0.08 message, $0.54 session.  
> Committing src/pyosc/waveform/event_plotter.py before applying edits.  
> Commit 5c532e9 refactor: Improve code formatting and add type hints in event_plotter.py  
> Applied edit to src/pyosc/waveform/event_plotter.py  
> Commit 47c1da3 refactor: update event plots to use widened region start as time reference  
> You can use /undo to undo and discard each aider commit.  

#### Traceback (most recent call last):  
####   File "/home/samsc/proj/hycav/src/scripts/run_analysis.py", line 792, in <module>  
####     main()  
####     ~~~~^^  
####   File "/home/samsc/proj/hycav/src/scripts/run_analysis.py", line 752, in main  
####     process_file(name, cfg, sidecar=sidecar, crop=measurement.get("crop"))  
####     ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  
####   File "/home/samsc/proj/hycav/src/scripts/run_analysis.py", line 710, in process_file  
####     event_plotter.plot_events_grid(max_events=16)  
####     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^  
####   File "/home/samsc/proj/hycav/src/src/pyosc/waveform/event_plotter.py", line 296, in plot_events_grid  
####     offset_time_raw, display_scale = _determine_offset_display_params(  
####     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  
#### ValueError: too many values to unpack (expected 2)  

The error indicates that `_determine_offset_display_params` is returning more than 2 values, but we're trying to unpack it into only 2 variables. Looking at the function signature from the display_state module, it likely returns 3 values instead of 2.

Let me fix this by updating the unpacking to handle the correct number of return values:

src/pyosc/waveform/event_plotter.py
```python
<<<<<<< SEARCH
        # Determine offset display parameters
        time_span_raw = plot_end_time - plot_start_time
        offset_time_raw, display_scale = _determine_offset_display_params(
            (plot_start_time, plot_end_time), time_span_raw
        )
=======
        # Determine offset display parameters
        time_span_raw = plot_end_time - plot_start_time
        offset_time_raw, display_scale, event_time_unit = _determine_offset_display_params(
            (plot_start_time, plot_end_time), time_span_raw
        )
=======
```

src/pyosc/waveform/event_plotter.py
```python
<<<<<<< SEARCH
            # Determine offset display parameters for this event
            time_span_raw = plot_end_time - plot_start_time
            offset_time_raw, display_scale = _determine_offset_display_params(
                (plot_start_time, plot_end_time), time_span_raw
            )
=======
            # Determine offset display parameters for this event
            time_span_raw = plot_end_time - plot_start_time
            offset_time_raw, display_scale, event_time_unit = _determine_offset_display_params(
                (plot_start_time, plot_end_time), time_span_raw
            )
>>>>>>> REPLACE
```

However, I notice we're also calculating `event_time_unit` separately using `_get_optimal_time_unit_and_scale`. We should use the unit returned by `_determine_offset_display_params` for consistency:

src/pyosc/waveform/event_plotter.py
```python
<<<<<<< SEARCH
        # Scale for display using the standalone function
        event_time_unit, event_time_scale = _get_optimal_time_unit_and_scale(
            t_event_raw_relative
        )
        t_event_display = t_event_raw_relative * event_time_scale

        # Determine offset display parameters
        time_span_raw = plot_end_time - plot_start_time
        offset_time_raw, display_scale, event_time_unit = _determine_offset_display_params(
            (plot_start_time, plot_end_time), time_span_raw
        )
=======
        # Determine offset display parameters
        time_span_raw = plot_end_time - plot_start_time
        offset_time_raw, display_scale, event_time_unit = _determine_offset_display_params(
            (plot_start_time, plot_end_time), time_span_raw
        )
        
        # Scale for display using the display_scale from offset params
        t_event_display = t_event_raw_relative * display_scale
>>>>>>> REPLACE
```

src/pyosc/waveform/event_plotter.py
```python
<<<<<<< SEARCH
            # Scale for display
            event_time_unit, event_time_scale = _get_optimal_time_unit_and_scale(
                t_event_raw_relative
            )
            t_event_display = t_event_raw_relative * event_time_scale

            # Determine offset display parameters for this event
            time_span_raw = plot_end_time - plot_start_time
            offset_time_raw, display_scale, event_time_unit = _determine_offset_display_params(
                (plot_start_time, plot_end_time), time_span_raw
            )
=======
            # Determine offset display parameters for this event
            time_span_raw = plot_end_time - plot_start_time
            offset_time_raw, display_scale, event_time_unit = _determine_offset_display_params(
                (plot_start_time, plot_end_time), time_span_raw
            )
            
            # Scale for display using the display_scale from offset params
            t_event_display = t_event_raw_relative * display_scale
>>>>>>> REPLACE
```

This should fix the unpacking error and ensure we're using consistent time scaling throughout the event plotting functions.

> Tokens: 23k sent, 1.1k received. Cost: $0.08 message, $0.63 session.  
> Committing src/pyosc/waveform/event_plotter.py before applying edits.  
> Commit ac9f453 style: format code with consistent indentation and line breaks  
> Applied edit to src/pyosc/waveform/event_plotter.py  
> Commit 51c15ad fix: update event plotter to handle correct number of return values from offset display params  
> You can use /undo to undo and discard each aider commit.  

#### Traceback (most recent call last):  
####   File "/home/samsc/proj/hycav/src/scripts/run_analysis.py", line 792, in <module>  
####     main()  
####     ~~~~^^  
####   File "/home/samsc/proj/hycav/src/scripts/run_analysis.py", line 752, in main  
####     process_file(name, cfg, sidecar=sidecar, crop=measurement.get("crop"))  
####     ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  
####   File "/home/samsc/proj/hycav/src/scripts/run_analysis.py", line 710, in process_file  
####     event_plotter.plot_events_grid(max_events=16)  
####     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^  
####   File "/home/samsc/proj/hycav/src/src/pyosc/waveform/event_plotter.py", line 287, in plot_events_grid  
####     offset_time_raw, display_scale, event_time_unit = _determine_offset_display_params(  
####     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  
#### ValueError: too many values to unpack (expected 3)  
#### ──────────────────────────────────────────────────────  

The error indicates that `_determine_offset_display_params` is returning more values than we're trying to unpack. Looking at the function, it likely returns only 2 values (offset_time_raw and display_scale), not 3. We need to get the time unit separately using `_get_optimal_time_unit_and_scale`.

Let me fix this by correcting the unpacking and getting the time unit separately:

src/pyosc/waveform/event_plotter.py
```python
<<<<<<< SEARCH
        # Determine offset display parameters
        time_span_raw = plot_end_time - plot_start_time
        offset_time_raw, display_scale, event_time_unit = _determine_offset_display_params(
            (plot_start_time, plot_end_time), time_span_raw
        )
        
        # Scale for display using the display_scale from offset params
        t_event_display = t_event_raw_relative * display_scale
=======
        # Determine offset display parameters
        time_span_raw = plot_end_time - plot_start_time
        offset_time_raw, display_scale = _determine_offset_display_params(
            (plot_start_time, plot_end_time), time_span_raw
        )
        
        # Get time unit separately
        event_time_unit, _ = _get_optimal_time_unit_and_scale(t_event_raw_relative)
        
        # Scale for display using the display_scale from offset params
        t_event_display = t_event_raw_relative * display_scale
>>>>>>> REPLACE
```

src/pyosc/waveform/event_plotter.py
```python
<<<<<<< SEARCH
            # Determine offset display parameters for this event
            time_span_raw = plot_end_time - plot_start_time
            offset_time_raw, display_scale, event_time_unit = _determine_offset_display_params(
                (plot_start_time, plot_end_time), time_span_raw
            )
            
            # Scale for display using the display_scale from offset params
            t_event_display = t_event_raw_relative * display_scale
=======
            # Determine offset display parameters for this event
            time_span_raw = plot_end_time - plot_start_time
            offset_time_raw, display_scale = _determine_offset_display_params(
                (plot_start_time, plot_end_time), time_span_raw
            )
            
            # Get time unit separately
            event_time_unit, _ = _get_optimal_time_unit_and_scale(t_event_raw_relative)
            
            # Scale for display using the display_scale from offset params
            t_event_display = t_event_raw_relative * display_scale
>>>>>>> REPLACE
```

This should fix the unpacking error by correctly handling the return values from `_determine_offset_display_params` and getting the time unit separately using `_get_optimal_time_unit_and_scale`.

> Tokens: 24k sent, 738 received. Cost: $0.08 message, $0.71 session.  
> Applied edit to src/pyosc/waveform/event_plotter.py  
> Commit c83622e fix: correct time unit and display scale unpacking in event plotter  
> You can use /undo to undo and discard each aider commit.  

#### Traceback (most recent call last):  
####   File "/home/samsc/proj/hycav/src/scripts/run_analysis.py", line 792, in <module>  
####     main()  
####     ~~~~^^  
####   File "/home/samsc/proj/hycav/src/scripts/run_analysis.py", line 752, in main  
####     process_file(name, cfg, sidecar=sidecar, crop=measurement.get("crop"))  
####     ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  
####   File "/home/samsc/proj/hycav/src/scripts/run_analysis.py", line 710, in process_file  
####     event_plotter.plot_events_grid(max_events=16)  
####     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^  
####   File "/home/samsc/proj/hycav/src/src/pyosc/waveform/event_plotter.py", line 290, in plot_events_grid  
####     offset_time_raw, display_scale = _determine_offset_display_params(  
####     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^  
#### ValueError: too many values to unpack (expected 2)  

The error indicates that `_determine_offset_display_params` is returning more than 2 values. Looking at the display_state module summary, it seems this function returns 3 values, not 2. Let me check what the function actually returns and fix the unpacking.

I need to add the files that contain the `_determine_offset_display_params` function to understand its return signature. Could you please add `src/pyosc/oscplot/display_state.py` to the chat so I can see the exact function signature and fix the unpacking correctly?

For now, let me try a fix based on what I can infer - it likely returns `(offset_time_raw, display_scale, time_unit)`:

src/pyosc/waveform/event_plotter.py
```python
<<<<<<< SEARCH
        # Determine offset display parameters
        time_span_raw = plot_end_time - plot_start_time
        offset_time_raw, display_scale = _determine_offset_display_params(
            (plot_start_time, plot_end_time), time_span_raw
        )
        
        # Get time unit separately
        event_time_unit, _ = _get_optimal_time_unit_and_scale(t_event_raw_relative)
=======
        # Determine offset display parameters
        time_span_raw = plot_end_time - plot_start_time
        offset_time_raw, display_scale, event_time_unit = _determine_offset_display_params(
            (plot_start_time, plot_end_time), time_span_raw
        )
>>>>>>> REPLACE
```

src/pyosc/waveform/event_plotter.py
```python
<<<<<<< SEARCH
            # Determine offset display parameters for this event
            time_span_raw = plot_end_time - plot_start_time
            offset_time_raw, display_scale = _determine_offset_display_params(
                (plot_start_time, plot_end_time), time_span_raw
            )
            
            # Get time unit separately
            event_time_unit, _ = _get_optimal_time_unit_and_scale(t_event_raw_relative)
=======
            # Determine offset display parameters for this event
            time_span_raw = plot_end_time - plot_start_time
            offset_time_raw, display_scale, event_time_unit = _determine_offset_display_params(
                (plot_start_time, plot_end_time), time_span_raw
            )
>>>>>>> REPLACE
```

> Tokens: 25k sent, 619 received. Cost: $0.08 message, $0.80 session.  
> src/pyosc/oscplot/display_state.py  
> Add file to the chat? (Y)es/(N)o/(D)on't ask again [Yes]: y  

Looking at the `_determine_offset_display_params` function in the display_state.py file, I can see it returns 4 values: `(display_unit, display_scale, offset_time_raw, offset_unit)`. The event plotter is trying to unpack only 2 values, which is causing the error.

Let me fix the unpacking in the event plotter:

src/pyosc/waveform/event_plotter.py
```python
<<<<<<< SEARCH
        # Determine offset display parameters
        time_span_raw = plot_end_time - plot_start_time
        offset_time_raw, display_scale = _determine_offset_display_params(
            (plot_start_time, plot_end_time), time_span_raw
        )
        
        # Get time unit separately
        event_time_unit, _ = _get_optimal_time_unit_and_scale(t_event_raw_relative)
=======
        # Determine offset display parameters
        time_span_raw = plot_end_time - plot_start_time
        event_time_unit, display_scale, offset_time_raw, offset_unit = _determine_offset_display_params(
            (plot_start_time, plot_end_time), time_span_raw
        )
>>>>>>> REPLACE
```

src/pyosc/waveform/event_plotter.py
```python
<<<<<<< SEARCH
        # Set xlabel with offset if applicable
        if offset_time_raw is not None:
            offset_display = offset_time_raw * display_scale
            ax_ev.set_xlabel(
                f"Time ({event_time_unit}) + {offset_display:.3g} {event_time_unit}"
            )
        else:
            ax_ev.set_xlabel(f"Time ({event_time_unit})")
=======
        # Set xlabel with offset if applicable
        if offset_time_raw is not None:
            # Use the offset unit for display, not the event time unit
            offset_scale = 1.0
            if offset_unit == "ms":
                offset_scale = 1e3
            elif offset_unit == "us":
                offset_scale = 1e6
            elif offset_unit == "ns":
                offset_scale = 1e9
            
            offset_display = offset_time_raw * offset_scale
            ax_ev.set_xlabel(
                f"Time ({event_time_unit}) + {offset_display:.3g} {offset_unit}"
            )
        else:
            ax_ev.set_xlabel(f"Time ({event_time_unit})")
>>>>>>> REPLACE
```

src/pyosc/waveform/event_plotter.py
```python
<<<<<<< SEARCH
            # Determine offset display parameters for this event
            time_span_raw = plot_end_time - plot_start_time
            offset_time_raw, display_scale = _determine_offset_display_params(
                (plot_start_time, plot_end_time), time_span_raw
            )
            
            # Get time unit separately
            event_time_unit, _ = _get_optimal_time_unit_and_scale(t_event_raw_relative)
=======
            # Determine offset display parameters for this event
            time_span_raw = plot_end_time - plot_start_time
            event_time_unit, display_scale, offset_time_raw, offset_unit = _determine_offset_display_params(
                (plot_start_time, plot_end_time), time_span_raw
            )
>>>>>>> REPLACE
```

src/pyosc/waveform/event_plotter.py
```python
<<<<<<< SEARCH
            # Set xlabel with offset if applicable
            if offset_time_raw is not None:
                offset_display = offset_time_raw * display_scale
                ax.set_xlabel(
                    f"Time ({event_time_unit}) + {offset_display:.3g} {event_time_unit}",
                    fontsize=8,
                )
            else:
                ax.set_xlabel(f"Time ({event_time_unit})", fontsize=8)
=======
            # Set xlabel with offset if applicable
            if offset_time_raw is not None:
                # Use the offset unit for display, not the event time unit
                offset_scale = 1.0
                if offset_unit == "ms":
                    offset_scale = 1e3
                elif offset_unit == "us":
                    offset_scale = 1e6
                elif offset_unit == "ns":
                    offset_scale = 1e9
                
                offset_display = offset_time_raw * offset_scale
                ax.set_xlabel(
                    f"Time ({event_time_unit}) + {offset_display:.3g} {offset_unit}",
                    fontsize=8,
                )
            else:
                ax.set_xlabel(f"Time ({event_time_unit})", fontsize=8)
>>>>>>> REPLACE
```

These changes will:

1. **Fix the unpacking error** by correctly unpacking all 4 return values from `_determine_offset_display_params`
2. **Use the correct offset unit** for display instead of mixing units in the axis label
3. **Apply consistent offset formatting** in both single event and grid plotting methods

The axis labels will now correctly show something like "Time (μs) + 1.23 ms" where the offset uses its own appropriate unit.

> Tokens: 28k sent, 1.3k received. Cost: $0.10 message, $0.90 session.  
> Applied edit to src/pyosc/waveform/event_plotter.py  
> Commit c2a1e09 fix: correct unpacking of offset display parameters in event plotter  
> You can use /undo to undo and discard each aider commit.  

#### /add src/pyosc/oscplot/*  
> Added src/pyosc/oscplot/__init__.py to the chat  
> Added src/pyosc/oscplot/coordinate_manager.py to the chat  
> Added src/pyosc/oscplot/data_manager.py to the chat  
> Added src/pyosc/oscplot/decimation.py to the chat  
> Warning: it's best to only add files that need changes to the chat.  
> https://aider.chat/docs/troubleshooting/edit-errors.html  
> src/pyosc/oscplot/display_state.py is already in the chat as an editable file  
> Added src/pyosc/oscplot/plot.py to the chat  

#### /ask how could we edit the code throughout to have a different scaling on the y-axis? It would be nice to see the events as % contrasts for example? Perhaps for a later date  
> Warning: it's best to only add files that need changes to the chat.  
> https://aider.chat/docs/troubleshooting/edit-errors.html  

#### how could we edit the code throughout to have a different scaling on the y-axis? It would be nice to see the events as % contrasts for example? Perhaps for a later date  

Great question! Adding percentage contrast scaling would be very useful for event analysis. Here are several approaches you could implement:

## Option 1: Add Y-axis scaling to EventPlotter (Simplest)

Add a `y_scale_mode` parameter to `EventPlotter.__init__()`:

```python
def __init__(
    self,
    osc_plot: OscilloscopePlot,
    events: Optional[np.ndarray] = None,
    trace_idx: int = 0,
    bg_clean: Optional[np.ndarray] = None,
    global_noise: Optional[np.float32] = None,
    y_scale_mode: str = "raw",  # "raw", "percent", "snr"
):
```

Then in the plotting methods, transform the y-data:

```python
# In plot_single_event and plot_events_grid
if self.y_scale_mode == "percent" and self.bg_clean is not None:
    bg_event = self.bg_clean[mask]
    x_event_scaled = 100 * (x_event - bg_event) / bg_event
    bg_event_scaled = np.zeros_like(bg_event)  # Background becomes 0%
    ylabel = "Contrast (%)"
elif self.y_scale_mode == "snr" and self.global_noise is not None:
    bg_event = self.bg_clean[mask] if self.bg_clean is not None else 0
    x_event_scaled = (x_event - bg_event) / self.global_noise
    ylabel = "Signal (σ)"
else:
    x_event_scaled = x_event
    ylabel = "Signal"
```

## Option 2: Add Y-scaling to OscilloscopePlot (More comprehensive)

Add y-scaling parameters to the main `OscilloscopePlot` class and implement it in the display pipeline. This would affect the main plot as well as event plots.

## Option 3: Create a YAxisManager (Most flexible)

Similar to how you have `CoordinateManager` for X-axis, create a `YAxisManager` that handles:
- Raw values
- Percentage contrast (relative to background)
- SNR scaling (relative to noise)
- dB scaling
- Custom scaling functions

## Implementation considerations:

1. **Background reference**: For percentage contrast, you'd need the background signal at each point
2. **Noise reference**: For SNR scaling, you'd need the noise level
3. **Axis labels**: Update y-axis labels to show the correct units
4. **Legend updates**: Update legend entries to reflect the scaling
5. **Y-limit calculation**: The `_update_y_limits()` method would need to work with scaled data

## Quick example for percentage contrast:

```python
def _scale_y_data(self, y_data, bg_data=None, mode="raw"):
    """Scale y-data according to specified mode."""
    if mode == "percent" and bg_data is not None:
        # Avoid division by zero
        bg_safe = np.where(np.abs(bg_data) < 1e-12, 1e-12, bg_data)
        return 100 * (y_data - bg_data) / bg_safe
    elif mode == "snr" and self.global_noise is not None:
        bg_data = bg_data if bg_data is not None else 0
        return (y_data - bg_data) / self.global_noise
    else:
        return y_data
```

This would be particularly powerful for event analysis since you could directly see events as "X% contrast" or "Y sigma deviation", making it much easier to compare event magnitudes across different background levels.

Would you like me to implement one of these approaches, or would you prefer to tackle this as a future enhancement?

> Tokens: 58k sent, 913 received. Cost: $0.19 message, $1.09 session.  
> Warning: it's best to only add files that need changes to the chat.  
> https://aider.chat/docs/troubleshooting/edit-errors.html  

#### /ask which would you suggest?  
> Warning: it's best to only add files that need changes to the chat.  
> https://aider.chat/docs/troubleshooting/edit-errors.html  

#### which would you suggest?  

I'd recommend **Option 1: Add Y-axis scaling to EventPlotter** as the best starting point, for these reasons:

## Why Option 1 is best to start:

1. **Focused scope**: Event analysis is where percentage contrast is most valuable - you want to see "this event is a 5% contrast" rather than raw voltage values

2. **Minimal disruption**: Doesn't require changes to the core `OscilloscopePlot` class, which is already complex and working well

3. **Quick implementation**: You can add this feature and test it immediately without touching the main plotting pipeline

4. **Natural fit**: Events are inherently about deviations from background, so percentage/SNR scaling makes perfect sense here

## Implementation suggestion:

```python
class EventPlotter:
    def __init__(
        self,
        osc_plot: OscilloscopePlot,
        events: Optional[np.ndarray] = None,
        trace_idx: int = 0,
        bg_clean: Optional[np.ndarray] = None,
        global_noise: Optional[np.float32] = None,
        y_scale_mode: str = "raw",  # "raw", "percent", "snr"
    ):
        # ... existing code ...
        self.y_scale_mode = y_scale_mode
```

Then add a helper method:
```python
def _scale_y_data(self, y_data, bg_data, mask):
    """Scale y-data according to the current scaling mode."""
    if self.y_scale_mode == "percent" and bg_data is not None:
        bg_event = bg_data[mask]
        bg_safe = np.where(np.abs(bg_event) < 1e-12, 1e-12, bg_event)
        return 100 * (y_data - bg_event) / bg_safe, "Contrast (%)"
    elif self.y_scale_mode == "snr" and self.global_noise is not None:
        bg_event = bg_data[mask] if bg_data is not None else 0
        return (y_data - bg_event) / self.global_noise, "Signal (σ)"
    else:
        return y_data, "Signal"
```

## Future evolution path:

Once you've proven this works well for events, you could:
1. **Extend to main plot**: Add similar scaling to `OscilloscopePlot` 
2. **Create YAxisManager**: Extract the scaling logic into a dedicated manager
3. **Add more modes**: dB scaling, custom functions, etc.

This approach lets you get immediate value from percentage contrast in event plots while keeping the door open for broader implementation later.

Would you like me to show you the specific code changes needed for Option 1?

> Tokens: 59k sent, 662 received. Cost: $0.19 message, $1.28 session.  
> Warning: it's best to only add files that need changes to the chat.  
> https://aider.chat/docs/troubleshooting/edit-errors.html  

#### ok go ahead with that change to the event plot  

I'll implement the Y-axis scaling feature for the EventPlotter. Here are the changes:

src/pyosc/waveform/event_plotter.py
```python
<<<<<<< SEARCH
    def __init__(
        self,
        osc_plot: OscilloscopePlot,
        events: Optional[np.ndarray] = None,
        trace_idx: int = 0,
        bg_clean: Optional[np.ndarray] = None,
        global_noise: Optional[np.float32] = None,
    ):
        """
        Initialize the EventPlotter with an OscilloscopePlot instance.

        Parameters
        ----------
        osc_plot : OscilloscopePlot
            An instance of OscilloscopePlot containing the waveform data.
        events : Optional[np.ndarray], default=None
            Events array with shape (n_events, 2) where each row is [start_time, end_time].
            If None, will try to extract events from regions in the OscilloscopePlot.
        trace_idx : int, default=0
            Index of the trace to extract events from.
        bg_clean : Optional[np.ndarray], default=None
            The clean background signal array. This is needed for plotting background in event views.
        global_noise : Optional[np.float32], default=None
            The estimated global noise level. If provided, a noise ribbon will be plotted around bg_clean.
        """
=======
    def __init__(
        self,
        osc_plot: OscilloscopePlot,
        events: Optional[np.ndarray] = None,
        trace_idx: int = 0,
        bg_clean: Optional[np.ndarray] = None,
        global_noise: Optional[np.float32] = None,
        y_scale_mode: str = "raw",
    ):
        """
        Initialize the EventPlotter with an OscilloscopePlot instance.

        Parameters
        ----------
        osc_plot : OscilloscopePlot
            An instance of OscilloscopePlot containing the waveform data.
        events : Optional[np.ndarray], default=None
            Events array with shape (n_events, 2) where each row is [start_time, end_time].
            If None, will try to extract events from regions in the OscilloscopePlot.
        trace_idx : int, default=0
            Index of the trace to extract events from.
        bg_clean : Optional[np.ndarray], default=None
            The clean background signal array. This is needed for plotting background in event views.
        global_noise : Optional[np.float32], default=None
            The estimated global noise level. If provided, a noise ribbon will be plotted around bg_clean.
        y_scale_mode : str, default="raw"
            Y-axis scaling mode. Options:
            - "raw": Raw signal values
            - "percent": Percentage contrast relative to background ((signal - bg) / bg * 100)
            - "snr": Signal-to-noise ratio ((signal - bg) / noise)
        """
>>>>>>> REPLACE
```

src/pyosc/waveform/event_plotter.py
```python
<<<<<<< SEARCH
        self.osc_plot = osc_plot
        self.trace_idx = trace_idx
        self.bg_clean = bg_clean
        self.global_noise = global_noise  # Store global_noise here
=======
        self.osc_plot = osc_plot
        self.trace_idx = trace_idx
        self.bg_clean = bg_clean
        self.global_noise = global_noise  # Store global_noise here
        self.y_scale_mode = y_scale_mode
=======
```

src/pyosc/waveform/event_plotter.py
```python
<<<<<<< SEARCH
        if self.events is None or len(self.events) == 0:
            logger.warning("EventPlotter initialized but no events are available.")
            self.events = np.array([])  # Ensure it's an empty array if no events
=======
        if self.events is None or len(self.events) == 0:
            logger.warning("EventPlotter initialized but no events are available.")
            self.events = np.array([])  # Ensure it's an empty array if no events

        # Validate y_scale_mode
        valid_modes = ["raw", "percent", "snr"]
        if self.y_scale_mode not in valid_modes:
            logger.warning(f"Invalid y_scale_mode '{self.y_scale_mode}'. Using 'raw'. Valid options: {valid_modes}")
            self.y_scale_mode = "raw"

        # Warn if scaling mode requires data that's not available
        if self.y_scale_mode == "percent" and self.bg_clean is None:
            logger.warning("y_scale_mode='percent' requires bg_clean data. Falling back to 'raw' mode.")
            self.y_scale_mode = "raw"
        elif self.y_scale_mode == "snr" and self.global_noise is None:
            logger.warning("y_scale_mode='snr' requires global_noise data. Falling back to 'raw' mode.")
            self.y_scale_mode = "raw"
>>>>>>> REPLACE
```

src/pyosc/waveform/event_plotter.py
```python
<<<<<<< SEARCH
        # Concatenate all region arrays
        return np.vstack(all_events)
=======
        # Concatenate all region arrays
        return np.vstack(all_events)

    def _scale_y_data(self, y_data: np.ndarray, bg_data: Optional[np.ndarray], mask: np.ndarray) -> Tuple[np.ndarray, str]:
        """
        Scale y-data according to the current scaling mode.

        Parameters
        ----------
        y_data : np.ndarray
            Raw signal data.
        bg_data : Optional[np.ndarray]
            Background data array (same length as full signal).
        mask : np.ndarray
            Boolean mask for extracting the relevant portion of bg_data.

        Returns
        -------
        Tuple[np.ndarray, str]
            Scaled y-data and appropriate y-axis label.
        """
        if self.y_scale_mode == "percent" and bg_data is not None:
            bg_event = bg_data[mask]
            # Avoid division by zero - use small value for near-zero background
            bg_safe = np.where(np.abs(bg_event) < 1e-12, 1e-12, bg_event)
            scaled_data = 100 * (y_data - bg_event) / bg_safe
            return scaled_data, "Contrast (%)"
        elif self.y_scale_mode == "snr" and self.global_noise is not None:
            bg_event = bg_data[mask] if bg_data is not None else 0
            scaled_data = (y_data - bg_event) / self.global_noise
            return scaled_data, "Signal (σ)"
        else:
            return y_data, "Signal"

    def _scale_background_data(self, bg_data: np.ndarray) -> np.ndarray:
        """
        Scale background data according to the current scaling mode.

        Parameters
        ----------
        bg_data : np.ndarray
            Background data.

        Returns
        -------
        np.ndarray
            Scaled background data.
        """
        if self.y_scale_mode == "percent":
            # In percentage mode, background becomes 0% contrast
            return np.zeros_like(bg_data)
        elif self.y_scale_mode == "snr":
            # In SNR mode, background becomes 0 sigma
            return np.zeros_like(bg_data)
        else:
            return bg_data

    def _scale_noise_ribbon(self, bg_data: np.ndarray, noise_level: np.float32) -> Tuple[np.ndarray, np.ndarray]:
        """
        Scale noise ribbon bounds according to the current scaling mode.

        Parameters
        ----------
        bg_data : np.ndarray
            Background data.
        noise_level : np.float32
            Noise level (±1σ).

        Returns
        -------
        Tuple[np.ndarray, np.ndarray]
            Lower and upper bounds for noise ribbon.
        """
        if self.y_scale_mode == "percent":
            # In percentage mode, noise becomes ±(noise/bg * 100)%
            bg_safe = np.where(np.abs(bg_data) < 1e-12, 1e-12, bg_data)
            noise_percent = 100 * noise_level / np.abs(bg_safe)
            return -noise_percent, noise_percent
        elif self.y_scale_mode == "snr":
            # In SNR mode, noise becomes ±1σ
            return np.full_like(bg_data, -1.0), np.full_like(bg_data, 1.0)
        else:
            return bg_data - noise_level, bg_data + noise_level
>>>>>>> REPLACE
```

src/pyosc/waveform/event_plotter.py
```python
<<<<<<< SEARCH
        fig_ev, ax_ev = plt.subplots(figsize=(6, 3))
        ax_ev.plot(
            t_event_display,
            x_event,
            label="Event",
            color="black",
            # marker="o",
            mfc="none",
        )

        if self.bg_clean is not None:
            bg_event = self.bg_clean[mask]
            ax_ev.plot(
                t_event_display,
                bg_event,
                label="BG",
                color="orange",
                ls="--",
            )
            if self.global_noise is not None:
                # Plot noise ribbon around the background
                ax_ev.fill_between(
                    t_event_display,
                    bg_event - self.global_noise,
                    bg_event + self.global_noise,
                    color="gray",
                    alpha=0.3,
                    label="Noise (±1σ)",
                )
        else:
            logger.warning(
                "Clean background (bg_clean) not provided to EventPlotter, cannot plot."
            )
=======
        # Scale the data according to the selected mode
        x_event_scaled, ylabel = self._scale_y_data(x_event, self.bg_clean, mask)

        fig_ev, ax_ev = plt.subplots(figsize=(6, 3))
        ax_ev.plot(
            t_event_display,
            x_event_scaled,
            label="Event",
            color="black",
            # marker="o",
            mfc="none",
        )

        if self.bg_clean is not None:
            bg_event_scaled = self._scale_background_data(self.bg_clean[mask])
            ax_ev.plot(
                t_event_display,
                bg_event_scaled,
                label="BG",
                color="orange",
                ls="--",
            )
            if self.global_noise is not None:
                # Plot noise ribbon around the background
                noise_lower, noise_upper = self._scale_noise_ribbon(self.bg_clean[mask], self.global_noise)
                ax_ev.fill_between(
                    t_event_display,
                    noise_lower,
                    noise_upper,
                    color="gray",
                    alpha=0.3,
                    label="Noise (±1σ)",
                )
        else:
            logger.warning(
                "Clean background (bg_clean) not provided to EventPlotter, cannot plot."
            )
>>>>>>> REPLACE
```

src/pyosc/waveform/event_plotter.py
```python
<<<<<<< SEARCH
        ax_ev.set_title(
            f"{self.osc_plot.data.get_trace_name(self.trace_idx)} Event {event_index + 1}"
        )
        ax_ev.set_ylabel("Signal")
        ax_ev.legend(loc="lower right")
=======
        ax_ev.set_title(
            f"{self.osc_plot.data.get_trace_name(self.trace_idx)} Event {event_index + 1}"
        )
        ax_ev.set_ylabel(ylabel)
        ax_ev.legend(loc="lower right")
>>>>>>> REPLACE
```

src/pyosc/waveform/event_plotter.py
```python
<<<<<<< SEARCH
            if not np.any(mask):
                ax.text(
                    0.5,
                    0.5,
                    f"Event {i + 1}\nNo data",
                    ha="center",
                    va="center",
                    transform=ax.transAxes,
                )
                ax.set_xticks([])
                ax.set_yticks([])
                continue

            # Make time relative to plot window start
            t_event_raw_relative = t_event_raw - plot_start_time

            # Determine offset display parameters for this event
            time_span_raw = plot_end_time - plot_start_time
            event_time_unit, display_scale, offset_time_raw, offset_unit = _determine_offset_display_params(
                (plot_start_time, plot_end_time), time_span_raw
            )
            
            # Scale for display using the display_scale from offset params
            t_event_display = t_event_raw_relative * display_scale

            # Create time formatter for axis
            time_formatter = _create_time_formatter(offset_time_raw, display_scale)

            # Plot event
            ax.plot(
                t_event_display,
                x_event,
                "-ok",
                mfc="none",
                linewidth=1,
                label="Signal",
                ms=4,
            )

            if self.bg_clean is not None:
                bg_event = self.bg_clean[mask]
                ax.plot(
                    t_event_display,
                    bg_event,
                    "orange",
                    linestyle="--",
                    alpha=0.7,
                    label="BG",
                )
                if self.global_noise is not None:
                    # Plot noise ribbon around the background
                    ax.fill_between(
                        t_event_display,
                        bg_event - self.global_noise,
                        bg_event + self.global_noise,
                        color="gray",
                        alpha=0.3,
                        label="Noise (±1σ)",
                    )
            else:
                logger.warning(
                    f"Background data not available for event {i + 1}. Ensure bg_clean is passed to EventPlotter."
                )
=======
            if not np.any(mask):
                ax.text(
                    0.5,
                    0.5,
                    f"Event {i + 1}\nNo data",
                    ha="center",
                    va="center",
                    transform=ax.transAxes,
                )
                ax.set_xticks([])
                ax.set_yticks([])
                continue

            # Make time relative to plot window start
            t_event_raw_relative = t_event_raw - plot_start_time

            # Determine offset display parameters for this event
            time_span_raw = plot_end_time - plot_start_time
            event_time_unit, display_scale, offset_time_raw, offset_unit = _determine_offset_display_params(
                (plot_start_time, plot_end_time), time_span_raw
            )
            
            # Scale for display using the display_scale from offset params
            t_event_display = t_event_raw_relative * display_scale

            # Create time formatter for axis
            time_formatter = _create_time_formatter(offset_time_raw, display_scale)

            # Scale the data according to the selected mode
            x_event_scaled, ylabel = self._scale_y_data(x_event, self.bg_clean, mask)

            # Plot event
            ax.plot(
                t_event_display,
                x_event_scaled,
                "-ok",
                mfc="none",
                linewidth=1,
                label="Signal",
                ms=4,
            )

            if self.bg_clean is not None:
                bg_event_scaled = self._scale_background_data(self.bg_clean[mask])
                ax.plot(
                    t_event_display,
                    bg_event_scaled,
                    "orange",
                    linestyle="--",
                    alpha=0.7,
                    label="BG",
                )
                if self.global_noise is not None:
                    # Plot noise ribbon around the background
                    noise_lower, noise_upper = self._scale_noise_ribbon(self.bg_clean[mask], self.global_noise)
                    ax.fill_between(
                        t_event_display,
                        noise_lower,
                        noise_upper,
                        color="gray",
                        alpha=0.3,
                        label="Noise (±1σ)",
                    )
            else:
                logger.warning(
                    f"Background data not available for event {i + 1}. Ensure bg_clean is passed to EventPlotter."
                )
>>>>>>> REPLACE
```

src/pyosc/waveform/event_plotter.py
```python
<<<<<<< SEARCH
            ax.set_ylabel("Signal", fontsize=8)
            ax.tick_params(labelsize=7)
=======
            ax.set_ylabel(ylabel, fontsize=8)
            ax.tick_params(labelsize=7)
>>>>>>> REPLACE
```

Now you can use the EventPlotter with different Y-axis scaling modes:

```bash
# Example usage in your analysis script:
# For percentage contrast:
event_plotter = EventPlotter(plot, events, bg_clean=bg_clean, global_noise=global_noise, y_scale_mode="percent")

# For SNR scaling:
event_plotter = EventPlotter(plot, events, bg_clean=bg_clean, global_noise=global_noise, y_scale_mode="snr")

# For raw values (default):
event_plotter = EventPlotter(plot, events, bg_clean=bg_clean, global_noise=global_noise, y_scale_mode="raw")
```

> Tokens: 63k sent, 4.6k received. Cost: $0.26 message, $1.53 session.  
> Applied edit to src/pyosc/waveform/event_plotter.py  
> Commit 7c9216f feat: Add Y-axis scaling modes to EventPlotter for percentage contrast and SNR analysis

This commit introduces flexible Y-axis scaling for event plots, allowing users to view events in:
- Raw signal values
- Percentage contrast relative to background
- Signal-to-noise ratio (σ)

Key changes:
- Added `y_scale_mode` parameter to EventPlotter
- Implemented scaling methods for Y-data, background, and noise ribbon
- Updated plot methods to support dynamic Y-axis scaling
- Added input validation and warning messages for scaling modes  
> # Example usage in your analysis script:                                                                        
# For percentage contrast:                                                                                      
event_plotter = EventPlotter(plot, events, bg_clean=bg_clean, global_noise=global_noise, y_scale_mode="percent")
                                                                                                                
# For SNR scaling:                                                                                              
event_plotter = EventPlotter(plot, events, bg_clean=bg_clean, global_noise=global_noise, y_scale_mode="snr")    
                                                                                                                
# For raw values (default):                                                                                     
event_plotter = EventPlotter(plot, events, bg_clean=bg_clean, global_noise=global_noise, y_scale_mode="raw")  
> Run shell commands? (Y)es/(N)o/(D)on't ask again [Yes]: n  
> You can use /undo to undo and discard each aider commit.  

#### /ask ok I'm a little confused. The event plot has a very long title (could be over two lines or reduced font size I think), and the y-axis is still scaled in the same way? did we need to update the analysis script?  
> Warning: it's best to only add files that need changes to the chat.  
> https://aider.chat/docs/troubleshooting/edit-errors.html  

#### ok I'm a little confused. The event plot has a very long title (could be over two lines or reduced font size I think), and the y-axis is still scaled in the same way? did we need to update the analysis script?  

You're right! There are two issues:

1. **Long title**: The event plot title is using the full oscilloscope plot name, which includes the noise information and can be very long.

2. **Y-axis scaling**: You need to update your analysis script to use the new Y-axis scaling feature.

## Fix 1: Shorter event plot titles

Let's update the event plotter to use shorter, cleaner titles:

src/pyosc/waveform/event_plotter.py
```python
<<<<<<< SEARCH
        ax_ev.set_title(
            f"{self.osc_plot.data.get_trace_name(self.trace_idx)} Event {event_index + 1}"
        )
=======
        # Use a shorter title - just the base trace name without noise info
        trace_name = self.osc_plot.data.get_trace_name(self.trace_idx)
        # Remove noise information from title if present
        clean_name = trace_name.split(" | ")[0] if " | " in trace_name else trace_name
        ax_ev.set_title(f"{clean_name} - Event {event_index + 1}")
>>>>>>> REPLACE
```

src/pyosc/waveform/event_plotter.py
```python
<<<<<<< SEARCH
        # Get trace name safely
        trace_name = self.osc_plot.data.get_trace_name(self.trace_idx)

        fig.suptitle(
            f"{trace_name} - Events 1-{n_events} (of {len(self.events)} total)"
        )
=======
        # Get trace name safely and clean it
        trace_name = self.osc_plot.data.get_trace_name(self.trace_idx)
        # Remove noise information from title if present
        clean_name = trace_name.split(" | ")[0] if " | " in trace_name else trace_name

        fig.suptitle(
            f"{clean_name} - Events 1-{n_events} (of {len(self.events)} total)",
            fontsize=12
        )
>>>>>>> REPLACE
```

## Fix 2: Update analysis script to use Y-axis scaling

In your `scripts/run_analysis.py`, update the EventPlotter creation to use percentage contrast:

scripts/run_analysis.py
```python
<<<<<<< SEARCH
    # Create event plotter for detailed event analysis
    event_plotter = EventPlotter(
        plot, events, bg_clean=bg_clean, global_noise=global_noise
    )
    event_plotter.plot_events_grid(max_events=16)
=======
    # Create event plotter for detailed event analysis with percentage contrast
    event_plotter = EventPlotter(
        plot, events, bg_clean=bg_clean, global_noise=global_noise, y_scale_mode="percent"
    )
    event_plotter.plot_events_grid(max_events=16)
>>>>>>> REPLACE
```

Now your event plots will show:
- **Shorter, cleaner titles** without the noise information
- **Y-axis in percentage contrast** (e.g., "-5% contrast" for a 5% dip below background)
- **Background line at 0%** (since it's the reference)
- **Noise ribbon showing ±X%** relative to the background level

You can also try `y_scale_mode="snr"` if you prefer to see events in terms of signal-to-noise ratio (σ units).

> Tokens: 66k sent, 849 received. Cost: $0.21 message, $1.74 session.  
> Warning: it's best to only add files that need changes to the chat.  
> https://aider.chat/docs/troubleshooting/edit-errors.html  

#### go ahead with those fixes  

src/pyosc/waveform/event_plotter.py
```python
<<<<<<< SEARCH
        ax_ev.set_title(
            f"{self.osc_plot.data.get_trace_name(self.trace_idx)} Event {event_index + 1}"
        )
=======
        # Use a shorter title - just the base trace name without noise info
        trace_name = self.osc_plot.data.get_trace_name(self.trace_idx)
        # Remove noise information from title if present
        clean_name = trace_name.split(" | ")[0] if " | " in trace_name else trace_name
        ax_ev.set_title(f"{clean_name} - Event {event_index + 1}")
>>>>>>> REPLACE
```

src/pyosc/waveform/event_plotter.py
```python
<<<<<<< SEARCH
        # Get trace name safely
        trace_name = self.osc_plot.data.get_trace_name(self.trace_idx)

        fig.suptitle(
            f"{trace_name} - Events 1-{n_events} (of {len(self.events)} total)"
        )
=======
        # Get trace name safely and clean it
        trace_name = self.osc_plot.data.get_trace_name(self.trace_idx)
        # Remove noise information from title if present
        clean_name = trace_name.split(" | ")[0] if " | " in trace_name else trace_name

        fig.suptitle(
            f"{clean_name} - Events 1-{n_events} (of {len(self.events)} total)",
            fontsize=12
        )
>>>>>>> REPLACE
```

scripts/run_analysis.py
```python
<<<<<<< SEARCH
    # Create event plotter for detailed event analysis
    event_plotter = EventPlotter(
        plot, events, bg_clean=bg_clean, global_noise=global_noise
    )
    event_plotter.plot_events_grid(max_events=16)
=======
    # Create event plotter for detailed event analysis with percentage contrast
    event_plotter = EventPlotter(
        plot, events, bg_clean=bg_clean, global_noise=global_noise, y_scale_mode="percent"
    )
    event_plotter.plot_events_grid(max_events=16)
>>>>>>> REPLACE
```

> Tokens: 70k sent, 594 received. Cost: $0.22 message, $1.96 session.  
> Applied edit to src/pyosc/waveform/event_plotter.py  
> Applied edit to scripts/run_analysis.py  
> Commit d327410 refactor: Simplify event plot titles and add percentage contrast scaling  
> You can use /undo to undo and discard each aider commit.  

#### /model openrouter/google/gemini-2.5-flash  
> Warning: it's best to only add files that need changes to the chat.  
> https://aider.chat/docs/troubleshooting/edit-errors.html  
> Aider v0.85.1  
> Main model: openrouter/google/gemini-2.5-flash with whole edit format  
> Weak model: openrouter/anthropic/claude-3-5-haiku  
> Git repo: .git with 25 files  
> Repo-map: using 4096 tokens, files refresh  
> Added scripts/run_analysis.py to the chat.  
> Added src/pyosc/oscplot/__init__.py to the chat.  
> Added src/pyosc/oscplot/coordinate_manager.py to the chat.  
> Added src/pyosc/oscplot/data_manager.py to the chat.  
> Added src/pyosc/oscplot/decimation.py to the chat.  
> Added src/pyosc/oscplot/display_state.py to the chat.  
> Added src/pyosc/oscplot/plot.py to the chat.  
> Added src/pyosc/waveform/event_plotter.py to the chat.  
> Added .aider_conventions.md to the chat (read-only).  
> Added share.md to the chat (read-only).  
> Restored previous conversation history.  

#### /editor-model openrouter/google/gemini-2.5-flash  
> Warning: it's best to only add files that need changes to the chat.  
> https://aider.chat/docs/troubleshooting/edit-errors.html  
> Aider v0.85.1  
> Main model: openrouter/google/gemini-2.5-flash with whole edit format  
> Weak model: openrouter/anthropic/claude-3-5-haiku  
> Git repo: .git with 25 files  
> Repo-map: using 4096 tokens, files refresh  
> Added scripts/run_analysis.py to the chat.  
> Added src/pyosc/oscplot/__init__.py to the chat.  
> Added src/pyosc/oscplot/coordinate_manager.py to the chat.  
> Added src/pyosc/oscplot/data_manager.py to the chat.  
> Added src/pyosc/oscplot/decimation.py to the chat.  
> Added src/pyosc/oscplot/display_state.py to the chat.  
> Added src/pyosc/oscplot/plot.py to the chat.  
> Added src/pyosc/waveform/event_plotter.py to the chat.  
> Added .aider_conventions.md to the chat (read-only).  
> Added share.md to the chat (read-only).  
> Restored previous conversation history.  

#### /ask ok that works nicely, what are your thoughts on extending the y-axis changes elsewhere?  
> Warning: it's best to only add files that need changes to the chat.  
> https://aider.chat/docs/troubleshooting/edit-errors.html  

#### ok that works nicely, what are your thoughts on extending the y-axis changes elsewhere?  

The y-axis scaling changes you've implemented in `EventPlotter` are a significant improvement for visualizing events. Extending this functionality to `OscilloscopePlot` would be a logical next step, offering users more control over how the main waveform is displayed.

Here's a breakdown of my thoughts on extending this:

**Benefits of Extension:**

*   **Consistency:** Users would have a consistent experience across both event plots and the main oscilloscope plot, using the same scaling modes (`raw`, `percent`, `snr`).
*   **Enhanced Analysis:** Displaying the main signal in terms of percentage contrast or SNR could immediately highlight regions of interest or anomalies that might be less obvious in raw voltage values, especially when dealing with signals that have a large DC offset or varying background levels.
*   **Improved Readability:** For certain applications, seeing the signal relative to noise or background is more intuitive and informative than absolute voltage.

**Challenges and Considerations:**

1.  **`OscilloscopePlot` Complexity:** `OscilloscopePlot` is already quite complex with its decimation, multiple traces, and various display elements (lines, ribbons, regions). Integrating Y-axis scaling will add another layer of complexity.
2.  **`bg_clean` and `global_noise` Availability:** The `OscilloscopePlot` currently doesn't inherently store `bg_clean` or `global_noise` as part of its `TimeSeriesDataManager`. These are calculated in `run_analysis.py` and passed to `EventPlotter`. To implement `percent` and `snr` scaling in `OscilloscopePlot`, these values would need to be:
    *   Passed during `OscilloscopePlot` initialization.
    *   Stored as attributes within `OscilloscopePlot`.
    *   Potentially updated if the background or noise estimation changes (though this is less likely for a static plot).
3.  **Scaling of All Elements:** When the Y-axis mode changes, *all* plotted elements (main signal, added lines, ribbons, envelopes, and regions) would need to be scaled accordingly. This means the `_scale_y_data`, `_scale_background_data`, and `_scale_noise_ribbon` logic (or similar) would need to be adapted and applied to every element before plotting.
4.  **User Interface:** How would the user select the Y-axis scaling mode in `OscilloscopePlot`?
    *   A parameter in the constructor (like `EventPlotter`)?
    *   A method call (`set_y_scale_mode`)?
    *   A GUI element (if you plan to add interactive controls beyond basic matplotlib navigation)?
5.  **Performance:** Scaling all data points for every redraw (zoom/pan) could introduce a minor performance overhead, though NumPy operations are generally fast. The current decimation strategy should help mitigate this.
6.  **Edge Cases:**
    *   What happens if `bg_clean` is zero or very close to zero when `percent` scaling is used? (You already handle this in `EventPlotter` with `bg_safe`).
    *   What if `global_noise` is zero for `snr` scaling?

**Proposed Implementation Steps (High-Level):**

1.  **Add `y_scale_mode`, `bg_clean`, `global_noise` to `OscilloscopePlot.__init__`:**
    *   Make them optional, with `raw` as the default `y_scale_mode`.
    *   Store them as instance attributes.
    *   Add validation similar to `EventPlotter` to warn if `percent` or `snr` is chosen without the necessary `bg_clean` or `global_noise`.
2.  **Modify `_get_plot_data` and `_render_plot_elements`:**
    *   The `_get_plot_data` method currently returns `t_raw`, `x_new`, `x_min`, `x_max` in raw units. This is good. The scaling should happen *after* decimation but *before* plotting.
    *   Introduce a new helper method, e.g., `_apply_y_scaling(data_array, bg_array_for_scaling, noise_for_scaling)` that encapsulates the scaling logic (similar to `_scale_y_data` in `EventPlotter`).
    *   In `_render_plot_elements` (or a new helper function called from there), apply this scaling to:
        *   The main `x_new` signal.
        *   `x_min` and `x_max` for the main envelope.
        *   All `_lines`, `_ribbons`, and `_envelopes` data *before* they are passed to `ax.plot` or `ax.fill_between`. This will require iterating through their `data_raw`, `center_data_raw`, `min_data_raw`, `max_data_raw` attributes and applying the scaling.
3.  **Update `_update_y_limits`:** The `_update_y_limits` method would then operate on the *scaled* data, which is what you want.
4.  **Update Y-axis Label:** Dynamically change the `ax.set_ylabel` based on `self.y_scale_mode`.

This would be a substantial change, but it would significantly enhance the `OscilloscopePlot`'s capabilities. I recommend tackling it incrementally, perhaps starting with just the main signal and then extending to other elements.

> Tokens: 51k sent, 1.2k received. Cost: $0.02 message, $1.98 session.  
> Warning: it's best to only add files that need changes to the chat.  
> https://aider.chat/docs/troubleshooting/edit-errors.html  

#### /exit  

# aider chat started at 2025-07-10 15:27:05

> /home/samsc/.local/bin/aider  
> Aider v0.85.1  
> Main model: openrouter/anthropic/claude-sonnet-4 with diff edit format, prompt cache, infinite output  
> Weak model: openrouter/anthropic/claude-3-5-haiku  
> Git repo: .git with 26 files  
> Repo-map: using 4096 tokens, files refresh  
> Added share.md to the chat (read-only).  
> Added .aider_conventions.md to the chat (read-only).  

#### /add scripts/run_analysis.py  
> Added scripts/run_analysis.py to the chat  

#### /ask it looks like the below section is quite slow for e.g. 1000 events. Could we speed it up e.g. with numba? def calculate_clean_background(  
####     t: np.ndarray,  
####     x: np.ndarray,  
####     events_initial: np.ndarray,  
####     smooth_n: int,  
####     bg_initial: np.ndarray,  
#### ) -> np.ndarray:  
####     """  
####     Stage 4: Calculate clean background by masking events.  
####   
####     Parameters  
####     ----------  
####     t : np.ndarray  
####         Time array.  
####     x : np.ndarray  
####         Signal array.  
####     events_initial : np.ndarray  
####         Initial events array.  
####     smooth_n : int  
####         Smoothing window size in samples.  
####     bg_initial : np.ndarray  
####         Initial background estimate.  
####   
####     Returns  
####     -------  
####     np.ndarray  
####         Clean background estimate.  
####     """  
####     logger.info("Calculating clean background")  
####   
####     # Create mask to exclude events  
####     mask = np.ones_like(x, dtype=bool)  
####     for t_start, t_end in events_initial:  
####         event_mask = (t >= t_start) & (t < t_end)  
####         mask[event_mask] = False  

#### it looks like the below section is quite slow for e.g. 1000 events. Could we speed it up e.g. with numba? def calculate_clean_background(  
####     t: np.ndarray,  
####     x: np.ndarray,  
####     events_initial: np.ndarray,  
####     smooth_n: int,  
####     bg_initial: np.ndarray,  
#### ) -> np.ndarray:  
####     """  
####     Stage 4: Calculate clean background by masking events.  
####   
####     Parameters  
####     ----------  
####     t : np.ndarray  
####         Time array.  
####     x : np.ndarray  
####         Signal array.  
####     events_initial : np.ndarray  
####         Initial events array.  
####     smooth_n : int  
####         Smoothing window size in samples.  
####     bg_initial : np.ndarray  
####         Initial background estimate.  
####   
####     Returns  
####     -------  
####     np.ndarray  
####         Clean background estimate.  
####     """  
####     logger.info("Calculating clean background")  
####   
####     # Create mask to exclude events  
####     mask = np.ones_like(x, dtype=bool)  
####     for t_start, t_end in events_initial:  
####         event_mask = (t >= t_start) & (t < t_end)  
####         mask[event_mask] = False  

Yes, the masking loop can definitely be sped up with numba. The current approach creates a boolean mask for each event individually, which is inefficient for many events.

Here's a numba-optimized version:

```python
from numba import njit

@njit
def _create_event_mask_numba(t: np.ndarray, events: np.ndarray) -> np.ndarray:
    """
    Create a boolean mask to exclude event regions using numba for speed.
    
    Parameters
    ----------
    t : np.ndarray
        Time array.
    events : np.ndarray
        Events array with shape (n_events, 2) where each row is [t_start, t_end].
    
    Returns
    -------
    np.ndarray
        Boolean mask where True means keep the sample, False means exclude.
    """
    mask = np.ones(len(t), dtype=np.bool_)
    
    for i in range(len(events)):
        t_start = events[i, 0]
        t_end = events[i, 1]
        
        # Find indices where time is within event bounds
        for j in range(len(t)):
            if t_start <= t[j] < t_end:
                mask[j] = False
    
    return mask
```

Then modify the `calculate_clean_background` function:

```python
def calculate_clean_background(
    t: np.ndarray,
    x: np.ndarray,
    events_initial: np.ndarray,
    smooth_n: int,
    bg_initial: np.ndarray,
) -> np.ndarray:
    """
    Stage 4: Calculate clean background by masking events.
    """
    logger.info("Calculating clean background")

    # Create mask to exclude events using numba-optimized function
    mask = _create_event_mask_numba(t, events_initial)

    # Rest of the function remains the same...
    logger.debug(
        f"Masked {np.sum(~mask)} samples ({100 * np.sum(~mask) / len(mask):.1f}%) for clean background"
    )

    t_masked = t[mask]
    x_masked = x[mask]
    bg_clean = np.copy(bg_initial)

    if np.sum(mask) > 2 * smooth_n:
        from scipy.interpolate import interp1d

        interp = interp1d(t_masked, x_masked, kind="linear", fill_value="extrapolate")
        x_interp = interp(t)
        bg_clean = savgol_filter(x_interp, smooth_n, 3).astype(np.float32)
        logger.debug(
            f"Clean background: mean={np.mean(bg_clean):.3g}, std={np.std(bg_clean):.3g}"
        )
    else:
        logger.debug(
            "Insufficient unmasked samples for clean background - using initial"
        )

    return bg_clean
```

This should be significantly faster for 1000+ events since numba compiles the masking loop to optimised machine code, avoiding the Python loop overhead.

> Tokens: 14k sent, 776 received. Cost: $0.05 message, $0.05 session.  

#### /ask sorry I think maybe the full `calculate_clean_background` is quite slow - any ideas?  

#### sorry I think maybe the full `calculate_clean_background` is quite slow - any ideas?  

Looking at the full `calculate_clean_background` function, there are several potential bottlenecks beyond just the masking loop:

1. **The masking loop** (which we addressed)
2. **scipy.interpolate.interp1d** - this can be slow for large datasets
3. **savgol_filter** - Savitzky-Golay filtering can be expensive

Here are some optimisation strategies:

## Option 1: Faster interpolation with numba
Replace scipy's interp1d with a numba-optimised linear interpolation:

```python
@njit
def _linear_interp_numba(x_known: np.ndarray, y_known: np.ndarray, x_new: np.ndarray) -> np.ndarray:
    """Fast linear interpolation using numba."""
    result = np.empty_like(x_new)
    
    for i in range(len(x_new)):
        x_val = x_new[i]
        
        # Handle extrapolation
        if x_val <= x_known[0]:
            result[i] = y_known[0]
        elif x_val >= x_known[-1]:
            result[i] = y_known[-1]
        else:
            # Find interpolation indices
            for j in range(len(x_known) - 1):
                if x_known[j] <= x_val <= x_known[j + 1]:
                    # Linear interpolation
                    t = (x_val - x_known[j]) / (x_known[j + 1] - x_known[j])
                    result[i] = y_known[j] + t * (y_known[j + 1] - y_known[j])
                    break
    
    return result
```

## Option 2: Skip interpolation entirely
For many cases, you might not need interpolation at all:

```python
def calculate_clean_background(
    t: np.ndarray,
    x: np.ndarray,
    events_initial: np.ndarray,
    smooth_n: int,
    bg_initial: np.ndarray,
) -> np.ndarray:
    """Stage 4: Calculate clean background by masking events."""
    logger.info("Calculating clean background")

    # Fast masking with numba
    mask = _create_event_mask_numba(t, events_initial)
    
    logger.debug(
        f"Masked {np.sum(~mask)} samples ({100 * np.sum(~mask) / len(mask):.1f}%) for clean background"
    )

    if np.sum(mask) > 2 * smooth_n:
        # Option A: Direct filtering on masked data, then interpolate gaps
        x_for_filter = np.copy(x)
        x_for_filter[~mask] = bg_initial[~mask]  # Fill masked regions with initial bg
        bg_clean = savgol_filter(x_for_filter, smooth_n, 3).astype(np.float32)
        
        # Option B: Or use faster interpolation
        # t_masked = t[mask]
        # x_masked = x[mask]
        # x_interp = _linear_interp_numba(t_masked, x_masked, t)
        # bg_clean = savgol_filter(x_interp, smooth_n, 3).astype(np.float32)
        
        logger.debug(
            f"Clean background: mean={np.mean(bg_clean):.3g}, std={np.std(bg_clean):.3g}"
        )
    else:
        logger.debug("Insufficient unmasked samples for clean background - using initial")
        bg_clean = np.copy(bg_initial)

    return bg_clean
```

## Option 3: Use scipy's faster filters
Replace savgol_filter with a simpler moving average or use scipy's optimised filters:

```python
from scipy.ndimage import uniform_filter1d

# Replace savgol_filter with:
bg_clean = uniform_filter1d(x_interp, size=smooth_n).astype(np.float32)
```

**Which bottleneck do you think is the main culprit?** You could add some timing statements to identify:

```python
import time

def calculate_clean_background(...):
    start = time.time()
    
    # Masking
    mask = _create_event_mask_numba(t, events_initial)
    mask_time = time.time()
    
    # Interpolation
    if np.sum(mask) > 2 * smooth_n:
        t_masked = t[mask]
        x_masked = x[mask]
        interp_start = time.time()
        # ... interpolation code
        interp_time = time.time()
        
        # Filtering
        filter_start = time.time()
        bg_clean = savgol_filter(x_interp, smooth_n, 3).astype(np.float32)
        filter_time = time.time()
        
        logger.debug(f"Timing: mask={mask_time-start:.3f}s, interp={interp_time-interp_start:.3f}s, filter={filter_time-filter_start:.3f}s")
```

> Tokens: 15k sent, 1.3k received. Cost: $0.06 message, $0.12 session.  

#### /add scripts/run_analysis03.py  
> Added scripts/run_analysis03.py to the chat  

#### /ask these two scripts have a lot of duplicated code. Should we move the functions to `pysosc/waveform` and just have the script contain config & main?  

#### these two scripts have a lot of duplicated code. Should we move the functions to `pysosc/waveform` and just have the script contain config & main?  

Yes, absolutely! The scripts have significant code duplication and would benefit from refactoring. Moving the shared functions to `pyosc/waveform` would make the code more maintainable and reusable.

Here's how I'd suggest organising it:

## New module: `src/pyosc/waveform/analysis.py`

Move these functions to a new analysis module:
- `load_data`
- `calculate_smoothing_parameters` 
- `calculate_initial_background`
- `estimate_noise`
- `detect_initial_events`
- `calculate_clean_background` (with optimisations)
- `analyze_thresholds`
- `detect_final_events`
- `analyze_events`
- `create_oscilloscope_plot`
- `process_file`

## Updated scripts structure:

```python
# scripts/run_analysis.py
from pyosc.waveform.analysis import process_file, configure_logging
from pyosc.waveform.io import get_waveform_params

CONFIG = {
    # ... config dict
}

def main():
    configure_logging(CONFIG.get("LOG_LEVEL", "INFO"))
    
    for measurement in CONFIG["MEASUREMENTS"]:
        # ... existing logic
        process_file(name, cfg, sidecar=sidecar, crop=measurement.get("crop"))

if __name__ == "__main__":
    # matplotlib setup
    main()
```

## Benefits:
1. **DRY principle** - eliminate code duplication
2. **Reusability** - other scripts can import these functions
3. **Testability** - easier to write unit tests for individual functions
4. **Maintainability** - bug fixes and improvements in one place
5. **Cleaner scripts** - focus on configuration and workflow

## Update `src/pyosc/waveform/__init__.py`:
```python
__all__ = [
    "get_waveform_params",
    "rd", 
    "detect_events",
    "merge_overlapping_events",
    "EventPlotter",
    "process_file",  # Add new exports
    "configure_logging",
    # ... other analysis functions as needed
]
```

Would you like me to help create the `analysis.py` module with the refactored functions?

> Tokens: 25k sent, 552 received. Cost: $0.08 message, $0.20 session.  
> src/pyosc/waveform/__init__.py  
> Add file to the chat? (Y)es/(N)o/(D)on't ask again [Yes]: y  

Yes, we should add a few more files to properly refactor this. Please add these files to the chat:

1. **src/pyosc/waveform/event_detector.py** - We need to see the current event detection functions to avoid duplication
2. **src/pyosc/waveform/event_plotter.py** - The scripts use `EventPlotter`, so we need to see its current interface
3. **src/pyosc/waveform/io.py** - The scripts use `get_waveform_params` and `rd` functions

Once I can see these files, I can help create a new **src/pyosc/waveform/analysis.py** module that:

- Contains all the shared analysis functions from the scripts
- Properly imports from the existing waveform modules
- Maintains compatibility with the current API
- Includes the numba optimisations for `calculate_clean_background`

This will let us clean up the scripts to just contain:
- Configuration dictionaries
- Matplotlib setup
- Main function that calls the analysis functions

Would you like to add those files so we can proceed with the refactoring?

> Tokens: 25k sent, 266 received. Cost: $0.08 message, $0.28 session.  
> src/pyosc/waveform/event_detector.py  
> Add file to the chat? (Y)es/(N)o/(A)ll/(S)kip all/(D)on't ask again [Yes]: a  
> src/pyosc/waveform/event_plotter.py  

#### Add file to the chat? (Y)es/(N)o/(A)ll/(S)kip all/(D)on't ask again [Yes]: all  
> Add file to the chat? (Y)es/(N)o/(A)ll/(S)kip all/(D)on't ask again [Yes]: a  
> src/pyosc/waveform/io.py  

#### Add file to the chat? (Y)es/(N)o/(A)ll/(S)kip all/(D)on't ask again [Yes]: all  
> Add file to the chat? (Y)es/(N)o/(A)ll/(S)kip all/(D)on't ask again [Yes]: a  
> Warning: it's best to only add files that need changes to the chat.  
> https://aider.chat/docs/troubleshooting/edit-errors.html  

Perfect! Now I have all the files needed. Let's create the refactored structure.

## Create: `src/pyosc/waveform/analysis.py`

This new module will contain all the shared analysis functions from the scripts:

```python
import os
import sys
import time
from typing import Any, Dict, List, Optional, Tuple

import matplotlib.pyplot as plt
import numpy as np
from loguru import logger
from numba import njit
from scipy.interpolate import interp1d
from scipy.signal import savgol_filter

from pyosc.oscplot.plot import OscilloscopePlot
from pyosc.waveform.event_detector import (
    MEDIAN_TO_STD_FACTOR,
    detect_events,
    merge_overlapping_events,
)
from pyosc.waveform.event_plotter import EventPlotter
from pyosc.waveform.io import rd


@njit
def _create_event_mask_numba(t: np.ndarray, events: np.ndarray) -> np.ndarray:
    """
    Create a boolean mask to exclude event regions using numba for speed.
    
    Parameters
    ----------
    t : np.ndarray
        Time array.
    events : np.ndarray
        Events array with shape (n_events, 2) where each row is [t_start, t_end].
    
    Returns
    -------
    np.ndarray
        Boolean mask where True means keep the sample, False means exclude.
    """
    mask = np.ones(len(t), dtype=np.bool_)
    
    for i in range(len(events)):
        t_start = events[i, 0]
        t_end = events[i, 1]
        
        # Find indices where time is within event bounds
        for j in range(len(t)):
            if t_start <= t[j] < t_end:
                mask[j] = False
    
    return mask


def configure_logging(log_level: str = "INFO") -> None:
    """
    Configure loguru logging with specified level.

    Parameters
    ----------
    log_level : str, default="INFO"
        Logging level: DEBUG, INFO, WARNING, ERROR, CRITICAL.
    """
    logger.remove()
    logger.add(
        sys.stderr,
        level=log_level.upper(),
        format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <level>{message}</level>",
        colorize=True,
    )


def load_data(
    name: str,
    cfg: Dict[str, Any],
    sidecar: Optional[str] = None,
    crop: Optional[List[int]] = None,
) -> Tuple[np.ndarray, np.ndarray]:
    """
    Stage 1: Load waveform data from file.

    Parameters
    ----------
    name : str
        Filename of the waveform data.
    cfg : Dict[str, Any]
        Configuration dictionary with SAMPLING_INTERVAL key.
    sidecar : str, optional
        XML sidecar filename.
    crop : List[int], optional
        Crop indices [start, end].

    Returns
    -------
    Tuple[np.ndarray, np.ndarray]
        Time and signal arrays.
    """
    logger.info(f"Loading data from {name}")
    t, x = rd(
        name,
        cfg["SAMPLING_INTERVAL"],
        data_path=cfg["DATA_PATH"],
        xml_filename=sidecar,
        crop=crop,
    )

    logger.debug(
        f"Signal statistics: min={np.min(x):.3g}, max={np.max(x):.3g}, mean={np.mean(x):.3g}, std={np.std(x):.3g}"
    )

    return t, x


def calculate_smoothing_parameters(cfg: Dict[str, Any]) -> Tuple[int, int]:
    """
    Calculate smoothing window size and minimum event length in samples.

    Parameters
    ----------
    cfg : Dict[str, Any]
        Configuration dictionary.

    Returns
    -------
    Tuple[int, int]
        Smoothing window size and minimum event length in samples.
    """
    if "SAMPLING_INTERVAL" not in cfg:
        raise KeyError("SAMPLING_INTERVAL must be set in configuration")

    if cfg["SMOOTH_WIN_T"] is not None:
        smooth_n = int(cfg["SMOOTH_WIN_T"] / cfg["SAMPLING_INTERVAL"])
    elif cfg["SMOOTH_WIN_F"] is not None:
        smooth_n = int(1 / (cfg["SMOOTH_WIN_F"] * cfg["SAMPLING_INTERVAL"]))
    else:
        raise ValueError("Set either SMOOTH_WIN_T or SMOOTH_WIN_F")

    if smooth_n % 2 == 0:
        smooth_n += 1

    min_event_n = int(cfg["MIN_EVENT_T"] / cfg["SAMPLING_INTERVAL"])

    smooth_freq_hz = 1 / (smooth_n * cfg["SAMPLING_INTERVAL"])
    logger.info(
        f"--Smooth window: {smooth_n} samples ({cfg['SMOOTH_WIN_T'] * 1e6:.1f} µs, {smooth_freq_hz:.1f} Hz)"
    )
    logger.info(
        f"--Min event length: {min_event_n} samples ({cfg['MIN_EVENT_T'] * 1e6:.1f} µs)"
    )
    logger.info(f"--Detection SNR: {cfg['DETECTION_SNR']}")
    logger.info(f"--Min keep SNR: {cfg['MIN_EVENT_KEEP_SNR']}")
    logger.info(f"--Widen fraction: {cfg['WIDEN_FRAC']}")
    logger.info(
        f"--Signal polarity: {cfg['SIGNAL_POLARITY']} ({'negative' if cfg['SIGNAL_POLARITY'] < 0 else 'positive'} events)"
    )

    return smooth_n, min_event_n


def calculate_initial_background(
    t: np.ndarray, x: np.ndarray, smooth_n: int
) -> np.ndarray:
    """
    Stage 2: Calculate initial background estimate.

    Parameters
    ----------
    t : np.ndarray
        Time array.
    x : np.ndarray
        Signal array.
    smooth_n : int
        Smoothing window size in samples.

    Returns
    -------
    np.ndarray
        Initial background estimate.
    """
    logger.info("Calculating initial background")
    bg_initial = savgol_filter(x, smooth_n, 3).astype(np.float32)
    logger.debug(
        f"Initial background: mean={np.mean(bg_initial):.3g}, std={np.std(bg_initial):.3g}"
    )
    return bg_initial


def estimate_noise(x: np.ndarray, bg_initial: np.ndarray) -> np.float32:
    """
    Stage 2: Estimate noise level.

    Parameters
    ----------
    x : np.ndarray
        Signal array.
    bg_initial : np.ndarray
        Initial background estimate.

    Returns
    -------
    np.float32
        Estimated noise level.
    """
    global_noise = np.float32(np.median(np.abs(x - bg_initial)) * MEDIAN_TO_STD_FACTOR)

    signal_rms = np.sqrt(np.mean(x**2))
    signal_range = np.max(x) - np.min(x)
    noise_pct_rms = 100 * global_noise / signal_rms if signal_rms > 0 else 0
    noise_pct_range = 100 * global_noise / signal_range if signal_range > 0 else 0

    logger.info(
        f"Global noise level: {global_noise:.3g} ({noise_pct_rms:.1f}% of RMS, {noise_pct_range:.1f}% of range)"
    )

    snr_estimate = np.std(x) / global_noise
    logger.info(f"Estimated signal SNR: {snr_estimate:.2f}")

    return global_noise


def detect_initial_events(
    t: np.ndarray,
    x: np.ndarray,
    bg_initial: np.ndarray,
    global_noise: np.float32,
    cfg: Dict[str, Any],
    min_event_n: int,
) -> np.ndarray:
    """
    Stage 3: Detect initial events.

    Parameters
    ----------
    t : np.ndarray
        Time array.
    x : np.ndarray
        Signal array.
    bg_initial : np.ndarray
        Initial background estimate.
    global_noise : np.float32
        Estimated noise level.
    cfg : Dict[str, Any]
        Configuration dictionary.
    min_event_n : int
        Minimum event length in samples.

    Returns
    -------
    np.ndarray
        Array of initial events.
    """
    logger.info("Detecting initial events")
    min_event_amp = np.float32(cfg["MIN_EVENT_KEEP_SNR"]) * global_noise

    logger.info(f"Detection threshold: {cfg['DETECTION_SNR']}σ below background")
    logger.info(f"Keep threshold: {cfg['MIN_EVENT_KEEP_SNR']}σ below background")
    logger.info(f"Min event amplitude threshold: {min_event_amp:.3g}")

    events_initial, _ = detect_events(
        t,
        x,
        bg_initial,
        snr_threshold=np.float32(cfg["DETECTION_SNR"]),
        min_event_len=min_event_n,
        min_event_amp=min_event_amp,
        widen_frac=np.float32(cfg["WIDEN_FRAC"]),
        global_noise=global_noise,
        signal_polarity=cfg["SIGNAL_POLARITY"],
    )

    logger.info(f"Found {len(events_initial)} initial events after filtering")

    events_initial = merge_overlapping_events(events_initial)
    logger.info(f"After merging: {len(events_initial)} events")

    return events_initial


def calculate_clean_background(
    t: np.ndarray,
    x: np.ndarray,
    events_initial: np.ndarray,
    smooth_n: int,
    bg_initial: np.ndarray,
) -> np.ndarray:
    """
    Stage 4: Calculate clean background by masking events.

    Parameters
    ----------
    t : np.ndarray
        Time array.
    x : np.ndarray
        Signal array.
    events_initial : np.ndarray
        Initial events array.
    smooth_n : int
        Smoothing window size in samples.
    bg_initial : np.ndarray
        Initial background estimate.

    Returns
    -------
    np.ndarray
        Clean background estimate.
    """
    logger.info("Calculating clean background")
    start_time = time.time()

    # Fast masking with numba
    mask = _create_event_mask_numba(t, events_initial)
    mask_time = time.time()

    logger.debug(
        f"Masked {np.sum(~mask)} samples ({100 * np.sum(~mask) / len(mask):.1f}%) for clean background"
    )

    t_masked = t[mask]
    x_masked = x[mask]

    if np.sum(mask) > 2 * smooth_n:
        interp_start = time.time()
        interp = interp1d(t_masked, x_masked, kind="linear", fill_value="extrapolate")
        x_interp = interp(t)
        interp_end = time.time()
        
        filter_start = time.time()
        bg_clean = savgol_filter(x_interp, smooth_n, 3).astype(np.float32)
        filter_end = time.time()
        
        logger.debug(f"Timing: mask={mask_time-start_time:.3f}s, interp={interp_end-interp_start:.3f}s, filter={filter_end-filter_start:.3f}s")
        logger.debug(
            f"Clean background: mean={np.mean(bg_clean):.3g}, std={np.std(bg_clean):.3g}"
        )
    else:
        logger.debug("Insufficient unmasked samples for clean background - using initial")
        bg_clean = bg_initial

    return bg_clean


def analyze_thresholds(
    x: np.ndarray, bg_clean: np.ndarray, global_noise: np.float32, cfg: Dict[str, Any]
) -> Tuple[np.ndarray, np.ndarray]:
    """
    Analyze threshold statistics and create threshold arrays.

    Parameters
    ----------
    x : np.ndarray
        Signal array.
    bg_clean : np.ndarray
        Clean background estimate.
    global_noise : np.float32
        Estimated noise level.
    cfg : Dict[str, Any]
        Configuration dictionary.

    Returns
    -------
    Tuple[np.ndarray, np.ndarray]
        Detection and keep threshold arrays.
    """
    logger.info("Analyzing thresholds")

    if cfg["SIGNAL_POLARITY"] < 0:
        detection_threshold = bg_clean - cfg["DETECTION_SNR"] * global_noise
        keep_threshold = bg_clean - cfg["MIN_EVENT_KEEP_SNR"] * global_noise
        below_detection_pct = 100 * np.sum(x < detection_threshold) / len(x)
        below_keep_pct = 100 * np.sum(x < keep_threshold) / len(x)
        logger.info(f"Samples below detection threshold: {below_detection_pct:.2f}%")
        logger.info(f"Samples below keep threshold: {below_keep_pct:.2f}%")
    else:
        detection_threshold = bg_clean + cfg["DETECTION_SNR"] * global_noise
        keep_threshold = bg_clean + cfg["MIN_EVENT_KEEP_SNR"] * global_noise
        above_detection_pct = 100 * np.sum(x > detection_threshold) / len(x)
        above_keep_pct = 100 * np.sum(x > keep_threshold) / len(x)
        logger.info(f"Samples above detection threshold: {above_detection_pct:.2f}%")
        logger.info(f"Samples above keep threshold: {above_keep_pct:.2f}%")

    return detection_threshold, keep_threshold


def detect_final_events(
    t: np.ndarray,
    x: np.ndarray,
    bg_clean: np.ndarray,
    global_noise: np.float32,
    cfg: Dict[str, Any],
    min_event_n: int,
) -> np.ndarray:
    """
    Stage 5: Detect final events using clean background.

    Parameters
    ----------
    t : np.ndarray
        Time array.
    x : np.ndarray
        Signal array.
    bg_clean : np.ndarray
        Clean background estimate.
    global_noise : np.float32
        Estimated noise level.
    cfg : Dict[str, Any]
        Configuration dictionary.
    min_event_n : int
        Minimum event length in samples.

    Returns
    -------
    np.ndarray
        Array of final events.
    """
    logger.info("Detecting final events")
    min_event_amp = np.float32(cfg["MIN_EVENT_KEEP_SNR"]) * global_noise

    events, noise = detect_events(
        t,
        x,
        bg_clean,
        snr_threshold=np.float32(cfg["DETECTION_SNR"]),
        min_event_len=min_event_n,
        min_event_amp=min_event_amp,
        widen_frac=np.float32(cfg["WIDEN_FRAC"]),
        global_noise=global_noise,
        signal_polarity=cfg["SIGNAL_POLARITY"],
    )

    events = merge_overlapping_events(events)
    logger.info(f"Detected {len(events)} final events")

    return events


def analyze_events(
    t: np.ndarray,
    x: np.ndarray,
    bg_clean: np.ndarray,
    events: np.ndarray,
    global_noise: np.float32,
    cfg: Dict[str, Any],
) -> None:
    """
    Stage 7: Analyze event characteristics.

    Parameters
    ----------
    t : np.ndarray
        Time array.
    x : np.ndarray
        Signal array.
    bg_clean : np.ndarray
        Clean background estimate.
    events : np.ndarray
        Events array.
    global_noise : np.float32
        Estimated noise level.
    cfg : Dict[str, Any]
        Configuration dictionary.
    """
    if len(events) == 0:
        logger.info("No events to analyze")
        return
    if len(events) > 1000:
        logger.warning(
            f"Detected {len(events)} events, which is more than 1000. Skipping analysis."
        )
        return

    event_durations = (events[:, 1] - events[:, 0]) * 1000000  # Convert to µs
    event_amplitudes = []

    for t_start, t_end in events:
        event_mask = (t >= t_start) & (t < t_end)
        if np.any(event_mask):
            if cfg["SIGNAL_POLARITY"] < 0:
                amp = np.min(x[event_mask] - bg_clean[event_mask])
            else:
                amp = np.max(x[event_mask] - bg_clean[event_mask])
            event_amplitudes.append(abs(amp))

    if event_amplitudes:
        logger.info(
            f"Event durations (µs): min={np.min(event_durations):.2f}, max={np.max(event_durations):.2f}, mean={np.mean(event_durations):.2f}"
        )
        logger.info(
            f"Event amplitudes: min={np.min(event_amplitudes):.3g}, max={np.max(event_amplitudes):.3g}, mean={np.mean(event_amplitudes):.3g}"
        )
        logger.info(
            f"Event amplitude SNRs: min={np.min(event_amplitudes) / global_noise:.2f}, max={np.max(event_amplitudes) / global_noise:.2f}"
        )

    final_signal_rms = np.sqrt(np.mean(x**2))
    final_noise_pct_rms = (
        100 * global_noise / final_signal_rms if final_signal_rms > 0 else 0
    )
    final_signal_range = np.max(x) - np.min(x)
    final_noise_pct_range = (
        100 * global_noise / final_signal_range if final_signal_range > 0 else 0
    )

    logger.info(
        f"Noise summary: {global_noise:.3g} ({final_noise_pct_rms:.1f}% of RMS, {final_noise_pct_range:.1f}% of range)"
    )


def create_oscilloscope_plot(
    t: np.ndarray,
    x: np.ndarray,
    bg_initial: np.ndarray,
    bg_clean: np.ndarray,
    events: np.ndarray,
    detection_threshold: np.ndarray,
    keep_threshold: np.ndarray,
    name: str,
    cfg: Dict[str, Any],
    smooth_n: int,
    global_noise: Optional[np.float32] = None,
) -> OscilloscopePlot:
    """
    Stage 6: Create oscilloscope plot with all visualization elements.

    Parameters
    ----------
    t : np.ndarray
        Time array.
    x : np.ndarray
        Signal array.
    bg_initial : np.ndarray
        Initial background estimate.
    bg_clean : np.ndarray
        Clean background estimate.
    events : np.ndarray
        Events array.
    detection_threshold : np.ndarray
        Detection threshold array.
    keep_threshold : np.ndarray
        Keep threshold array.
    name : str
        Name for the plot.
    cfg : Dict[str, Any]
        Configuration dictionary.
    smooth_n : int
        Smoothing window size in samples.
    global_noise : Optional[np.float32], default=None
        Estimated noise level. If provided, will be plotted as a ribbon.

    Returns
    -------
    OscilloscopePlot
        Configured oscilloscope plot.
    """
    logger.info("Creating visualization")

    plot_name = name
    if global_noise is not None:
        plot_signal_rms = np.sqrt(np.mean(x**2))
        plot_noise_pct_rms = (
            100 * global_noise / plot_signal_rms if plot_signal_rms > 0 else 0
        )
        plot_name = f"{name} | Global noise: {global_noise:.3g} ({plot_noise_pct_rms:.1f}% of RMS)"

    plot = OscilloscopePlot(
        t,
        x,
        name=plot_name,
        max_plot_points=cfg["MAX_PLOT_POINTS"],
        mode_switch_threshold=cfg["ENVELOPE_MODE_LIMIT"],
        envelope_window_samples=smooth_n,
    )

    plot.add_line(
        t,
        bg_clean,
        label="Background",
        color="orange",
        alpha=0.6,
        linewidth=1.5,
        display_mode=OscilloscopePlot.MODE_BOTH,
    )

    if global_noise is not None:
        plot.add_ribbon(
            t,
            bg_clean,
            global_noise,
            label="Noise (±1σ)",
            color="gray",
            alpha=0.3,
            display_mode=OscilloscopePlot.MODE_DETAIL,
        )

    plot.add_line(
        t,
        detection_threshold,
        label=f"Detection ({cfg['DETECTION_SNR']}σ)",
        color="red",
        alpha=0.7,
        linestyle=":",
        linewidth=1.5,
        display_mode=OscilloscopePlot.MODE_DETAIL,
    )

    plot.add_line(
        t,
        keep_threshold,
        label=f"Keep ({cfg['MIN_EVENT_KEEP_SNR']}σ)",
        color="darkred",
        alpha=0.7,
        linestyle="--",
        linewidth=1.5,
        display_mode=OscilloscopePlot.MODE_DETAIL,
    )

    if len(events) > 0:
        plot.add_regions(
            events,
            label="Events",
            color="crimson",
            alpha=0.4,
            display_mode=OscilloscopePlot.MODE_BOTH,
        )

    plot.render()
    return plot


def process_file(
    name: str,
    cfg: Dict[str, Any],
    sidecar: Optional[str] = None,
    crop: Optional[List[int]] = None,
    yscale_mode: str = "snr",
    show_plots: bool = True,
) -> None:
    """
    Process a single waveform file for event detection.

    Parameters
    ----------
    name : str
        Filename of the waveform data.
    cfg : Dict[str, Any]
        Configuration dictionary with SAMPLING_INTERVAL key.
    sidecar : str, optional
        XML sidecar filename.
    crop : List[int], optional
        Crop indices [start, end].
    yscale_mode : str, default="snr"
        Y-axis scaling mode for event plotter.
    show_plots : bool, default=True
        Whether to show plots interactively.
    """
    start_time = time.time()
    logger.info(f"Processing {name} with parameters:")

    # Stage 1: Load Data
    t, x = load_data(name, cfg, sidecar, crop)

    # Calculate parameters
    smooth_n, min_event_n = calculate_smoothing_parameters(cfg)

    # Stage 2: Background Calculation
    bg_initial = calculate_initial_background(t, x, smooth_n)
    global_noise = estimate_noise(x, bg_initial)

    # Stage 3: Initial Event Detection
    bg_time = time.time()
    logger.debug(f"Background calculation took {bg_time - start_time:.3f}s")
    events_initial = detect_initial_events(
        t, x, bg_initial, global_noise, cfg, min_event_n
    )

    # Stage 4: Clean Background Calculation
    bg_clean = calculate_clean_background(t, x, events_initial, smooth_n, bg_initial)

    # Analyze thresholds
    detection_threshold, keep_threshold = analyze_thresholds(
        x, bg_clean, global_noise, cfg
    )

    # Stage 5: Final Event Detection
    events = detect_final_events(t, x, bg_clean, global_noise, cfg, min_event_n)
    detection_time = time.time()
    logger.debug(f"Event detection took {detection_time - bg_time:.3f}s")

    # Stage 7: Event Analysis
    analyze_events(t, x, bg_clean, events, global_noise, cfg)

    logger.debug(f"Total processing time: {detection_time - start_time:.3f}s")

    # Stage 6: Visualization
    plot = create_oscilloscope_plot(
        t,
        x,
        bg_initial,
        bg_clean,
        events,
        detection_threshold,
        keep_threshold,
        name,
        cfg,
        smooth_n,
        global_noise=global_noise,
    )
    
    # Save plots
    data_dir = cfg.get("DATA_PATH", "")
    analysis_dir = data_dir[:-1] if data_dir.endswith("/") else data_dir
    analysis_dir += "_analysis/"
    if not os.path.exists(analysis_dir):
        os.makedirs(analysis_dir)
    plot.save(analysis_dir + f"osc_{name}.png")

    # Create event plotter
    event_plotter = EventPlotter(
        plot, events, bg_clean=bg_clean, global_noise=global_noise, y_scale_mode=yscale_mode
    )
    event_plotter.plot_events_grid(max_events=16)
    event_plotter.save(analysis_dir + f"events_{name}_events.png")
    
    if show_plots:
        plt.show(block=True)
```

## Update: `src/pyosc/waveform/__init__.py`

```python
"""
Waveform-specific components for PyOSC.

This package contains components specific to waveform data processing and visualization.
"""

from pyosc.waveform.analysis import configure_logging, process_file
from pyosc.waveform.event_detector import detect_events, merge_overlapping_events
from pyosc.waveform.event_plotter import EventPlotter
from pyosc.waveform.io import get_waveform_params, rd

__all__ = [
    "get_waveform_params",
    "rd",
    "detect_events",
    "merge_overlapping_events",
    "EventPlotter",
    "configure_logging",
    "process_file",
]
```

Now the scripts can be dramatically simplified to just contain configuration and matplotlib setup. Would you like me to show the simplified script versions next?

> Tokens: 37k sent, 7.1k received. Cost: $0.22 message, $0.49 session.  
> Warning: it's best to only add files that need changes to the chat.  
> https://aider.chat/docs/troubleshooting/edit-errors.html  

#### please implement those lib changes first please, we'll do the scripts next  

I'll create the new analysis module and update the waveform package's `__init__.py` file.

src/pyosc/waveform/analysis.py
```python
<<<<<<< SEARCH
=======
import os
import sys
import time
from typing import Any, Dict, List, Optional, Tuple

import matplotlib.pyplot as plt
import numpy as np
from loguru import logger
from numba import njit
from scipy.interpolate import interp1d
from scipy.signal import savgol_filter

from pyosc.oscplot.plot import OscilloscopePlot
from pyosc.waveform.event_detector import (
    MEDIAN_TO_STD_FACTOR,
    detect_events,
    merge_overlapping_events,
)
from pyosc.waveform.event_plotter import EventPlotter
from pyosc.waveform.io import rd


@njit
def _create_event_mask_numba(t: np.ndarray, events: np.ndarray) -> np.ndarray:
    """
    Create a boolean mask to exclude event regions using numba for speed.
    
    Parameters
    ----------
    t : np.ndarray
        Time array.
    events : np.ndarray
        Events array with shape (n_events, 2) where each row is [t_start, t_end].
    
    Returns
    -------
    np.ndarray
        Boolean mask where True means keep the sample, False means exclude.
    """
    mask = np.ones(len(t), dtype=np.bool_)
    
    for i in range(len(events)):
        t_start = events[i, 0]
        t_end = events[i, 1]
        
        # Find indices where time is within event bounds
        for j in range(len(t)):
            if t_start <= t[j] < t_end:
                mask[j] = False
    
    return mask


def configure_logging(log_level: str = "INFO") -> None:
    """
    Configure loguru logging with specified level.

    Parameters
    ----------
    log_level : str, default="INFO"
        Logging level: DEBUG, INFO, WARNING, ERROR, CRITICAL.
    """
    logger.remove()
    logger.add(
        sys.stderr,
        level=log_level.upper(),
        format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <level>{message}</level>",
        colorize=True,
    )


def load_data(
    name: str,
    cfg: Dict[str, Any],
    sidecar: Optional[str] = None,
    crop: Optional[List[int]] = None,
) -> Tuple[np.ndarray, np.ndarray]:
    """
    Stage 1: Load waveform data from file.

    Parameters
    ----------
    name : str
        Filename of the waveform data.
    cfg : Dict[str, Any]
        Configuration dictionary with SAMPLING_INTERVAL key.
    sidecar : str, optional
        XML sidecar filename.
    crop : List[int], optional
        Crop indices [start, end].

    Returns
    -------
    Tuple[np.ndarray, np.ndarray]
        Time and signal arrays.
    """
    logger.info(f"Loading data from {name}")
    t, x = rd(
        name,
        cfg["SAMPLING_INTERVAL"],
        data_path=cfg["DATA_PATH"],
        xml_filename=sidecar,
        crop=crop,
    )

    logger.debug(
        f"Signal statistics: min={np.min(x):.3g}, max={np.max(x):.3g}, mean={np.mean(x):.3g}, std={np.std(x):.3g}"
    )

    return t, x


def calculate_smoothing_parameters(cfg: Dict[str, Any]) -> Tuple[int, int]:
    """
    Calculate smoothing window size and minimum event length in samples.

    Parameters
    ----------
    cfg : Dict[str, Any]
        Configuration dictionary.

    Returns
    -------
    Tuple[int, int]
        Smoothing window size and minimum event length in samples.
    """
    if "SAMPLING_INTERVAL" not in cfg:
        raise KeyError("SAMPLING_INTERVAL must be set in configuration")

    if cfg["SMOOTH_WIN_T"] is not None:
        smooth_n = int(cfg["SMOOTH_WIN_T"] / cfg["SAMPLING_INTERVAL"])
    elif cfg["SMOOTH_WIN_F"] is not None:
        smooth_n = int(1 / (cfg["SMOOTH_WIN_F"] * cfg["SAMPLING_INTERVAL"]))
    else:
        raise ValueError("Set either SMOOTH_WIN_T or SMOOTH_WIN_F")

    if smooth_n % 2 == 0:
        smooth_n += 1

    min_event_n = int(cfg["MIN_EVENT_T"] / cfg["SAMPLING_INTERVAL"])

    smooth_freq_hz = 1 / (smooth_n * cfg["SAMPLING_INTERVAL"])
    logger.info(
        f"--Smooth window: {smooth_n} samples ({cfg['SMOOTH_WIN_T'] * 1e6:.1f} µs, {smooth_freq_hz:.1f} Hz)"
    )
    logger.info(
        f"--Min event length: {min_event_n} samples ({cfg['MIN_EVENT_T'] * 1e6:.1f} µs)"
    )
    logger.info(f"--Detection SNR: {cfg['DETECTION_SNR']}")
    logger.info(f"--Min keep SNR: {cfg['MIN_EVENT_KEEP_SNR']}")
    logger.info(f"--Widen fraction: {cfg['WIDEN_FRAC']}")
    logger.info(
        f"--Signal polarity: {cfg['SIGNAL_POLARITY']} ({'negative' if cfg['SIGNAL_POLARITY'] < 0 else 'positive'} events)"
    )

    return smooth_n, min_event_n


def calculate_initial_background(
    t: np.ndarray, x: np.ndarray, smooth_n: int
) -> np.ndarray:
    """
    Stage 2: Calculate initial background estimate.

    Parameters
    ----------
    t : np.ndarray
        Time array.
    x : np.ndarray
        Signal array.
    smooth_n : int
        Smoothing window size in samples.

    Returns
    -------
    np.ndarray
        Initial background estimate.
    """
    logger.info("Calculating initial background")
    bg_initial = savgol_filter(x, smooth_n, 3).astype(np.float32)
    logger.debug(
        f"Initial background: mean={np.mean(bg_initial):.3g}, std={np.std(bg_initial):.3g}"
    )
    return bg_initial


def estimate_noise(x: np.ndarray, bg_initial: np.ndarray) -> np.float32:
    """
    Stage 2: Estimate noise level.

    Parameters
    ----------
    x : np.ndarray
        Signal array.
    bg_initial : np.ndarray
        Initial background estimate.

    Returns
    -------
    np.float32
        Estimated noise level.
    """
    global_noise = np.float32(np.median(np.abs(x - bg_initial)) * MEDIAN_TO_STD_FACTOR)

    signal_rms = np.sqrt(np.mean(x**2))
    signal_range = np.max(x) - np.min(x)
    noise_pct_rms = 100 * global_noise / signal_rms if signal_rms > 0 else 0
    noise_pct_range = 100 * global_noise / signal_range if signal_range > 0 else 0

    logger.info(
        f"Global noise level: {global_noise:.3g} ({noise_pct_rms:.1f}% of RMS, {noise_pct_range:.1f}% of range)"
    )

    snr_estimate = np.std(x) / global_noise
    logger.info(f"Estimated signal SNR: {snr_estimate:.2f}")

    return global_noise


def detect_initial_events(
    t: np.ndarray,
    x: np.ndarray,
    bg_initial: np.ndarray,
    global_noise: np.float32,
    cfg: Dict[str, Any],
    min_event_n: int,
) -> np.ndarray:
    """
    Stage 3: Detect initial events.

    Parameters
    ----------
    t : np.ndarray
        Time array.
    x : np.ndarray
        Signal array.
    bg_initial : np.ndarray
        Initial background estimate.
    global_noise : np.float32
        Estimated noise level.
    cfg : Dict[str, Any]
        Configuration dictionary.
    min_event_n : int
        Minimum event length in samples.

    Returns
    -------
    np.ndarray
        Array of initial events.
    """
    logger.info("Detecting initial events")
    min_event_amp = np.float32(cfg["MIN_EVENT_KEEP_SNR"]) * global_noise

    logger.info(f"Detection threshold: {cfg['DETECTION_SNR']}σ below background")
    logger.info(f"Keep threshold: {cfg['MIN_EVENT_KEEP_SNR']}σ below background")
    logger.info(f"Min event amplitude threshold: {min_event_amp:.3g}")

    events_initial, _ = detect_events(
        t,
        x,
        bg_initial,
        snr_threshold=np.float32(cfg["DETECTION_SNR"]),
        min_event_len=min_event_n,
        min_event_amp=min_event_amp,
        widen_frac=np.float32(cfg["WIDEN_FRAC"]),
        global_noise=global_noise,
        signal_polarity=cfg["SIGNAL_POLARITY"],
    )

    logger.info(f"Found {len(events_initial)} initial events after filtering")

    events_initial = merge_overlapping_events(events_initial)
    logger.info(f"After merging: {len(events_initial)} events")

    return events_initial


def calculate_clean_background(
    t: np.ndarray,
    x: np.ndarray,
    events_initial: np.ndarray,
    smooth_n: int,
    bg_initial: np.ndarray,
) -> np.ndarray:
    """
    Stage 4: Calculate clean background by masking events.

    Parameters
    ----------
    t : np.ndarray
        Time array.
    x : np.ndarray
        Signal array.
    events_initial : np.ndarray
        Initial events array.
    smooth_n : int
        Smoothing window size in samples.
    bg_initial : np.ndarray
        Initial background estimate.

    Returns
    -------
    np.ndarray
        Clean background estimate.
    """
    logger.info("Calculating clean background")
    start_time = time.time()

    # Fast masking with numba
    mask = _create_event_mask_numba(t, events_initial)
    mask_time = time.time()

    logger.debug(
        f"Masked {np.sum(~mask)} samples ({100 * np.sum(~mask) / len(mask):.1f}%) for clean background"
    )

    t_masked = t[mask]
    x_masked = x[mask]

    if np.sum(mask) > 2 * smooth_n:
        interp_start = time.time()
        interp = interp1d(t_masked, x_masked, kind="linear", fill_value="extrapolate")
        x_interp = interp(t)
        interp_end = time.time()
        
        filter_start = time.time()
        bg_clean = savgol_filter(x_interp, smooth_n, 3).astype(np.float32)
        filter_end = time.time()
        
        logger.debug(f"Timing: mask={mask_time-start_time:.3f}s, interp={interp_end-interp_start:.3f}s, filter={filter_end-filter_start:.3f}s")
        logger.debug(
            f"Clean background: mean={np.mean(bg_clean):.3g}, std={np.std(bg_clean):.3g}"
        )
    else:
        logger.debug("Insufficient unmasked samples for clean background - using initial")
        bg_clean = bg_initial

    return bg_clean


def analyze_thresholds(
    x: np.ndarray, bg_clean: np.ndarray, global_noise: np.float32, cfg: Dict[str, Any]
) -> Tuple[np.ndarray, np.ndarray]:
    """
    Analyze threshold statistics and create threshold arrays.

    Parameters
    ----------
    x : np.ndarray
        Signal array.
    bg_clean : np.ndarray
        Clean background estimate.
    global_noise : np.float32
        Estimated noise level.
    cfg : Dict[str, Any]
        Configuration dictionary.

    Returns
    -------
    Tuple[np.ndarray, np.ndarray]
        Detection and keep threshold arrays.
    """
    logger.info("Analyzing thresholds")

    if cfg["SIGNAL_POLARITY"] < 0:
        detection_threshold = bg_clean - cfg["DETECTION_SNR"] * global_noise
        keep_threshold = bg_clean - cfg["MIN_EVENT_KEEP_SNR"] * global_noise
        below_detection_pct = 100 * np.sum(x < detection_threshold) / len(x)
        below_keep_pct = 100 * np.sum(x < keep_threshold) / len(x)
        logger.info(f"Samples below detection threshold: {below_detection_pct:.2f}%")
        logger.info(f"Samples below keep threshold: {below_keep_pct:.2f}%")
    else:
        detection_threshold = bg_clean + cfg["DETECTION_SNR"] * global_noise
        keep_threshold = bg_clean + cfg["MIN_EVENT_KEEP_SNR"] * global_noise
        above_detection_pct = 100 * np.sum(x > detection_threshold) / len(x)
        above_keep_pct = 100 * np.sum(x > keep_threshold) / len(x)
        logger.info(f"Samples above detection threshold: {above_detection_pct:.2f}%")
        logger.info(f"Samples above keep threshold: {above_keep_pct:.2f}%")

    return detection_threshold, keep_threshold


def detect_final_events(
    t: np.ndarray,
    x: np.ndarray,
    bg_clean: np.ndarray,
    global_noise: np.float32,
    cfg: Dict[str, Any],
    min_event_n: int,
) -> np.ndarray:
    """
    Stage 5: Detect final events using clean background.

    Parameters
    ----------
    t : np.ndarray
        Time array.
    x : np.ndarray
        Signal array.
    bg_clean : np.ndarray
        Clean background estimate.
    global_noise : np.float32
        Estimated noise level.
    cfg : Dict[str, Any]
        Configuration dictionary.
    min_event_n : int
        Minimum event length in samples.

    Returns
    -------
    np.ndarray
        Array of final events.
    """
    logger.info("Detecting final events")
    min_event_amp = np.float32(cfg["MIN_EVENT_KEEP_SNR"]) * global_noise

    events, noise = detect_events(
        t,
        x,
        bg_clean,
        snr_threshold=np.float32(cfg["DETECTION_SNR"]),
        min_event_len=min_event_n,
        min_event_amp=min_event_amp,
        widen_frac=np.float32(cfg["WIDEN_FRAC"]),
        global_noise=global_noise,
        signal_polarity=cfg["SIGNAL_POLARITY"],
    )

    events = merge_overlapping_events(events)
    logger.info(f"Detected {len(events)} final events")

    return events


def analyze_events(
    t: np.ndarray,
    x: np.ndarray,
    bg_clean: np.ndarray,
    events: np.ndarray,
    global_noise: np.float32,
    cfg: Dict[str, Any],
) -> None:
    """
    Stage 7: Analyze event characteristics.

    Parameters
    ----------
    t : np.ndarray
        Time array.
    x : np.ndarray
        Signal array.
    bg_clean : np.ndarray
        Clean background estimate.
    events : np.ndarray
        Events array.
    global_noise : np.float32
        Estimated noise level.
    cfg : Dict[str, Any]
        Configuration dictionary.
    """
    if len(events) == 0:
        logger.info("No events to analyze")
        return
    if len(events) > 1000:
        logger.warning(
            f"Detected {len(events)} events, which is more than 1000. Skipping analysis."
        )
        return

    event_durations = (events[:, 1] - events[:, 0]) * 1000000  # Convert to µs
    event_amplitudes = []

    for t_start, t_end in events:
        event_mask = (t >= t_start) & (t < t_end)
        if np.any(event_mask):
            if cfg["SIGNAL_POLARITY"] < 0:
                amp = np.min(x[event_mask] - bg_clean[event_mask])
            else:
                amp = np.max(x[event_mask] - bg_clean[event_mask])
            event_amplitudes.append(abs(amp))

    if event_amplitudes:
        logger.info(
            f"Event durations (µs): min={np.min(event_durations):.2f}, max={np.max(event_durations):.2f}, mean={np.mean(event_durations):.2f}"
        )
        logger.info(
            f"Event amplitudes: min={np.min(event_amplitudes):.3g}, max={np.max(event_amplitudes):.3g}, mean={np.mean(event_amplitudes):.3g}"
        )
        logger.info(
            f"Event amplitude SNRs: min={np.min(event_amplitudes) / global_noise:.2f}, max={np.max(event_amplitudes) / global_noise:.2f}"
        )

    final_signal_rms = np.sqrt(np.mean(x**2))
    final_noise_pct_rms = (
        100 * global_noise / final_signal_rms if final_signal_rms > 0 else 0
    )
    final_signal_range = np.max(x) - np.min(x)
    final_noise_pct_range = (
        100 * global_noise / final_signal_range if final_signal_range > 0 else 0
    )

    logger.info(
        f"Noise summary: {global_noise:.3g} ({final_noise_pct_rms:.1f}% of RMS, {final_noise_pct_range:.1f}% of range)"
    )


def create_oscilloscope_plot(
    t: np.ndarray,
    x: np.ndarray,
    bg_initial: np.ndarray,
    bg_clean: np.ndarray,
    events: np.ndarray,
    detection_threshold: np.ndarray,
    keep_threshold: np.ndarray,
    name: str,
    cfg: Dict[str, Any],
    smooth_n: int,
    global_noise: Optional[np.float32] = None,
) -> OscilloscopePlot:
    """
    Stage 6: Create oscilloscope plot with all visualization elements.

    Parameters
    ----------
    t : np.ndarray
        Time array.
    x : np.ndarray
        Signal array.
    bg_initial : np.ndarray
        Initial background estimate.
    bg_clean : np.ndarray
        Clean background estimate.
    events : np.ndarray
        Events array.
    detection_threshold : np.ndarray
        Detection threshold array.
    keep_threshold : np.ndarray
        Keep threshold array.
    name : str
        Name for the plot.
    cfg : Dict[str, Any]
        Configuration dictionary.
    smooth_n : int
        Smoothing window size in samples.
    global_noise : Optional[np.float32], default=None
        Estimated noise level. If provided, will be plotted as a ribbon.

    Returns
    -------
    OscilloscopePlot
        Configured oscilloscope plot.
    """
    logger.info("Creating visualization")

    plot_name = name
    if global_noise is not None:
        plot_signal_rms = np.sqrt(np.mean(x**2))
        plot_noise_pct_rms = (
            100 * global_noise / plot_signal_rms if plot_signal_rms > 0 else 0
        )
        plot_name = f"{name} | Global noise: {global_noise:.3g} ({plot_noise_pct_rms:.1f}% of RMS)"

    plot = OscilloscopePlot(
        t,
        x,
        name=plot_name,
        max_plot_points=cfg["MAX_PLOT_POINTS"],
        mode_switch_threshold=cfg["ENVELOPE_MODE_LIMIT"],
        envelope_window_samples=smooth_n,
    )

    plot.add_line(
        t,
        bg_clean,
        label="Background",
        color="orange",
        alpha=0.6,
        linewidth=1.5,
        display_mode=OscilloscopePlot.MODE_BOTH,
    )

    if global_noise is not None:
        plot.add_ribbon(
            t,
            bg_clean,
            global_noise,
            label="Noise (±1σ)",
            color="gray",
            alpha=0.3,
            display_mode=OscilloscopePlot.MODE_DETAIL,
        )

    plot.add_line(
        t,
        detection_threshold,
        label=f"Detection ({cfg['DETECTION_SNR']}σ)",
        color="red",
        alpha=0.7,
        linestyle=":",
        linewidth=1.5,
        display_mode=OscilloscopePlot.MODE_DETAIL,
    )

    plot.add_line(
        t,
        keep_threshold,
        label=f"Keep ({cfg['MIN_EVENT_KEEP_SNR']}σ)",
        color="darkred",
        alpha=0.7,
        linestyle="--",
        linewidth=1.5,
        display_mode=OscilloscopePlot.MODE_DETAIL,
    )

    if len(events) > 0:
        plot.add_regions(
            events,
            label="Events",
            color="crimson",
            alpha=0.4,
            display_mode=OscilloscopePlot.MODE_BOTH,
        )

    plot.render()
    return plot


def process_file(
    name: str,
    cfg: Dict[str, Any],
    sidecar: Optional[str] = None,
    crop: Optional[List[int]] = None,
    yscale_mode: str = "snr",
    show_plots: bool = True,
) -> None:
    """
    Process a single waveform file for event detection.

    Parameters
    ----------
    name : str
        Filename of the waveform data.
    cfg : Dict[str, Any]
        Configuration dictionary with SAMPLING_INTERVAL key.
    sidecar : str, optional
        XML sidecar filename.
    crop : List[int], optional
        Crop indices [start, end].
    yscale_mode : str, default="snr"
        Y-axis scaling mode for event plotter.
    show_plots : bool, default=True
        Whether to show plots interactively.
    """
    start_time = time.time()
    logger.info(f"Processing {name} with parameters:")

    # Stage 1: Load Data
    t, x = load_data(name, cfg, sidecar, crop)

    # Calculate parameters
    smooth_n, min_event_n = calculate_smoothing_parameters(cfg)

    # Stage 2: Background Calculation
    bg_initial = calculate_initial_background(t, x, smooth_n)
    global_noise = estimate_noise(x, bg_initial)

    # Stage 3: Initial Event Detection
    bg_time = time.time()
    logger.debug(f"Background calculation took {bg_time - start_time:.3f}s")
    events_initial = detect_initial_events(
        t, x, bg_initial, global_noise, cfg, min_event_n
    )

    # Stage 4: Clean Background Calculation
    bg_clean = calculate_clean_background(t, x, events_initial, smooth_n, bg_initial)

    # Analyze thresholds
    detection_threshold, keep_threshold = analyze_thresholds(
        x, bg_clean, global_noise, cfg
    )

    # Stage 5: Final Event Detection
    events = detect_final_events(t, x, bg_clean, global_noise, cfg, min_event_n)
    detection_time = time.time()
    logger.debug(f"Event detection took {detection_time - bg_time:.3f}s")

    # Stage 7: Event Analysis
    analyze_events(t, x, bg_clean, events, global_noise, cfg)

    logger.debug(f"Total processing time: {detection_time - start_time:.3f}s")

    # Stage 6: Visualization
    plot = create_oscilloscope_plot(
        t,
        x,
        bg_initial,
        bg_clean,
        events,
        detection_threshold,
        keep_threshold,
        name,
        cfg,
        smooth_n,
        global_noise=global_noise,
    )
    
    # Save plots
    data_dir = cfg.get("DATA_PATH", "")
    analysis_dir = data_dir[:-1] if data_dir.endswith("/") else data_dir
    analysis_dir += "_analysis/"
    if not os.path.exists(analysis_dir):
        os.makedirs(analysis_dir)
    plot.save(analysis_dir + f"osc_{name}.png")

    # Create event plotter
    event_plotter = EventPlotter(
        plot, events, bg_clean=bg_clean, global_noise=global_noise, y_scale_mode=yscale_mode
    )
    event_plotter.plot_events_grid(max_events=16)
    event_plotter.save(analysis_dir + f"events_{name}_events.png")
    
    if show_plots:
        plt.show(block=True)
>>>>>>> REPLACE
```

src/pyosc/waveform/__init__.py
```python
<<<<<<< SEARCH
"""
Waveform-specific components for PyOSC.

This package contains components specific to waveform data processing and visualization.
"""

from pyosc.waveform.event_detector import detect_events, merge_overlapping_events
from pyosc.waveform.event_plotter import EventPlotter
from pyosc.waveform.io import get_waveform_params, rd

__all__ = [
    "get_waveform_params",
    "rd",
    "detect_events",
    "merge_overlapping_events",
    "EventPlotter",
]
=======
"""
Waveform-specific components for PyOSC.

This package contains components specific to waveform data processing and visualization.
"""

from pyosc.waveform.analysis import configure_logging, process_file
from pyosc.waveform.event_detector import detect_events, merge_overlapping_events
from pyosc.waveform.event_plotter import EventPlotter
from pyosc.waveform.io import get_waveform_params, rd

__all__ = [
    "get_waveform_params",
    "rd",
    "detect_events",
    "merge_overlapping_events",
    "EventPlotter",
    "configure_logging",
    "process_file",
]
>>>>>>> REPLACE
```

The library changes are now complete! The new `analysis.py` module includes:

1. **Numba-optimised masking** with `_create_event_mask_numba()` for faster event masking
2. **All shared analysis functions** extracted from the scripts
3. **Timing information** for the clean background calculation to help identify bottlenecks
4. **Configurable plot display** with the `show_plots` parameter

The `__init__.py` file has been updated to export the new functions. Now the scripts can be dramatically simplified to just contain configuration and matplotlib setup.

> Tokens: 46k sent, 7.4k received. Cost: $0.25 message, $0.74 session.  
> src/pyosc/waveform/analysis.py  
> Create new file? (Y)es/(N)o [Yes]: y  
> Applied edit to src/pyosc/waveform/analysis.py  
> Applied edit to src/pyosc/waveform/__init__.py  
> Commit 988a4e0 The changes look good! The new `analysis.py` module consolidates all the shared analysis functions from the scripts and introduces some optimizations like the numba-accelerated event masking.

Would you like me to help you simplify the scripts next to use these new functions? The scripts should now be much more concise, focusing mainly on configuration and matplotlib setup.  
> You can use /undo to undo and discard each aider commit.  

#### please implement that changes for the scripts  

I'll simplify both scripts to use the new analysis functions from the library. Here are the changes:

scripts/run_analysis.py
```python
<<<<<<< SEARCH
import sys
import time
from typing import Any, Dict, List, Optional, Tuple
from warnings import warn
import os

import matplotlib as mpl
import matplotlib.pyplot as plt
import numpy as np
from loguru import logger
from scipy.signal import savgol_filter

# Import from your library structure
from pyosc.oscplot.plot import OscilloscopePlot
from pyosc.waveform.event_detector import (
    MEDIAN_TO_STD_FACTOR,
    detect_events,
    merge_overlapping_events,
)
from pyosc.waveform.event_plotter import EventPlotter
from pyosc.waveform.io import get_waveform_params, rd
=======
from warnings import warn

import matplotlib as mpl

from pyosc.waveform import configure_logging, get_waveform_params, process_file
>>>>>>> REPLACE
```

scripts/run_analysis.py
```python
<<<<<<< SEARCH
# --- Stage functions for clear separation of logical stages ---


def load_data(
    name: str,
    cfg: Dict[str, Any],
    sidecar: Optional[str] = None,
    crop: Optional[List[int]] = None,
) -> Tuple[np.ndarray, np.ndarray]:
    """
    Stage 1: Load waveform data from file.

    Parameters
    ----------
    name : str
        Filename of the waveform data.
    cfg : Dict[str, Any]
        Configuration dictionary with SAMPLING_INTERVAL key.
    sidecar : str, optional
        XML sidecar filename.
    crop : List[int], optional
        Crop indices [start, end].

    Returns
    -------
    Tuple[np.ndarray, np.ndarray]
        Time and signal arrays.
    """
    logger.info(f"Loading data from {name}")
    t, x = rd(
        name,
        cfg["SAMPLING_INTERVAL"],
        data_path=cfg["DATA_PATH"],
        xml_filename=sidecar,
        crop=crop,
    )

    # Debug: Signal statistics
    logger.debug(
        f"Signal statistics: min={np.min(x):.3g}, max={np.max(x):.3g}, mean={np.mean(x):.3g}, std={np.std(x):.3g}"
    )

    return t, x


def calculate_smoothing_parameters(cfg: Dict[str, Any]) -> Tuple[int, int]:
    """
    Calculate smoothing window size and minimum event length in samples.

    Parameters
    ----------
    cfg : Dict[str, Any]
        Configuration dictionary.

    Returns
    -------
    Tuple[int, int]
        Smoothing window size and minimum event length in samples.
    """
    if "SAMPLING_INTERVAL" not in cfg:
        raise KeyError("SAMPLING_INTERVAL must be set in configuration")

    if cfg["SMOOTH_WIN_T"] is not None:
        smooth_n = int(cfg["SMOOTH_WIN_T"] / cfg["SAMPLING_INTERVAL"])
    elif cfg["SMOOTH_WIN_F"] is not None:
        smooth_n = int(1 / (cfg["SMOOTH_WIN_F"] * cfg["SAMPLING_INTERVAL"]))
    else:
        raise ValueError("Set either SMOOTH_WIN_T or SMOOTH_WIN_F")

    if smooth_n % 2 == 0:
        smooth_n += 1

    min_event_n = int(cfg["MIN_EVENT_T"] / cfg["SAMPLING_INTERVAL"])

    # Processing parameters - users should see what parameters are being used
    smooth_freq_hz = 1 / (smooth_n * cfg["SAMPLING_INTERVAL"])
    logger.info(
        f"--Smooth window: {smooth_n} samples ({cfg['SMOOTH_WIN_T'] * 1e6:.1f} µs, {smooth_freq_hz:.1f} Hz)"
    )
    logger.info(
        f"--Min event length: {min_event_n} samples ({cfg['MIN_EVENT_T'] * 1e6:.1f} µs)"
    )
    logger.info(f"--Detection SNR: {cfg['DETECTION_SNR']}")
    logger.info(f"--Min keep SNR: {cfg['MIN_EVENT_KEEP_SNR']}")
    logger.info(f"--Widen fraction: {cfg['WIDEN_FRAC']}")
    logger.info(
        f"--Signal polarity: {cfg['SIGNAL_POLARITY']} ({'negative' if cfg['SIGNAL_POLARITY'] < 0 else 'positive'} events)"
    )

    return smooth_n, min_event_n


def calculate_initial_background(
    t: np.ndarray, x: np.ndarray, smooth_n: int
) -> np.ndarray:
    """
    Stage 2: Calculate initial background estimate.

    Parameters
    ----------
    t : np.ndarray
        Time array.
    x : np.ndarray
        Signal array.
    smooth_n : int
        Smoothing window size in samples.

    Returns
    -------
    np.ndarray
        Initial background estimate.
    """
    logger.info("Calculating initial background")
    bg_initial = savgol_filter(x, smooth_n, 3).astype(np.float32)
    logger.debug(
        f"Initial background: mean={np.mean(bg_initial):.3g}, std={np.std(bg_initial):.3g}"
    )
    return bg_initial


def estimate_noise(x: np.ndarray, bg_initial: np.ndarray) -> np.float32:
    """
    Stage 2: Estimate noise level.

    Parameters
    ----------
    x : np.ndarray
        Signal array.
    bg_initial : np.ndarray
        Initial background estimate.

    Returns
    -------
    np.float32
        Estimated noise level.
    """
    global_noise = np.float32(np.median(np.abs(x - bg_initial)) * MEDIAN_TO_STD_FACTOR)

    # Background and noise analysis - key metrics for users
    signal_rms = np.sqrt(np.mean(x**2))
    signal_range = np.max(x) - np.min(x)
    noise_pct_rms = 100 * global_noise / signal_rms if signal_rms > 0 else 0
    noise_pct_range = 100 * global_noise / signal_range if signal_range > 0 else 0

    logger.info(
        f"Global noise level: {global_noise:.3g} ({noise_pct_rms:.1f}% of RMS, {noise_pct_range:.1f}% of range)"
    )

    # Signal-to-noise ratio estimate
    snr_estimate = np.std(x) / global_noise
    logger.info(f"Estimated signal SNR: {snr_estimate:.2f}")

    return global_noise


def detect_initial_events(
    t: np.ndarray,
    x: np.ndarray,
    bg_initial: np.ndarray,
    global_noise: np.float32,
    cfg: Dict[str, Any],
    min_event_n: int,
) -> np.ndarray:
    """
    Stage 3: Detect initial events.

    Parameters
    ----------
    t : np.ndarray
        Time array.
    x : np.ndarray
        Signal array.
    bg_initial : np.ndarray
        Initial background estimate.
    global_noise : np.float32
        Estimated noise level.
    cfg : Dict[str, Any]
        Configuration dictionary.
    min_event_n : int
        Minimum event length in samples.

    Returns
    -------
    np.ndarray
        Array of initial events.
    """
    logger.info("Detecting initial events")
    min_event_amp = np.float32(cfg["MIN_EVENT_KEEP_SNR"]) * global_noise

    logger.info(f"Detection threshold: {cfg['DETECTION_SNR']}σ below background")
    logger.info(f"Keep threshold: {cfg['MIN_EVENT_KEEP_SNR']}σ below background")
    logger.info(f"Min event amplitude threshold: {min_event_amp:.3g}")

    events_initial, _ = detect_events(
        t,
        x,
        bg_initial,
        snr_threshold=np.float32(cfg["DETECTION_SNR"]),
        min_event_len=min_event_n,
        min_event_amp=min_event_amp,
        widen_frac=np.float32(cfg["WIDEN_FRAC"]),
        global_noise=global_noise,
        signal_polarity=cfg["SIGNAL_POLARITY"],
    )

    # Log number of initial events
    logger.info(f"Found {len(events_initial)} initial events after filtering")

    events_initial = merge_overlapping_events(events_initial)
    # Log number of merged events
    logger.info(f"After merging: {len(events_initial)} events")

    return events_initial


def calculate_clean_background(
    t: np.ndarray,
    x: np.ndarray,
    events_initial: np.ndarray,
    smooth_n: int,
    bg_initial: np.ndarray,
) -> np.ndarray:
    """
    Stage 4: Calculate clean background by masking events.

    Parameters
    ----------
    t : np.ndarray
        Time array.
    x : np.ndarray
        Signal array.
    events_initial : np.ndarray
        Initial events array.
    smooth_n : int
        Smoothing window size in samples.
    bg_initial : np.ndarray
        Initial background estimate.

    Returns
    -------
    np.ndarray
        Clean background estimate.
    """
    logger.info("Calculating clean background")
    start_time = time.time()
    # Create mask to exclude events
    mask = np.ones_like(x, dtype=bool)
    for t_start, t_end in events_initial:
        event_mask = (t >= t_start) & (t < t_end)
        mask[event_mask] = False
    mask_time = time.time()

    # Debug: Masking statistics
    logger.debug(
        f"Masked {np.sum(~mask)} samples ({100 * np.sum(~mask) / len(mask):.1f}%) for clean background"
    )

    t_masked = t[mask]
    x_masked = x[mask]

    if np.sum(mask) > 2 * smooth_n:
        from scipy.interpolate import interp1d

        interp_start = time.time()
        interp = interp1d(t_masked, x_masked, kind="linear", fill_value="extrapolate")
        x_interp = interp(t)
        interp_end = time.time()
        filter_start = time.time()
        bg_clean = savgol_filter(x_interp, smooth_n, 3).astype(np.float32)
        filter_end = time.time()
        logger.success(f"Timing: mask={mask_time-start_time:.3f}s, interp={interp_end-interp_start:.3f}s,"
                          f" filter={filter_end-filter_start:.3f}s")
        logger.debug(
            f"Clean background: mean={np.mean(bg_clean):.3g}, std={np.std(bg_clean):.3g}"
        )
    else:
        logger.debug(
            "Insufficient unmasked samples for clean background - using initial"
        )
        bg_clean = bg_initial

    return bg_clean


def analyze_thresholds(
    x: np.ndarray, bg_clean: np.ndarray, global_noise: np.float32, cfg: Dict[str, Any]
) -> Tuple[np.ndarray, np.ndarray]:
    """
    Analyze threshold statistics and create threshold arrays.

    Parameters
    ----------
    x : np.ndarray
        Signal array.
    bg_clean : np.ndarray
        Clean background estimate.
    global_noise : np.float32
        Estimated noise level.
    cfg : Dict[str, Any]
        Configuration dictionary.

    Returns
    -------
    Tuple[np.ndarray, np.ndarray]
        Detection and keep threshold arrays.
    """
    logger.info("Analyzing thresholds")

    if cfg["SIGNAL_POLARITY"] < 0:
        detection_threshold = bg_clean - cfg["DETECTION_SNR"] * global_noise
        keep_threshold = bg_clean - cfg["MIN_EVENT_KEEP_SNR"] * global_noise
        below_detection_pct = 100 * np.sum(x < detection_threshold) / len(x)
        below_keep_pct = 100 * np.sum(x < keep_threshold) / len(x)
        logger.info(f"Samples below detection threshold: {below_detection_pct:.2f}%")
        logger.info(f"Samples below keep threshold: {below_keep_pct:.2f}%")
    else:
        detection_threshold = bg_clean + cfg["DETECTION_SNR"] * global_noise
        keep_threshold = bg_clean + cfg["MIN_EVENT_KEEP_SNR"] * global_noise
        above_detection_pct = 100 * np.sum(x > detection_threshold) / len(x)
        above_keep_pct = 100 * np.sum(x > keep_threshold) / len(x)
        logger.info(f"Samples above detection threshold: {above_detection_pct:.2f}%")
        logger.info(f"Samples above keep threshold: {above_keep_pct:.2f}%")

    return detection_threshold, keep_threshold


def detect_final_events(
    t: np.ndarray,
    x: np.ndarray,
    bg_clean: np.ndarray,
    global_noise: np.float32,
    cfg: Dict[str, Any],
    min_event_n: int,
) -> np.ndarray:
    """
    Stage 5: Detect final events using clean background.

    Parameters
    ----------
    t : np.ndarray
        Time array.
    x : np.ndarray
        Signal array.
    bg_clean : np.ndarray
        Clean background estimate.
    global_noise : np.float32
        Estimated noise level.
    cfg : Dict[str, Any]
        Configuration dictionary.
    min_event_n : int
        Minimum event length in samples.

    Returns
    -------
    np.ndarray
        Array of final events.
    """
    logger.info("Detecting final events")
    min_event_amp = np.float32(cfg["MIN_EVENT_KEEP_SNR"]) * global_noise

    events, noise = detect_events(
        t,
        x,
        bg_clean,
        snr_threshold=np.float32(cfg["DETECTION_SNR"]),
        min_event_len=min_event_n,
        min_event_amp=min_event_amp,
        widen_frac=np.float32(cfg["WIDEN_FRAC"]),
        global_noise=global_noise,
        signal_polarity=cfg["SIGNAL_POLARITY"],
    )

    events = merge_overlapping_events(events)
    logger.info(f"Detected {len(events)} final events")

    return events


def analyze_events(
    t: np.ndarray,
    x: np.ndarray,
    bg_clean: np.ndarray,
    events: np.ndarray,
    global_noise: np.float32,
    cfg: Dict[str, Any],
) -> None:
    """
    Stage 7: Analyze event characteristics.

    Parameters
    ----------
    t : np.ndarray
        Time array.
    x : np.ndarray
        Signal array.
    bg_clean : np.ndarray
        Clean background estimate.
    events : np.ndarray
        Events array.
    global_noise : np.float32
        Estimated noise level.
    cfg : Dict[str, Any]
        Configuration dictionary.
    """
    if len(events) == 0:
        logger.info("No events to analyze")
        return

    # Event characteristics - users want to see final event properties
    event_durations = (events[:, 1] - events[:, 0]) * 1000000  # Convert to µs
    event_amplitudes = []

    for t_start, t_end in events:
        event_mask = (t >= t_start) & (t < t_end)
        if np.any(event_mask):
            if cfg["SIGNAL_POLARITY"] < 0:
                amp = np.min(x[event_mask] - bg_clean[event_mask])
            else:
                amp = np.max(x[event_mask] - bg_clean[event_mask])
            event_amplitudes.append(abs(amp))

    if event_amplitudes:
        logger.info(
            f"Event durations (µs): min={np.min(event_durations):.2f}, max={np.max(event_durations):.2f}, mean={np.mean(event_durations):.2f}"
        )
        logger.info(
            f"Event amplitudes: min={np.min(event_amplitudes):.3g}, max={np.max(event_amplitudes):.3g}, mean={np.mean(event_amplitudes):.3g}"
        )
        logger.info(
            f"Event amplitude SNRs: min={np.min(event_amplitudes) / global_noise:.2f}, max={np.max(event_amplitudes) / global_noise:.2f}"
        )

    # Calculate relative noise for final summary
    final_signal_rms = np.sqrt(np.mean(x**2))
    final_noise_pct_rms = (
        100 * global_noise / final_signal_rms if final_signal_rms > 0 else 0
    )
    final_signal_range = np.max(x) - np.min(x)
    final_noise_pct_range = (
        100 * global_noise / final_signal_range if final_signal_range > 0 else 0
    )

    logger.info(
        f"Noise summary: {global_noise:.3g} ({final_noise_pct_rms:.1f}% of RMS, {final_noise_pct_range:.1f}% of range)"
    )


def create_oscilloscope_plot(
    t: np.ndarray,
    x: np.ndarray,
    bg_initial: np.ndarray,  # bg_initial is not used in this function, but kept for consistency with previous calls
    bg_clean: np.ndarray,
    events: np.ndarray,
    detection_threshold: np.ndarray,
    keep_threshold: np.ndarray,
    name: str,
    cfg: Dict[str, Any],
    smooth_n: int,
    global_noise: Optional[np.float32] = None,  # Make global_noise optional
) -> OscilloscopePlot:
    """
    Stage 6: Create oscilloscope plot with all visualization elements.

    Parameters
    ----------
    t : np.ndarray
        Time array.
    x : np.ndarray
        Signal array.
    bg_initial : np.ndarray
        Initial background estimate.
    bg_clean : np.ndarray
        Clean background estimate.
    events : np.ndarray
        Events array.
    detection_threshold : np.ndarray
        Detection threshold array.
    keep_threshold : np.ndarray
        Keep threshold array.
    name : str
        Name for the plot.
    cfg : Dict[str, Any]
        Configuration dictionary.
    smooth_n : int
        Smoothing window size in samples.
    global_noise : Optional[np.float32], default=None
        Estimated noise level. If provided, will be plotted as a ribbon.

    Returns
    -------
    OscilloscopePlot
        Configured oscilloscope plot.
    """
    logger.info("Creating visualization")

    # Create the oscilloscope plot instance
    plot_name = name
    if global_noise is not None:
        plot_signal_rms = np.sqrt(np.mean(x**2))
        plot_noise_pct_rms = (
            100 * global_noise / plot_signal_rms if plot_signal_rms > 0 else 0
        )
        plot_name = f"{name} | Global noise: {global_noise:.3g} ({plot_noise_pct_rms:.1f}% of RMS)"

    plot = OscilloscopePlot(
        t,
        x,
        name=plot_name,
        max_plot_points=cfg["MAX_PLOT_POINTS"],
        mode_switch_threshold=cfg["ENVELOPE_MODE_LIMIT"],
        envelope_window_samples=smooth_n,
    )

    # Add background line
    plot.add_line(
        t,
        bg_clean,
        label="Background",
        color="orange",
        alpha=0.6,
        linewidth=1.5,
        display_mode=OscilloscopePlot.MODE_BOTH,
    )

    # Add noise ribbon if global_noise is provided
    if global_noise is not None:
        plot.add_ribbon(
            t,
            bg_clean,
            global_noise,
            label="Noise (±1σ)",
            color="gray",
            alpha=0.3,
            display_mode=OscilloscopePlot.MODE_DETAIL,
        )

    # Add detection threshold
    plot.add_line(
        t,
        detection_threshold,
        label=f"Detection ({cfg['DETECTION_SNR']}σ)",
        color="red",
        alpha=0.7,
        linestyle=":",
        linewidth=1.5,
        display_mode=OscilloscopePlot.MODE_DETAIL,
    )

    # Add keep threshold
    plot.add_line(
        t,
        keep_threshold,
        label=f"Keep ({cfg['MIN_EVENT_KEEP_SNR']}σ)",
        color="darkred",
        alpha=0.7,
        linestyle="--",
        linewidth=1.5,
        display_mode=OscilloscopePlot.MODE_DETAIL,
    )

    # Add events as regions
    if len(events) > 0:
        plot.add_regions(
            events,
            label="Events",
            color="crimson",
            alpha=0.4,
            display_mode=OscilloscopePlot.MODE_BOTH,
        )

    # Render the plot after all elements have been added
    plot.render()

    return plot


def process_file(
    name: str,
    cfg: Dict[str, Any],
    sidecar: Optional[str] = None,
    crop: Optional[List[int]] = None,
    yscale_mode: str = "snr",
) -> None:
    """
    Process a single waveform file for event detection.

    Parameters
    ----------
    name : str
        Filename of the waveform data.
    cfg : Dict[str, Any]
        Configuration dictionary with SAMPLING_INTERVAL key.
    sidecar : str, optional
        XML sidecar filename.
    crop : List[int], optional
        Crop indices [start, end].

    Raises
    ------
    ValueError
        If neither SMOOTH_WIN_T nor SMOOTH_WIN_F is set.
    KeyError
        If SAMPLING_INTERVAL is not in cfg.
    """
    start_time = time.time()
    logger.info(f"Processing {name} with parameters:")

    # Stage 1: Load Data
    t, x = load_data(name, cfg, sidecar, crop)

    # Calculate parameters
    smooth_n, min_event_n = calculate_smoothing_parameters(cfg)

    # Stage 2: Background Calculation
    bg_initial = calculate_initial_background(t, x, smooth_n)
    global_noise = estimate_noise(x, bg_initial)

    # Stage 3: Initial Event Detection
    bg_time = time.time()
    logger.debug(f"Background calculation took {bg_time - start_time:.3f}s")
    events_initial = detect_initial_events(
        t, x, bg_initial, global_noise, cfg, min_event_n
    )

    # Stage 4: Clean Background Calculation
    bg_clean = calculate_clean_background(t, x, events_initial, smooth_n, bg_initial)

    # Analyze thresholds
    detection_threshold, keep_threshold = analyze_thresholds(
        x, bg_clean, global_noise, cfg
    )

    # Stage 5: Final Event Detection
    events = detect_final_events(t, x, bg_clean, global_noise, cfg, min_event_n)

    detection_time = time.time()
    logger.debug(f"Event detection took {detection_time - bg_time:.3f}s")

    # Stage 7: Event Analysis
    analyze_events(t, x, bg_clean, events, global_noise, cfg)

    logger.debug(f"Total processing time: {detection_time - start_time:.3f}s")

    # Stage 6: Visualization

    plot = create_oscilloscope_plot(
        t,
        x,
        bg_initial,
        bg_clean,
        events,
        detection_threshold,
        keep_threshold,
        name,
        cfg,
        smooth_n,
        global_noise=global_noise,  # Pass global_noise as an explicit argument
    )
    data_dir = cfg.get("DATA_PATH", "")
    analysis_dir = data_dir[:-1] if data_dir.endswith("/") else data_dir
    analysis_dir += "_analysis/"
    if not os.path.exists(analysis_dir):
        os.makedirs(analysis_dir)
    plot.save(analysis_dir + f"osc_{name}.png")
    plt.show(block=True)

    # Create event plotter for detailed event analysis with percentage contrast
    event_plotter = EventPlotter(
        plot, events, bg_clean=bg_clean, global_noise=global_noise, y_scale_mode=yscale_mode
    )
    event_plotter.plot_events_grid(max_events=16)
    event_plotter.save(analysis_dir + f"events_{name}_events.png")
    plt.show(block=True)

def configure_logging(log_level: str = "INFO") -> None:
    """
    Configure loguru logging with specified level.

    Parameters
    ----------
    log_level : str, default="INFO"
        Logging level: DEBUG, INFO, WARNING, ERROR, CRITICAL.
    """
    # Remove default handler and add our own with specified level
    logger.remove()
    logger.add(
        sys.stderr,
        level=log_level.upper(),
        format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <level>{message}</level>",
        colorize=True,
    )
=======
>>>>>>> REPLACE
```

scripts/run_analysis.py
```python
<<<<<<< SEARCH
def main() -> None:
    """
    Main function to process all measurements.
    """
    cfg = CONFIG.copy()  # Create a copy to avoid modifying the original

    # Configure logging
    configure_logging(cfg.get("LOG_LEVEL", "INFO"))

    for measurement in cfg["MEASUREMENTS"]:
        name = measurement["data"]
        sidecar = measurement.get("sidecar")
        if sidecar:
            params = get_waveform_params(
                name, data_path=cfg["DATA_PATH"], xml_filename=sidecar
            )
            cfg["SAMPLING_INTERVAL"] = params["sampling_interval"]
        else:
            params = get_waveform_params(name, data_path=cfg["DATA_PATH"])
            cfg["SAMPLING_INTERVAL"] = params["sampling_interval"]
        process_file(name, cfg, sidecar=sidecar, crop=measurement.get("crop"), yscale_mode=cfg.get("YSCALE_MODE", "snr"))
=======
def main() -> None:
    """
    Main function to process all measurements.
    """
    cfg = CONFIG.copy()  # Create a copy to avoid modifying the original

    # Configure logging
    configure_logging(cfg.get("LOG_LEVEL", "INFO"))

    for measurement in cfg["MEASUREMENTS"]:
        name = measurement["data"]
        sidecar = measurement.get("sidecar")
        if sidecar:
            params = get_waveform_params(
                name, data_path=cfg["DATA_PATH"], xml_filename=sidecar
            )
            cfg["SAMPLING_INTERVAL"] = params["sampling_interval"]
        else:
            params = get_waveform_params(name, data_path=cfg["DATA_PATH"])
            cfg["SAMPLING_INTERVAL"] = params["sampling_interval"]
        process_file(
            name, 
            cfg, 
            sidecar=sidecar, 
            crop=measurement.get("crop"), 
            yscale_mode=cfg.get("YSCALE_MODE", "snr"),
            show_plots=True
        )
>>>>>>> REPLACE
```

scripts/run_analysis03.py
```python
<<<<<<< SEARCH
import sys
import time
from typing import Any, Dict, List, Optional, Tuple
from warnings import warn
import os

import matplotlib as mpl
import matplotlib.pyplot as plt
import numpy as np
from loguru import logger
from scipy.signal import savgol_filter

# Import from your library structure
from pyosc.oscplot.plot import OscilloscopePlot
from pyosc.waveform.event_detector import (
    MEDIAN_TO_STD_FACTOR,
    detect_events,
    merge_overlapping_events,
)
from pyosc.waveform.event_plotter import EventPlotter
from pyosc.waveform.io import get_waveform_params, rd
=======
from warnings import warn

import matplotlib as mpl

from pyosc.waveform import configure_logging, get_waveform_params, process_file
>>>>>>> REPLACE
```

scripts/run_analysis03.py
```python
<<<<<<< SEARCH
# --- Stage functions for clear separation of logical stages ---


def load_data(
    name: str,
    cfg: Dict[str, Any],
    sidecar: Optional[str] = None,
    crop: Optional[List[int]] = None,
) -> Tuple[np.ndarray, np.ndarray]:
    """
    Stage 1: Load waveform data from file.

    Parameters
    ----------
    name : str
        Filename of the waveform data.
    cfg : Dict[str, Any]
        Configuration dictionary with SAMPLING_INTERVAL key.
    sidecar : str, optional
        XML sidecar filename.
    crop : List[int], optional
        Crop indices [start, end].

    Returns
    -------
    Tuple[np.ndarray, np.ndarray]
        Time and signal arrays.
    """
    logger.info(f"Loading data from {name}")
    t, x = rd(
        name,
        cfg["SAMPLING_INTERVAL"],
        data_path=cfg["DATA_PATH"],
        xml_filename=sidecar,
        crop=crop,
    )

    # Debug: Signal statistics
    logger.debug(
        f"Signal statistics: min={np.min(x):.3g}, max={np.max(x):.3g}, mean={np.mean(x):.3g}, std={np.std(x):.3g}"
    )

    return t, x


def calculate_smoothing_parameters(cfg: Dict[str, Any]) -> Tuple[int, int]:
    """
    Calculate smoothing window size and minimum event length in samples.

    Parameters
    ----------
    cfg : Dict[str, Any]
        Configuration dictionary.

    Returns
    -------
    Tuple[int, int]
        Smoothing window size and minimum event length in samples.
    """
    if "SAMPLING_INTERVAL" not in cfg:
        raise KeyError("SAMPLING_INTERVAL must be set in configuration")

    if cfg["SMOOTH_WIN_T"] is not None:
        smooth_n = int(cfg["SMOOTH_WIN_T"] / cfg["SAMPLING_INTERVAL"])
    elif cfg["SMOOTH_WIN_F"] is not None:
        smooth_n = int(1 / (cfg["SMOOTH_WIN_F"] * cfg["SAMPLING_INTERVAL"]))
    else:
        raise ValueError("Set either SMOOTH_WIN_T or SMOOTH_WIN_F")

    if smooth_n % 2 == 0:
        smooth_n += 1

    min_event_n = int(cfg["MIN_EVENT_T"] / cfg["SAMPLING_INTERVAL"])

    # Processing parameters - users should see what parameters are being used
    smooth_freq_hz = 1 / (smooth_n * cfg["SAMPLING_INTERVAL"])
    logger.info(
        f"--Smooth window: {smooth_n} samples ({cfg['SMOOTH_WIN_T'] * 1e6:.1f} µs, {smooth_freq_hz:.1f} Hz)"
    )
    logger.info(
        f"--Min event length: {min_event_n} samples ({cfg['MIN_EVENT_T'] * 1e6:.1f} µs)"
    )
    logger.info(f"--Detection SNR: {cfg['DETECTION_SNR']}")
    logger.info(f"--Min keep SNR: {cfg['MIN_EVENT_KEEP_SNR']}")
    logger.info(f"--Widen fraction: {cfg['WIDEN_FRAC']}")
    logger.info(
        f"--Signal polarity: {cfg['SIGNAL_POLARITY']} ({'negative' if cfg['SIGNAL_POLARITY'] < 0 else 'positive'} events)"
    )

    return smooth_n, min_event_n


def calculate_initial_background(
    t: np.ndarray, x: np.ndarray, smooth_n: int
) -> np.ndarray:
    """
    Stage 2: Calculate initial background estimate.

    Parameters
    ----------
    t : np.ndarray
        Time array.
    x : np.ndarray
        Signal array.
    smooth_n : int
        Smoothing window size in samples.

    Returns
    -------
    np.ndarray
        Initial background estimate.
    """
    logger.info("Calculating initial background")
    bg_initial = savgol_filter(x, smooth_n, 3).astype(np.float32)
    logger.debug(
        f"Initial background: mean={np.mean(bg_initial):.3g}, std={np.std(bg_initial):.3g}"
    )
    return bg_initial


def estimate_noise(x: np.ndarray, bg_initial: np.ndarray) -> np.float32:
    """
    Stage 2: Estimate noise level.

    Parameters
    ----------
    x : np.ndarray
        Signal array.
    bg_initial : np.ndarray
        Initial background estimate.

    Returns
    -------
    np.float32
        Estimated noise level.
    """
    global_noise = np.float32(np.median(np.abs(x - bg_initial)) * MEDIAN_TO_STD_FACTOR)

    # Background and noise analysis - key metrics for users
    signal_rms = np.sqrt(np.mean(x**2))
    signal_range = np.max(x) - np.min(x)
    noise_pct_rms = 100 * global_noise / signal_rms if signal_rms > 0 else 0
    noise_pct_range = 100 * global_noise / signal_range if signal_range > 0 else 0

    logger.info(
        f"Global noise level: {global_noise:.3g} ({noise_pct_rms:.1f}% of RMS, {noise_pct_range:.1f}% of range)"
    )

    # Signal-to-noise ratio estimate
    snr_estimate = np.std(x) / global_noise
    logger.info(f"Estimated signal SNR: {snr_estimate:.2f}")

    return global_noise


def detect_initial_events(
    t: np.ndarray,
    x: np.ndarray,
    bg_initial: np.ndarray,
    global_noise: np.float32,
    cfg: Dict[str, Any],
    min_event_n: int,
) -> np.ndarray:
    """
    Stage 3: Detect initial events.

    Parameters
    ----------
    t : np.ndarray
        Time array.
    x : np.ndarray
        Signal array.
    bg_initial : np.ndarray
        Initial background estimate.
    global_noise : np.float32
        Estimated noise level.
    cfg : Dict[str, Any]
        Configuration dictionary.
    min_event_n : int
        Minimum event length in samples.

    Returns
    -------
    np.ndarray
        Array of initial events.
    """
    logger.info("Detecting initial events")
    min_event_amp = np.float32(cfg["MIN_EVENT_KEEP_SNR"]) * global_noise

    logger.info(f"Detection threshold: {cfg['DETECTION_SNR']}σ below background")
    logger.info(f"Keep threshold: {cfg['MIN_EVENT_KEEP_SNR']}σ below background")
    logger.info(f"Min event amplitude threshold: {min_event_amp:.3g}")

    events_initial, _ = detect_events(
        t,
        x,
        bg_initial,
        snr_threshold=np.float32(cfg["DETECTION_SNR"]),
        min_event_len=min_event_n,
        min_event_amp=min_event_amp,
        widen_frac=np.float32(cfg["WIDEN_FRAC"]),
        global_noise=global_noise,
        signal_polarity=cfg["SIGNAL_POLARITY"],
    )

    # Log number of initial events
    logger.info(f"Found {len(events_initial)} initial events after filtering")

    events_initial = merge_overlapping_events(events_initial)
    # Log number of merged events
    logger.info(f"After merging: {len(events_initial)} events")

    return events_initial


def calculate_clean_background(
    t: np.ndarray,
    x: np.ndarray,
    events_initial: np.ndarray,
    smooth_n: int,
    bg_initial: np.ndarray,
) -> np.ndarray:
    """
    Stage 4: Calculate clean background by masking events.

    Parameters
    ----------
    t : np.ndarray
        Time array.
    x : np.ndarray
        Signal array.
    events_initial : np.ndarray
        Initial events array.
    smooth_n : int
        Smoothing window size in samples.
    bg_initial : np.ndarray
        Initial background estimate.

    Returns
    -------
    np.ndarray
        Clean background estimate.
    """
    logger.info("Calculating clean background")

    # Create mask to exclude events
    mask = np.ones_like(x, dtype=bool)
    for t_start, t_end in events_initial:
        event_mask = (t >= t_start) & (t < t_end)
        mask[event_mask] = False

    # Debug: Masking statistics
    logger.debug(
        f"Masked {np.sum(~mask)} samples ({100 * np.sum(~mask) / len(mask):.1f}%) for clean background"
    )

    t_masked = t[mask]
    x_masked = x[mask]
    bg_clean = np.copy(bg_initial)

    if np.sum(mask) > 2 * smooth_n:
        from scipy.interpolate import interp1d

        interp = interp1d(t_masked, x_masked, kind="linear", fill_value="extrapolate")
        x_interp = interp(t)
        bg_clean = savgol_filter(x_interp, smooth_n, 3).astype(np.float32)
        logger.debug(
            f"Clean background: mean={np.mean(bg_clean):.3g}, std={np.std(bg_clean):.3g}"
        )
    else:
        logger.debug(
            "Insufficient unmasked samples for clean background - using initial"
        )

    return bg_clean


def analyze_thresholds(
    x: np.ndarray, bg_clean: np.ndarray, global_noise: np.float32, cfg: Dict[str, Any]
) -> Tuple[np.ndarray, np.ndarray]:
    """
    Analyze threshold statistics and create threshold arrays.

    Parameters
    ----------
    x : np.ndarray
        Signal array.
    bg_clean : np.ndarray
        Clean background estimate.
    global_noise : np.float32
        Estimated noise level.
    cfg : Dict[str, Any]
        Configuration dictionary.

    Returns
    -------
    Tuple[np.ndarray, np.ndarray]
        Detection and keep threshold arrays.
    """
    logger.info("Analyzing thresholds")

    if cfg["SIGNAL_POLARITY"] < 0:
        detection_threshold = bg_clean - cfg["DETECTION_SNR"] * global_noise
        keep_threshold = bg_clean - cfg["MIN_EVENT_KEEP_SNR"] * global_noise
        below_detection_pct = 100 * np.sum(x < detection_threshold) / len(x)
        below_keep_pct = 100 * np.sum(x < keep_threshold) / len(x)
        logger.info(f"Samples below detection threshold: {below_detection_pct:.2f}%")
        logger.info(f"Samples below keep threshold: {below_keep_pct:.2f}%")
    else:
        detection_threshold = bg_clean + cfg["DETECTION_SNR"] * global_noise
        keep_threshold = bg_clean + cfg["MIN_EVENT_KEEP_SNR"] * global_noise
        above_detection_pct = 100 * np.sum(x > detection_threshold) / len(x)
        above_keep_pct = 100 * np.sum(x > keep_threshold) / len(x)
        logger.info(f"Samples above detection threshold: {above_detection_pct:.2f}%")
        logger.info(f"Samples above keep threshold: {above_keep_pct:.2f}%")

    return detection_threshold, keep_threshold


def detect_final_events(
    t: np.ndarray,
    x: np.ndarray,
    bg_clean: np.ndarray,
    global_noise: np.float32,
    cfg: Dict[str, Any],
    min_event_n: int,
) -> np.ndarray:
    """
    Stage 5: Detect final events using clean background.

    Parameters
    ----------
    t : np.ndarray
        Time array.
    x : np.ndarray
        Signal array.
    bg_clean : np.ndarray
        Clean background estimate.
    global_noise : np.float32
        Estimated noise level.
    cfg : Dict[str, Any]
        Configuration dictionary.
    min_event_n : int
        Minimum event length in samples.

    Returns
    -------
    np.ndarray
        Array of final events.
    """
    logger.info("Detecting final events")
    min_event_amp = np.float32(cfg["MIN_EVENT_KEEP_SNR"]) * global_noise

    events, noise = detect_events(
        t,
        x,
        bg_clean,
        snr_threshold=np.float32(cfg["DETECTION_SNR"]),
        min_event_len=min_event_n,
        min_event_amp=min_event_amp,
        widen_frac=np.float32(cfg["WIDEN_FRAC"]),
        global_noise=global_noise,
        signal_polarity=cfg["SIGNAL_POLARITY"],
    )

    events = merge_overlapping_events(events)
    logger.info(f"Detected {len(events)} final events")

    return events


def analyze_events(
    t: np.ndarray,
    x: np.ndarray,
    bg_clean: np.ndarray,
    events: np.ndarray,
    global_noise: np.float32,
    cfg: Dict[str, Any],
) -> None:
    """
    Stage 7: Analyze event characteristics.

    Parameters
    ----------
    t : np.ndarray
        Time array.
    x : np.ndarray
        Signal array.
    bg_clean : np.ndarray
        Clean background estimate.
    events : np.ndarray
        Events array.
    global_noise : np.float32
        Estimated noise level.
    cfg : Dict[str, Any]
        Configuration dictionary.
    """
    if len(events) == 0:
        logger.info("No events to analyze")
        return
    if len(events) > 1000:
        logger.warning(
            f"Detected {len(events)} events, which is more than 1000. Skipping analysis."
        )
        return

    # Event characteristics - users want to see final event properties
    event_durations = (events[:, 1] - events[:, 0]) * 1000000  # Convert to µs
    event_amplitudes = []

    for t_start, t_end in events:
        event_mask = (t >= t_start) & (t < t_end)
        if np.any(event_mask):
            if cfg["SIGNAL_POLARITY"] < 0:
                amp = np.min(x[event_mask] - bg_clean[event_mask])
            else:
                amp = np.max(x[event_mask] - bg_clean[event_mask])
            event_amplitudes.append(abs(amp))

    if event_amplitudes:
        logger.info(
            f"Event durations (µs): min={np.min(event_durations):.2f}, max={np.max(event_durations):.2f}, mean={np.mean(event_durations):.2f}"
        )
        logger.info(
            f"Event amplitudes: min={np.min(event_amplitudes):.3g}, max={np.max(event_amplitudes):.3g}, mean={np.mean(event_amplitudes):.3g}"
        )
        logger.info(
            f"Event amplitude SNRs: min={np.min(event_amplitudes) / global_noise:.2f}, max={np.max(event_amplitudes) / global_noise:.2f}"
        )

    # Calculate relative noise for final summary
    final_signal_rms = np.sqrt(np.mean(x**2))
    final_noise_pct_rms = (
        100 * global_noise / final_signal_rms if final_signal_rms > 0 else 0
    )
    final_signal_range = np.max(x) - np.min(x)
    final_noise_pct_range = (
        100 * global_noise / final_signal_range if final_signal_range > 0 else 0
    )

    logger.info(
        f"Noise summary: {global_noise:.3g} ({final_noise_pct_rms:.1f}% of RMS, {final_noise_pct_range:.1f}% of range)"
    )


def create_oscilloscope_plot(
    t: np.ndarray,
    x: np.ndarray,
    bg_initial: np.ndarray,  # bg_initial is not used in this function, but kept for consistency with previous calls
    bg_clean: np.ndarray,
    events: np.ndarray,
    detection_threshold: np.ndarray,
    keep_threshold: np.ndarray,
    name: str,
    cfg: Dict[str, Any],
    smooth_n: int,
    global_noise: Optional[np.float32] = None,  # Make global_noise optional
) -> OscilloscopePlot:
    """
    Stage 6: Create oscilloscope plot with all visualization elements.

    Parameters
    ----------
    t : np.ndarray
        Time array.
    x : np.ndarray
        Signal array.
    bg_initial : np.ndarray
        Initial background estimate.
    bg_clean : np.ndarray
        Clean background estimate.
    events : np.ndarray
        Events array.
    detection_threshold : np.ndarray
        Detection threshold array.
    keep_threshold : np.ndarray
        Keep threshold array.
    name : str
        Name for the plot.
    cfg : Dict[str, Any]
        Configuration dictionary.
    smooth_n : int
        Smoothing window size in samples.
    global_noise : Optional[np.float32], default=None
        Estimated noise level. If provided, will be plotted as a ribbon.

    Returns
    -------
    OscilloscopePlot
        Configured oscilloscope plot.
    """
    logger.info("Creating visualization")

    # Create the oscilloscope plot instance
    plot_name = name
    if global_noise is not None:
        plot_signal_rms = np.sqrt(np.mean(x**2))
        plot_noise_pct_rms = (
            100 * global_noise / plot_signal_rms if plot_signal_rms > 0 else 0
        )
        plot_name = f"{name} | Global noise: {global_noise:.3g} ({plot_noise_pct_rms:.1f}% of RMS)"

    plot = OscilloscopePlot(
        t,
        x,
        name=plot_name,
        max_plot_points=cfg["MAX_PLOT_POINTS"],
        mode_switch_threshold=cfg["ENVELOPE_MODE_LIMIT"],
        envelope_window_samples=smooth_n,
    )

    # Add background line
    plot.add_line(
        t,
        bg_clean,
        label="Background",
        color="orange",
        alpha=0.6,
        linewidth=1.5,
        display_mode=OscilloscopePlot.MODE_BOTH,
    )

    # Add noise ribbon if global_noise is provided
    if global_noise is not None:
        plot.add_ribbon(
            t,
            bg_clean,
            global_noise,
            label="Noise (±1σ)",
            color="gray",
            alpha=0.3,
            display_mode=OscilloscopePlot.MODE_DETAIL,
        )

    # Add detection threshold
    plot.add_line(
        t,
        detection_threshold,
        label=f"Detection ({cfg['DETECTION_SNR']}σ)",
        color="red",
        alpha=0.7,
        linestyle=":",
        linewidth=1.5,
        display_mode=OscilloscopePlot.MODE_DETAIL,
    )

    # Add keep threshold
    plot.add_line(
        t,
        keep_threshold,
        label=f"Keep ({cfg['MIN_EVENT_KEEP_SNR']}σ)",
        color="darkred",
        alpha=0.7,
        linestyle="--",
        linewidth=1.5,
        display_mode=OscilloscopePlot.MODE_DETAIL,
    )

    # Add events as regions
    if len(events) > 0:
        plot.add_regions(
            events,
            label="Events",
            color="crimson",
            alpha=0.4,
            display_mode=OscilloscopePlot.MODE_BOTH,
        )

    # Render the plot after all elements have been added
    plot.render()

    return plot


def process_file(
    name: str,
    cfg: Dict[str, Any],
    sidecar: Optional[str] = None,
    crop: Optional[List[int]] = None,
    yscale_mode: str = "snr",
) -> None:
    """
    Process a single waveform file for event detection.

    Parameters
    ----------
    name : str
        Filename of the waveform data.
    cfg : Dict[str, Any]
        Configuration dictionary with SAMPLING_INTERVAL key.
    sidecar : str, optional
        XML sidecar filename.
    crop : List[int], optional
        Crop indices [start, end].

    Raises
    ------
    ValueError
        If neither SMOOTH_WIN_T nor SMOOTH_WIN_F is set.
    KeyError
        If SAMPLING_INTERVAL is not in cfg.
    """
    start_time = time.time()
    logger.info(f"Processing {name} with parameters:")

    # Stage 1: Load Data
    t, x = load_data(name, cfg, sidecar, crop)

    # Calculate parameters
    smooth_n, min_event_n = calculate_smoothing_parameters(cfg)

    # Stage 2: Background Calculation
    bg_initial = calculate_initial_background(t, x, smooth_n)
    global_noise = estimate_noise(x, bg_initial)

    # Stage 3: Initial Event Detection
    bg_time = time.time()
    logger.debug(f"Background calculation took {bg_time - start_time:.3f}s")
    events_initial = detect_initial_events(
        t, x, bg_initial, global_noise, cfg, min_event_n
    )

    # Stage 4: Clean Background Calculation
    bg_clean = calculate_clean_background(t, x, events_initial, smooth_n, bg_initial)

    # Analyze thresholds
    detection_threshold, keep_threshold = analyze_thresholds(
        x, bg_clean, global_noise, cfg
    )

    # Stage 5: Final Event Detection
    events = detect_final_events(t, x, bg_clean, global_noise, cfg, min_event_n)
    detection_time = time.time()
    logger.debug(f"Event detection took {detection_time - bg_time:.3f}s")

    # Stage 7: Event Analysis
    analyze_events(t, x, bg_clean, events, global_noise, cfg)

    logger.debug(f"Total processing time: {detection_time - start_time:.3f}s")

    # Stage 6: Visualization

    plot = create_oscilloscope_plot(
        t,
        x,
        bg_initial,
        bg_clean,
        events,
        detection_threshold,
        keep_threshold,
        name,
        cfg,
        smooth_n,
        global_noise=global_noise,  # Pass global_noise as an explicit argument
    )
    data_dir = cfg.get("DATA_PATH", "")
    analysis_dir = data_dir[:-1] if data_dir.endswith("/") else data_dir
    analysis_dir += "_analysis/"
    if not os.path.exists(analysis_dir):
        os.makedirs(analysis_dir)
    plot.save(analysis_dir + f"osc_{name}.png")

    # Create event plotter for detailed event analysis with percentage contrast
    event_plotter = EventPlotter(
        plot, events, bg_clean=bg_clean, global_noise=global_noise, y_scale_mode=yscale_mode
    )
    event_plotter.plot_events_grid(max_events=16)
    event_plotter.save(analysis_dir + f"events_{name}_events.png")
    plt.show(block=True)

def configure_logging(log_level: str = "INFO") -> None:
    """
    Configure loguru logging with specified level.

    Parameters
    ----------
    log_level : str, default="INFO"
        Logging level: DEBUG, INFO, WARNING, ERROR, CRITICAL.
    """
    # Remove default handler and add our own with specified level
    logger.remove()
    logger.add(
        sys.stderr,
        level=log_level.upper(),
        format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <level>{message}</level>",
        colorize=True,
    )
=======
>>>>>>> REPLACE
```

scripts/run_analysis03.py
```python
<<<<<<< SEARCH
def main() -> None:
    """
    Main function to process all measurements.
    """
    cfg = CONFIG.copy()  # Create a copy to avoid modifying the original

    # Configure logging
    configure_logging(cfg.get("LOG_LEVEL", "INFO"))

    for measurement in cfg["MEASUREMENTS"]:
        name = measurement["data"]
        sidecar = measurement.get("sidecar")
        if sidecar:
            params = get_waveform_params(
                name, data_path=cfg["DATA_PATH"], xml_filename=sidecar
            )
            cfg["SAMPLING_INTERVAL"] = params["sampling_interval"]
        else:
            params = get_waveform_params(name, data_path=cfg["DATA_PATH"])
            cfg["SAMPLING_INTERVAL"] = params["sampling_interval"]
        process_file(name, cfg, sidecar=sidecar, crop=measurement.get("crop"), yscale_mode=cfg.get("YSCALE_MODE", "snr"))
=======
def main() -> None:
    """
    Main function to process all measurements.
    """
    cfg = CONFIG.copy()  # Create a copy to avoid modifying the original

    # Configure logging
    configure_logging(cfg.get("LOG_LEVEL", "INFO"))

    for measurement in cfg["MEASUREMENTS"]:
        name = measurement["data"]
        sidecar = measurement.get("sidecar")
        if sidecar:
            params = get_waveform_params(
                name, data_path=cfg["DATA_PATH"], xml_filename=sidecar
            )
            cfg["SAMPLING_INTERVAL"] = params["sampling_interval"]
        else:
            params = get_waveform_params(name, data_path=cfg["DATA_PATH"])
            cfg["SAMPLING_INTERVAL"] = params["sampling_interval"]
        process_file(
            name, 
            cfg, 
            sidecar=sidecar, 
            crop=measurement.get("crop"), 
            yscale_mode=cfg.get("YSCALE_MODE", "snr"),
            show_plots=True
        )
>>>>>>> REPLACE
```

Now both scripts are dramatically simplified! They now:

1. **Import only what's needed** - just the analysis functions from the library
2. **Focus on configuration** - the CONFIG dictionary and matplotlib setup
3. **Use the library functions** - all the heavy lifting is done by `process_file()` from the analysis module
4. **Are much more maintainable** - bug fixes and improvements happen in one place

The scripts went from ~500+ lines each to about 100 lines each, with all the analysis logic now properly encapsulated in the reusable library module. The numba optimisations for the clean background calculation are also now available to both scripts automatically.

```bash
python scripts/run_analysis.py
python scripts/run_analysis03.py
```

> Tokens: 61k sent, 15k received. Cost: $0.41 message, $1.15 session.  
> Committing scripts/run_analysis.py before applying edits.  
> Committing scripts/run_analysis03.py before applying edits.  
> Commit 36f86c8 feat: enhance event detection with improved noise estimation and save plots  
> Applied edit to scripts/run_analysis03.py  
> Applied edit to scripts/run_analysis.py  
> Commit 7283997 refactor: Simplify scripts by using library functions and reducing code complexity  
> python scripts/run_analysis.py  
python scripts/run_analysis03.py  
> Run shell commands? (Y)es/(N)o/(D)on't ask again [Yes]: n  
> You can use /undo to undo and discard each aider commit.  

# aider chat started at 2025-07-10 15:54:55

> /home/samsc/.local/bin/aider  
> Aider v0.85.1  
> Main model: openrouter/anthropic/claude-sonnet-4 with diff edit format, prompt cache, infinite output  
> Weak model: openrouter/anthropic/claude-3-5-haiku  
> Git repo: .git with 27 files  
> Repo-map: using 4096 tokens, files refresh  
> Added share.md to the chat (read-only).  
> Added .aider_conventions.md to the chat (read-only).  

#### /add scripts/run_analysis.py src/pyosc/waveform/__init__.py src/pyosc/waveform/analysis.py  
> Added scripts/run_analysis.py to the chat  
> Added src/pyosc/waveform/__init__.py to the chat  
> Added src/pyosc/waveform/analysis.py to the chat  

#### /ask I don't think 'CONFIG' should be passed to the `process_file` function (it should just stay in the scripts). I would further like the option to set most of the filtering/etc. commands per-measurement (e.g. `MIN_EVENT_KEEP_SNR` inside `Measurements`). What do you think?  

#### I don't think 'CONFIG' should be passed to the `process_file` function (it should just stay in the scripts). I would further like the option to set most of the filtering/etc. commands per-measurement (e.g. `MIN_EVENT_KEEP_SNR` inside `Measurements`). What do you think?  

You're absolutely right on both points! This would make the code much cleaner and more flexible.

**Issues with current approach:**
1. Passing the entire `CONFIG` dict to `process_file` creates tight coupling between the script and the analysis module
2. No way to customize detection parameters per measurement

**Better approach:**
1. **Keep CONFIG in scripts only** - `process_file` should accept explicit parameters instead of a config dict
2. **Per-measurement parameters** - Allow measurements to override global settings

Here's what I'd suggest:

```python
# In scripts/run_analysis.py
CONFIG = {
    # Global defaults
    "SMOOTH_WIN_T": 50e-6,
    "DETECTION_SNR": 3,
    # ... other global settings
    
    "MEASUREMENTS": [
        {
            "data": "nf-300uw7mVpm-2mhz.Wfm.bin",
            "sidecar": "nf-300uw-2mhz.bin", 
            "crop": [100, -1],
            # Override for this measurement only
            "MIN_EVENT_KEEP_SNR": 8,  # More stringent for this file
            "DETECTION_SNR": 4,
        },
        {
            "data": "8mvpm.Wfm.bin",
            "crop": [100, -1],
            # Uses global defaults
        },
    ]
}
```

And `process_file` would have explicit parameters:
```python
def process_file(
    name: str,
    sampling_interval: float,
    data_path: str,
    smooth_win_t: float,
    detection_snr: float,
    min_event_keep_snr: float,
    # ... other explicit params
    sidecar: Optional[str] = None,
    crop: Optional[List[int]] = None,
) -> None:
```

This separates concerns properly - scripts handle configuration and file management, while the analysis module focuses purely on the processing logic. Would you like me to refactor this?

> Tokens: 15k sent, 497 received. Cost: $0.05 message, $0.05 session.  

#### /ask ok going back to the time to calculate the background (`calculate_initial_background` / `calculate_clean_background`) I have this info from the logs: 2025-07-10 15:55:42 | SUCCESS  | Timing: mask=0.173s, interp=10.728s, filter=22.282s /// so both interp and filtering are taking a while. I do kind of like the salgov filter though  

#### ok going back to the time to calculate the background (`calculate_initial_background` / `calculate_clean_background`) I have this info from the logs: 2025-07-10 15:55:42 | SUCCESS  | Timing: mask=0.173s, interp=10.728s, filter=22.282s /// so both interp and filtering are taking a while. I do kind of like the salgov filter though  

#### /ask how could we implement these changes?  

#### how could we implement these changes?  

Looking at your timing breakdown, the interpolation and filtering are indeed the main bottlenecks:

- **Masking**: 0.173s (fast with numba!)
- **Interpolation**: 10.728s (slow)
- **Filtering**: 22.282s (very slow)

Here are some optimisation options while keeping the Savitzky-Golay filter:

## Option 1: Faster interpolation
Replace scipy's `interp1d` with numpy's faster `np.interp`:

```python
def calculate_clean_background(
    t: np.ndarray,
    x: np.ndarray,
    events_initial: np.ndarray,
    smooth_n: int,
    bg_initial: np.ndarray,
) -> np.ndarray:
    # ... existing code ...
    
    if np.sum(mask) > 2 * smooth_n:
        interp_start = time.time()
        # Use numpy's faster linear interpolation
        x_interp = np.interp(t, t_masked, x_masked)
        interp_end = time.time()
        
        filter_start = time.time()
        bg_clean = savgol_filter(x_interp, smooth_n, 3).astype(np.float32)
        filter_end = time.time()
        # ... rest unchanged
```

`np.interp` is typically 5-10x faster than `scipy.interpolate.interp1d`.

## Option 2: Optimise Savitzky-Golay parameters
You could try reducing the polynomial order or using a smaller window:

```python
# Instead of savgol_filter(x_interp, smooth_n, 3)
bg_clean = savgol_filter(x_interp, smooth_n, 2).astype(np.float32)  # Lower polynomial order
```

## Option 3: Alternative to interpolation
Skip interpolation entirely and use a different approach:

```python
def calculate_clean_background(
    t: np.ndarray,
    x: np.ndarray,
    events_initial: np.ndarray,
    smooth_n: int,
    bg_initial: np.ndarray,
) -> np.ndarray:
    # ... masking code ...
    
    if np.sum(mask) > 2 * smooth_n:
        # Option A: Fill masked regions with initial background
        x_for_filter = np.copy(x)
        x_for_filter[~mask] = bg_initial[~mask]
        bg_clean = savgol_filter(x_for_filter, smooth_n, 3).astype(np.float32)
        
        # Option B: Or use median filter in masked regions
        # from scipy.ndimage import median_filter
        # x_for_filter = np.copy(x)
        # x_for_filter[~mask] = median_filter(x, size=smooth_n)[~mask]
        # bg_clean = savgol_filter(x_for_filter, smooth_n, 3).astype(np.float32)
```

**I'd recommend trying Option 1 first** - just replacing `interp1d` with `np.interp`. This should give you a significant speedup (probably 5-10x faster interpolation) while keeping the exact same Savitzky-Golay filtering you like.

Would you like me to implement the `np.interp` change?

> Tokens: 60k sent, 833 received. Cost: $0.19 message, $1.34 session.  

Here's how I'd implement these changes:

## 1. Update `process_file` function signature

Change `src/pyosc/waveform/analysis.py` to accept explicit parameters instead of a config dict:

```python
def process_file(
    name: str,
    sampling_interval: float,
    data_path: str,
    smooth_win_t: Optional[float] = None,
    smooth_win_f: Optional[float] = None,
    detection_snr: float = 3.0,
    min_event_keep_snr: float = 6.0,
    min_event_t: float = 0.75e-6,
    widen_frac: float = 10.0,
    signal_polarity: int = -1,
    max_plot_points: int = 10000,
    envelope_mode_limit: float = 10e-3,
    sidecar: Optional[str] = None,
    crop: Optional[List[int]] = None,
    yscale_mode: str = "snr",
    show_plots: bool = True,
) -> None:
```

## 2. Create internal config dict in `process_file`

Inside the function, reconstruct the config dict from parameters:

```python
def process_file(...):
    # Reconstruct config internally for existing helper functions
    cfg = {
        "SAMPLING_INTERVAL": sampling_interval,
        "DATA_PATH": data_path,
        "SMOOTH_WIN_T": smooth_win_t,
        "SMOOTH_WIN_F": smooth_win_f,
        "DETECTION_SNR": detection_snr,
        "MIN_EVENT_KEEP_SNR": min_event_keep_snr,
        "MIN_EVENT_T": min_event_t,
        "WIDEN_FRAC": widen_frac,
        "SIGNAL_POLARITY": signal_polarity,
        "MAX_PLOT_POINTS": max_plot_points,
        "ENVELOPE_MODE_LIMIT": envelope_mode_limit,
    }
    # Rest of function unchanged
```

## 3. Update script to merge per-measurement configs

In `scripts/run_analysis.py`, modify the main loop:

```python
def main() -> None:
    configure_logging(CONFIG.get("LOG_LEVEL", "INFO"))

    for measurement in CONFIG["MEASUREMENTS"]:
        # Merge global config with measurement-specific overrides
        merged_config = CONFIG.copy()
        merged_config.update(measurement)
        
        # Extract parameters for process_file
        name = merged_config["data"]
        sidecar = merged_config.get("sidecar")
        
        if sidecar:
            params = get_waveform_params(
                name, data_path=merged_config["DATA_PATH"], xml_filename=sidecar
            )
            sampling_interval = params["sampling_interval"]
        else:
            params = get_waveform_params(name, data_path=merged_config["DATA_PATH"])
            sampling_interval = params["sampling_interval"]
        
        # Call with explicit parameters
        process_file(
            name=name,
            sampling_interval=sampling_interval,
            data_path=merged_config["DATA_PATH"],
            smooth_win_t=merged_config.get("SMOOTH_WIN_T"),
            smooth_win_f=merged_config.get("SMOOTH_WIN_F"),
            detection_snr=merged_config.get("DETECTION_SNR", 3.0),
            min_event_keep_snr=merged_config.get("MIN_EVENT_KEEP_SNR", 6.0),
            min_event_t=merged_config.get("MIN_EVENT_T", 0.75e-6),
            widen_frac=merged_config.get("WIDEN_FRAC", 10.0),
            signal_polarity=merged_config.get("SIGNAL_POLARITY", -1),
            max_plot_points=merged_config.get("MAX_PLOT_POINTS", 10000),
            envelope_mode_limit=merged_config.get("ENVELOPE_MODE_LIMIT", 10e-3),
            sidecar=sidecar,
            crop=merged_config.get("crop"),
            yscale_mode=merged_config.get("YSCALE_MODE", "snr"),
            show_plots=True
        )
```

## 4. Benefits of this approach

- **Cleaner separation**: Scripts handle config, analysis handles processing
- **Per-measurement flexibility**: Each measurement can override any parameter
- **Better API**: `process_file` has a clear, documented interface
- **Backwards compatible**: Existing helper functions still work with internal config dict
- **Type safety**: Explicit parameters are easier to type-check

Would you like me to implement these changes step by step?

> Tokens: 15k sent, 1.2k received. Cost: $0.06 message, $0.12 session.  

#### oh no don't reconstruct the config dict inside the function, just use the kwargs directly. oh than that, go ahead  
>  
>  
> ^C again to exit  
> You can use /undo to undo and discard each aider commit.  

#### oh no don't reconstruct the config dict inside the function, just use the kwargs directly. oh than that, go ahead  

You're absolutely right! Much cleaner to use the parameters directly instead of reconstructing a config dict. Let me implement these changes:

src/pyosc/waveform/analysis.py
```python
<<<<<<< SEARCH
def calculate_smoothing_parameters(cfg: Dict[str, Any]) -> Tuple[int, int]:
    """
    Calculate smoothing window size and minimum event length in samples.

    Parameters
    ----------
    cfg : Dict[str, Any]
        Configuration dictionary.

    Returns
    -------
    Tuple[int, int]
        Smoothing window size and minimum event length in samples.
    """
    if "SAMPLING_INTERVAL" not in cfg:
        raise KeyError("SAMPLING_INTERVAL must be set in configuration")

    if cfg["SMOOTH_WIN_T"] is not None:
        smooth_n = int(cfg["SMOOTH_WIN_T"] / cfg["SAMPLING_INTERVAL"])
    elif cfg["SMOOTH_WIN_F"] is not None:
        smooth_n = int(1 / (cfg["SMOOTH_WIN_F"] * cfg["SAMPLING_INTERVAL"]))
    else:
        raise ValueError("Set either SMOOTH_WIN_T or SMOOTH_WIN_F")

    if smooth_n % 2 == 0:
        smooth_n += 1

    min_event_n = int(cfg["MIN_EVENT_T"] / cfg["SAMPLING_INTERVAL"])

    smooth_freq_hz = 1 / (smooth_n * cfg["SAMPLING_INTERVAL"])
    logger.info(
        f"--Smooth window: {smooth_n} samples ({cfg['SMOOTH_WIN_T'] * 1e6:.1f} µs, {smooth_freq_hz:.1f} Hz)"
    )
    logger.info(
        f"--Min event length: {min_event_n} samples ({cfg['MIN_EVENT_T'] * 1e6:.1f} µs)"
    )
    logger.info(f"--Detection SNR: {cfg['DETECTION_SNR']}")
    logger.info(f"--Min keep SNR: {cfg['MIN_EVENT_KEEP_SNR']}")
    logger.info(f"--Widen fraction: {cfg['WIDEN_FRAC']}")
    logger.info(
        f"--Signal polarity: {cfg['SIGNAL_POLARITY']} ({'negative' if cfg['SIGNAL_POLARITY'] < 0 else 'positive'} events)"
    )

    return smooth_n, min_event_n
=======
def calculate_smoothing_parameters(
    sampling_interval: float,
    smooth_win_t: Optional[float],
    smooth_win_f: Optional[float],
    min_event_t: float,
    detection_snr: float,
    min_event_keep_snr: float,
    widen_frac: float,
    signal_polarity: int,
) -> Tuple[int, int]:
    """
    Calculate smoothing window size and minimum event length in samples.

    Parameters
    ----------
    sampling_interval : float
        Sampling interval in seconds.
    smooth_win_t : Optional[float]
        Smoothing window in seconds.
    smooth_win_f : Optional[float]
        Smoothing window in Hz.
    min_event_t : float
        Minimum event duration in seconds.
    detection_snr : float
        Detection SNR threshold.
    min_event_keep_snr : float
        Minimum event keep SNR threshold.
    widen_frac : float
        Fraction to widen detected events.
    signal_polarity : int
        Signal polarity (-1 for negative, +1 for positive).

    Returns
    -------
    Tuple[int, int]
        Smoothing window size and minimum event length in samples.
    """
    if smooth_win_t is not None:
        smooth_n = int(smooth_win_t / sampling_interval)
    elif smooth_win_f is not None:
        smooth_n = int(1 / (smooth_win_f * sampling_interval))
    else:
        raise ValueError("Set either smooth_win_t or smooth_win_f")

    if smooth_n % 2 == 0:
        smooth_n += 1

    min_event_n = int(min_event_t / sampling_interval)

    smooth_freq_hz = 1 / (smooth_n * sampling_interval)
    logger.info(
        f"--Smooth window: {smooth_n} samples ({smooth_win_t * 1e6:.1f} µs, {smooth_freq_hz:.1f} Hz)"
    )
    logger.info(
        f"--Min event length: {min_event_n} samples ({min_event_t * 1e6:.1f} µs)"
    )
    logger.info(f"--Detection SNR: {detection_snr}")
    logger.info(f"--Min keep SNR: {min_event_keep_snr}")
    logger.info(f"--Widen fraction: {widen_frac}")
    logger.info(
        f"--Signal polarity: {signal_polarity} ({'negative' if signal_polarity < 0 else 'positive'} events)"
    )

    return smooth_n, min_event_n
>>>>>>> REPLACE
```

src/pyosc/waveform/analysis.py
```python
<<<<<<< SEARCH
def load_data(
    name: str,
    cfg: Dict[str, Any],
    sidecar: Optional[str] = None,
    crop: Optional[List[int]] = None,
) -> Tuple[np.ndarray, np.ndarray]:
    """
    Stage 1: Load waveform data from file.

    Parameters
    ----------
    name : str
        Filename of the waveform data.
    cfg : Dict[str, Any]
        Configuration dictionary with SAMPLING_INTERVAL key.
    sidecar : str, optional
        XML sidecar filename.
    crop : List[int], optional
        Crop indices [start, end].

    Returns
    -------
    Tuple[np.ndarray, np.ndarray]
        Time and signal arrays.
    """
    logger.info(f"Loading data from {name}")
    t, x = rd(
        name,
        cfg["SAMPLING_INTERVAL"],
        data_path=cfg["DATA_PATH"],
        xml_filename=sidecar,
        crop=crop,
    )

    logger.debug(
        f"Signal statistics: min={np.min(x):.3g}, max={np.max(x):.3g}, mean={np.mean(x):.3g}, std={np.std(x):.3g}"
    )

    return t, x
=======
def load_data(
    name: str,
    sampling_interval: float,
    data_path: str,
    sidecar: Optional[str] = None,
    crop: Optional[List[int]] = None,
) -> Tuple[np.ndarray, np.ndarray]:
    """
    Stage 1: Load waveform data from file.

    Parameters
    ----------
    name : str
        Filename of the waveform data.
    sampling_interval : float
        Sampling interval in seconds.
    data_path : str
        Path to data directory.
    sidecar : str, optional
        XML sidecar filename.
    crop : List[int], optional
        Crop indices [start, end].

    Returns
    -------
    Tuple[np.ndarray, np.ndarray]
        Time and signal arrays.
    """
    logger.info(f"Loading data from {name}")
    t, x = rd(
        name,
        sampling_interval,
        data_path=data_path,
        xml_filename=sidecar,
        crop=crop,
    )

    logger.debug(
        f"Signal statistics: min={np.min(x):.3g}, max={np.max(x):.3g}, mean={np.mean(x):.3g}, std={np.std(x):.3g}"
    )

    return t, x
>>>>>>> REPLACE
```

src/pyosc/waveform/analysis.py
```python
<<<<<<< SEARCH
def detect_initial_events(
    t: np.ndarray,
    x: np.ndarray,
    bg_initial: np.ndarray,
    global_noise: np.float32,
    cfg: Dict[str, Any],
    min_event_n: int,
) -> np.ndarray:
    """
    Stage 3: Detect initial events.

    Parameters
    ----------
    t : np.ndarray
        Time array.
    x : np.ndarray
        Signal array.
    bg_initial : np.ndarray
        Initial background estimate.
    global_noise : np.float32
        Estimated noise level.
    cfg : Dict[str, Any]
        Configuration dictionary.
    min_event_n : int
        Minimum event length in samples.

    Returns
    -------
    np.ndarray
        Array of initial events.
    """
    logger.info("Detecting initial events")
    min_event_amp = np.float32(cfg["MIN_EVENT_KEEP_SNR"]) * global_noise

    logger.info(f"Detection threshold: {cfg['DETECTION_SNR']}σ below background")
    logger.info(f"Keep threshold: {cfg['MIN_EVENT_KEEP_SNR']}σ below background")
    logger.info(f"Min event amplitude threshold: {min_event_amp:.3g}")

    events_initial, _ = detect_events(
        t,
        x,
        bg_initial,
        snr_threshold=np.float32(cfg["DETECTION_SNR"]),
        min_event_len=min_event_n,
        min_event_amp=min_event_amp,
        widen_frac=np.float32(cfg["WIDEN_FRAC"]),
        global_noise=global_noise,
        signal_polarity=cfg["SIGNAL_POLARITY"],
    )

    logger.info(f"Found {len(events_initial)} initial events after filtering")

    events_initial = merge_overlapping_events(events_initial)
    logger.info(f"After merging: {len(events_initial)} events")

    return events_initial
=======
def detect_initial_events(
    t: np.ndarray,
    x: np.ndarray,
    bg_initial: np.ndarray,
    global_noise: np.float32,
    detection_snr: float,
    min_event_keep_snr: float,
    widen_frac: float,
    signal_polarity: int,
    min_event_n: int,
) -> np.ndarray:
    """
    Stage 3: Detect initial events.

    Parameters
    ----------
    t : np.ndarray
        Time array.
    x : np.ndarray
        Signal array.
    bg_initial : np.ndarray
        Initial background estimate.
    global_noise : np.float32
        Estimated noise level.
    detection_snr : float
        Detection SNR threshold.
    min_event_keep_snr : float
        Minimum event keep SNR threshold.
    widen_frac : float
        Fraction to widen detected events.
    signal_polarity : int
        Signal polarity (-1 for negative, +1 for positive).
    min_event_n : int
        Minimum event length in samples.

    Returns
    -------
    np.ndarray
        Array of initial events.
    """
    logger.info("Detecting initial events")
    min_event_amp = np.float32(min_event_keep_snr) * global_noise

    logger.info(f"Detection threshold: {detection_snr}σ below background")
    logger.info(f"Keep threshold: {min_event_keep_snr}σ below background")
    logger.info(f"Min event amplitude threshold: {min_event_amp:.3g}")

    events_initial, _ = detect_events(
        t,
        x,
        bg_initial,
        snr_threshold=np.float32(detection_snr),
        min_event_len=min_event_n,
        min_event_amp=min_event_amp,
        widen_frac=np.float32(widen_frac),
        global_noise=global_noise,
        signal_polarity=signal_polarity,
    )

    logger.info(f"Found {len(events_initial)} initial events after filtering")

    events_initial = merge_overlapping_events(events_initial)
    logger.info(f"After merging: {len(events_initial)} events")

    return events_initial
>>>>>>> REPLACE
```

src/pyosc/waveform/analysis.py
```python
<<<<<<< SEARCH
def analyze_thresholds(
    x: np.ndarray, bg_clean: np.ndarray, global_noise: np.float32, cfg: Dict[str, Any]
) -> Tuple[np.ndarray, np.ndarray]:
    """
    Analyze threshold statistics and create threshold arrays.

    Parameters
    ----------
    x : np.ndarray
        Signal array.
    bg_clean : np.ndarray
        Clean background estimate.
    global_noise : np.float32
        Estimated noise level.
    cfg : Dict[str, Any]
        Configuration dictionary.

    Returns
    -------
    Tuple[np.ndarray, np.ndarray]
        Detection and keep threshold arrays.
    """
    logger.info("Analyzing thresholds")

    if cfg["SIGNAL_POLARITY"] < 0:
        detection_threshold = bg_clean - cfg["DETECTION_SNR"] * global_noise
        keep_threshold = bg_clean - cfg["MIN_EVENT_KEEP_SNR"] * global_noise
        below_detection_pct = 100 * np.sum(x < detection_threshold) / len(x)
        below_keep_pct = 100 * np.sum(x < keep_threshold) / len(x)
        logger.info(f"Samples below detection threshold: {below_detection_pct:.2f}%")
        logger.info(f"Samples below keep threshold: {below_keep_pct:.2f}%")
    else:
        detection_threshold = bg_clean + cfg["DETECTION_SNR"] * global_noise
        keep_threshold = bg_clean + cfg["MIN_EVENT_KEEP_SNR"] * global_noise
        above_detection_pct = 100 * np.sum(x > detection_threshold) / len(x)
        above_keep_pct = 100 * np.sum(x > keep_threshold) / len(x)
        logger.info(f"Samples above detection threshold: {above_detection_pct:.2f}%")
        logger.info(f"Samples above keep threshold: {above_keep_pct:.2f}%")

    return detection_threshold, keep_threshold
=======
def analyze_thresholds(
    x: np.ndarray,
    bg_clean: np.ndarray,
    global_noise: np.float32,
    detection_snr: float,
    min_event_keep_snr: float,
    signal_polarity: int,
) -> Tuple[np.ndarray, np.ndarray]:
    """
    Analyze threshold statistics and create threshold arrays.

    Parameters
    ----------
    x : np.ndarray
        Signal array.
    bg_clean : np.ndarray
        Clean background estimate.
    global_noise : np.float32
        Estimated noise level.
    detection_snr : float
        Detection SNR threshold.
    min_event_keep_snr : float
        Minimum event keep SNR threshold.
    signal_polarity : int
        Signal polarity (-1 for negative, +1 for positive).

    Returns
    -------
    Tuple[np.ndarray, np.ndarray]
        Detection and keep threshold arrays.
    """
    logger.info("Analyzing thresholds")

    if signal_polarity < 0:
        detection_threshold = bg_clean - detection_snr * global_noise
        keep_threshold = bg_clean - min_event_keep_snr * global_noise
        below_detection_pct = 100 * np.sum(x < detection_threshold) / len(x)
        below_keep_pct = 100 * np.sum(x < keep_threshold) / len(x)
        logger.info(f"Samples below detection threshold: {below_detection_pct:.2f}%")
        logger.info(f"Samples below keep threshold: {below_keep_pct:.2f}%")
    else:
        detection_threshold = bg_clean + detection_snr * global_noise
        keep_threshold = bg_clean + min_event_keep_snr * global_noise
        above_detection_pct = 100 * np.sum(x > detection_threshold) / len(x)
        above_keep_pct = 100 * np.sum(x > keep_threshold) / len(x)
        logger.info(f"Samples above detection threshold: {above_detection_pct:.2f}%")
        logger.info(f"Samples above keep threshold: {above_keep_pct:.2f}%")

    return detection_threshold, keep_threshold
>>>>>>> REPLACE
```

src/pyosc/waveform/analysis.py
```python
<<<<<<< SEARCH
def detect_final_events(
    t: np.ndarray,
    x: np.ndarray,
    bg_clean: np.ndarray,
    global_noise: np.float32,
    cfg: Dict[str, Any],
    min_event_n: int,
) -> np.ndarray:
    """
    Stage 5: Detect final events using clean background.

    Parameters
    ----------
    t : np.ndarray
        Time array.
    x : np.ndarray
        Signal array.
    bg_clean : np.ndarray
        Clean background estimate.
    global_noise : np.float32
        Estimated noise level.
    cfg : Dict[str, Any]
        Configuration dictionary.
    min_event_n : int
        Minimum event length in samples.

    Returns
    -------
    np.ndarray
        Array of final events.
    """
    logger.info("Detecting final events")
    min_event_amp = np.float32(cfg["MIN_EVENT_KEEP_SNR"]) * global_noise

    events, noise = detect_events(
        t,
        x,
        bg_clean,
        snr_threshold=np.float32(cfg["DETECTION_SNR"]),
        min_event_len=min_event_n,
        min_event_amp=min_event_amp,
        widen_frac=np.float32(cfg["WIDEN_FRAC"]),
        global_noise=global_noise,
        signal_polarity=cfg["SIGNAL_POLARITY"],
    )

    events = merge_overlapping_events(events)
    logger.info(f"Detected {len(events)} final events")

    return events
=======
def detect_final_events(
    t: np.ndarray,
    x: np.ndarray,
    bg_clean: np.ndarray,
    global_noise: np.float32,
    detection_snr: float,
    min_event_keep_snr: float,
    widen_frac: float,
    signal_polarity: int,
    min_event_n: int,
) -> np.ndarray:
    """
    Stage 5: Detect final events using clean background.

    Parameters
    ----------
    t : np.ndarray
        Time array.
    x : np.ndarray
        Signal array.
    bg_clean : np.ndarray
        Clean background estimate.
    global_noise : np.float32
        Estimated noise level.
    detection_snr : float
        Detection SNR threshold.
    min_event_keep_snr : float
        Minimum event keep SNR threshold.
    widen_frac : float
        Fraction to widen detected events.
    signal_polarity : int
        Signal polarity (-1 for negative, +1 for positive).
    min_event_n : int
        Minimum event length in samples.

    Returns
    -------
    np.ndarray
        Array of final events.
    """
    logger.info("Detecting final events")
    min_event_amp = np.float32(min_event_keep_snr) * global_noise

    events, noise = detect_events(
        t,
        x,
        bg_clean,
        snr_threshold=np.float32(detection_snr),
        min_event_len=min_event_n,
        min_event_amp=min_event_amp,
        widen_frac=np.float32(widen_frac),
        global_noise=global_noise,
        signal_polarity=signal_polarity,
    )

    events = merge_overlapping_events(events)
    logger.info(f"Detected {len(events)} final events")

    return events
>>>>>>> REPLACE
```

src/pyosc/waveform/analysis.py
```python
<<<<<<< SEARCH
def analyze_events(
    t: np.ndarray,
    x: np.ndarray,
    bg_clean: np.ndarray,
    events: np.ndarray,
    global_noise: np.float32,
    cfg: Dict[str, Any],
) -> None:
    """
    Stage 7: Analyze event characteristics.

    Parameters
    ----------
    t : np.ndarray
        Time array.
    x : np.ndarray
        Signal array.
    bg_clean : np.ndarray
        Clean background estimate.
    events : np.ndarray
        Events array.
    global_noise : np.float32
        Estimated noise level.
    cfg : Dict[str, Any]
        Configuration dictionary.
    """
    if len(events) == 0:
        logger.info("No events to analyze")
        return
    if len(events) > 1000:
        logger.warning(
            f"Detected {len(events)} events, which is more than 1000. Skipping analysis."
        )
        return

    event_durations = (events[:, 1] - events[:, 0]) * 1000000  # Convert to µs
    event_amplitudes = []

    for t_start, t_end in events:
        event_mask = (t >= t_start) & (t < t_end)
        if np.any(event_mask):
            if cfg["SIGNAL_POLARITY"] < 0:
                amp = np.min(x[event_mask] - bg_clean[event_mask])
            else:
                amp = np.max(x[event_mask] - bg_clean[event_mask])
            event_amplitudes.append(abs(amp))

    if event_amplitudes:
        logger.info(
            f"Event durations (µs): min={np.min(event_durations):.2f}, max={np.max(event_durations):.2f}, mean={np.mean(event_durations):.2f}"
        )
        logger.info(
            f"Event amplitudes: min={np.min(event_amplitudes):.3g}, max={np.max(event_amplitudes):.3g}, mean={np.mean(event_amplitudes):.3g}"
        )
        logger.info(
            f"Event amplitude SNRs: min={np.min(event_amplitudes) / global_noise:.2f}, max={np.max(event_amplitudes) / global_noise:.2f}"
        )

    final_signal_rms = np.sqrt(np.mean(x**2))
    final_noise_pct_rms = (
        100 * global_noise / final_signal_rms if final_signal_rms > 0 else 0
    )
    final_signal_range = np.max(x) - np.min(x)
    final_noise_pct_range = (
        100 * global_noise / final_signal_range if final_signal_range > 0 else 0
    )

    logger.info(
        f"Noise summary: {global_noise:.3g} ({final_noise_pct_rms:.1f}% of RMS, {final_noise_pct_range:.1f}% of range)"
    )
=======
def analyze_events(
    t: np.ndarray,
    x: np.ndarray,
    bg_clean: np.ndarray,
    events: np.ndarray,
    global_noise: np.float32,
    signal_polarity: int,
) -> None:
    """
    Stage 7: Analyze event characteristics.

    Parameters
    ----------
    t : np.ndarray
        Time array.
    x : np.ndarray
        Signal array.
    bg_clean : np.ndarray
        Clean background estimate.
    events : np.ndarray
        Events array.
    global_noise : np.float32
        Estimated noise level.
    signal_polarity : int
        Signal polarity (-1 for negative, +1 for positive).
    """
    if len(events) == 0:
        logger.info("No events to analyze")
        return
    if len(events) > 1000:
        logger.warning(
            f"Detected {len(events)} events, which is more than 1000. Skipping analysis."
        )
        return

    event_durations = (events[:, 1] - events[:, 0]) * 1000000  # Convert to µs
    event_amplitudes = []

    for t_start, t_end in events:
        event_mask = (t >= t_start) & (t < t_end)
        if np.any(event_mask):
            if signal_polarity < 0:
                amp = np.min(x[event_mask] - bg_clean[event_mask])
            else:
                amp = np.max(x[event_mask] - bg_clean[event_mask])
            event_amplitudes.append(abs(amp))

    if event_amplitudes:
        logger.info(
            f"Event durations (µs): min={np.min(event_durations):.2f}, max={np.max(event_durations):.2f}, mean={np.mean(event_durations):.2f}"
        )
        logger.info(
            f"Event amplitudes: min={np.min(event_amplitudes):.3g}, max={np.max(event_amplitudes):.3g}, mean={np.mean(event_amplitudes):.3g}"
        )
        logger.info(
            f"Event amplitude SNRs: min={np.min(event_amplitudes) / global_noise:.2f}, max={np.max(event_amplitudes) / global_noise:.2f}"
        )

    final_signal_rms = np.sqrt(np.mean(x**2))
    final_noise_pct_rms = (
        100 * global_noise / final_signal_rms if final_signal_rms > 0 else 0
    )
    final_signal_range = np.max(x) - np.min(x)
    final_noise_pct_range = (
        100 * global_noise / final_signal_range if final_signal_range > 0 else 0
    )

    logger.info(
        f"Noise summary: {global_noise:.3g} ({final_noise_pct_rms:.1f}% of RMS, {final_noise_pct_range:.1f}% of range)"
    )
>>>>>>> REPLACE
```

src/pyosc/waveform/analysis.py
```python
<<<<<<< SEARCH
def create_oscilloscope_plot(
    t: np.ndarray,
    x: np.ndarray,
    bg_initial: np.ndarray,
    bg_clean: np.ndarray,
    events: np.ndarray,
    detection_threshold: np.ndarray,
    keep_threshold: np.ndarray,
    name: str,
    cfg: Dict[str, Any],
    smooth_n: int,
    global_noise: Optional[np.float32] = None,
) -> OscilloscopePlot:
    """
    Stage 6: Create oscilloscope plot with all visualization elements.

    Parameters
    ----------
    t : np.ndarray
        Time array.
    x : np.ndarray
        Signal array.
    bg_initial : np.ndarray
        Initial background estimate.
    bg_clean : np.ndarray
        Clean background estimate.
    events : np.ndarray
        Events array.
    detection_threshold : np.ndarray
        Detection threshold array.
    keep_threshold : np.ndarray
        Keep threshold array.
    name : str
        Name for the plot.
    cfg : Dict[str, Any]
        Configuration dictionary.
    smooth_n : int
        Smoothing window size in samples.
    global_noise : Optional[np.float32], default=None
        Estimated noise level. If provided, will be plotted as a ribbon.

    Returns
    -------
    OscilloscopePlot
        Configured oscilloscope plot.
    """
    logger.info("Creating visualization")

    plot_name = name
    if global_noise is not None:
        plot_signal_rms = np.sqrt(np.mean(x**2))
        plot_noise_pct_rms = (
            100 * global_noise / plot_signal_rms if plot_signal_rms > 0 else 0
        )
        plot_name = f"{name} | Global noise: {global_noise:.3g} ({plot_noise_pct_rms:.1f}% of RMS)"

    plot = OscilloscopePlot(
        t,
        x,
        name=plot_name,
        max_plot_points=cfg["MAX_PLOT_POINTS"],
        mode_switch_threshold=cfg["ENVELOPE_MODE_LIMIT"],
        envelope_window_samples=smooth_n,
    )

    plot.add_line(
        t,
        bg_clean,
        label="Background",
        color="orange",
        alpha=0.6,
        linewidth=1.5,
        display_mode=OscilloscopePlot.MODE_BOTH,
    )

    if global_noise is not None:
        plot.add_ribbon(
            t,
            bg_clean,
            global_noise,
            label="Noise (±1σ)",
            color="gray",
            alpha=0.3,
            display_mode=OscilloscopePlot.MODE_DETAIL,
        )

    plot.add_line(
        t,
        detection_threshold,
        label=f"Detection ({cfg['DETECTION_SNR']}σ)",
        color="red",
        alpha=0.7,
        linestyle=":",
        linewidth=1.5,
        display_mode=OscilloscopePlot.MODE_DETAIL,
    )

    plot.add_line(
        t,
        keep_threshold,
        label=f"Keep ({cfg['MIN_EVENT_KEEP_SNR']}σ)",
        color="darkred",
        alpha=0.7,
        linestyle="--",
        linewidth=1.5,
        display_mode=OscilloscopePlot.MODE_DETAIL,
    )

    if len(events) > 0:
        plot.add_regions(
            events,
            label="Events",
            color="crimson",
            alpha=0.4,
            display_mode=OscilloscopePlot.MODE_BOTH,
        )

    plot.render()
    return plot
=======
def create_oscilloscope_plot(
    t: np.ndarray,
    x: np.ndarray,
    bg_initial: np.ndarray,
    bg_clean: np.ndarray,
    events: np.ndarray,
    detection_threshold: np.ndarray,
    keep_threshold: np.ndarray,
    name: str,
    detection_snr: float,
    min_event_keep_snr: float,
    max_plot_points: int,
    envelope_mode_limit: float,
    smooth_n: int,
    global_noise: Optional[np.float32] = None,
) -> OscilloscopePlot:
    """
    Stage 6: Create oscilloscope plot with all visualization elements.

    Parameters
    ----------
    t : np.ndarray
        Time array.
    x : np.ndarray
        Signal array.
    bg_initial : np.ndarray
        Initial background estimate.
    bg_clean : np.ndarray
        Clean background estimate.
    events : np.ndarray
        Events array.
    detection_threshold : np.ndarray
        Detection threshold array.
    keep_threshold : np.ndarray
        Keep threshold array.
    name : str
        Name for the plot.
    detection_snr : float
        Detection SNR threshold.
    min_event_keep_snr : float
        Minimum event keep SNR threshold.
    max_plot_points : int
        Maximum plot points for decimation.
    envelope_mode_limit : float
        Envelope mode limit.
    smooth_n : int
        Smoothing window size in samples.
    global_noise : Optional[np.float32], default=None
        Estimated noise level. If provided, will be plotted as a ribbon.

    Returns
    -------
    OscilloscopePlot
        Configured oscilloscope plot.
    """
    logger.info("Creating visualization")

    plot_name = name
    if global_noise is not None:
        plot_signal_rms = np.sqrt(np.mean(x**2))
        plot_noise_pct_rms = (
            100 * global_noise / plot_signal_rms if plot_signal_rms > 0 else 0
        )
        plot_name = f"{name} | Global noise: {global_noise:.3g} ({plot_noise_pct_rms:.1f}% of RMS)"

    plot = OscilloscopePlot(
        t,
        x,
        name=plot_name,
        max_plot_points=max_plot_points,
        mode_switch_threshold=envelope_mode_limit,
        envelope_window_samples=smooth_n,
    )

    plot.add_line(
        t,
        bg_clean,
        label="Background",
        color="orange",
        alpha=0.6,
        linewidth=1.5,
        display_mode=OscilloscopePlot.MODE_BOTH,
    )

    if global_noise is not None:
        plot.add_ribbon(
            t,
            bg_clean,
            global_noise,
            label="Noise (±1σ)",
            color="gray",
            alpha=0.3,
            display_mode=OscilloscopePlot.MODE_DETAIL,
        )

    plot.add_line(
        t,
        detection_threshold,
        label=f"Detection ({detection_snr}σ)",
        color="red",
        alpha=0.7,
        linestyle=":",
        linewidth=1.5,
        display_mode=OscilloscopePlot.MODE_DETAIL,
    )

    plot.add_line(
        t,
        keep_threshold,
        label=f"Keep ({min_event_keep_snr}σ)",
        color="darkred",
        alpha=0.7,
        linestyle="--",
        linewidth=1.5,
        display_mode=OscilloscopePlot.MODE_DETAIL,
    )

    if len(events) > 0:
        plot.add_regions(
            events,
            label="Events",
            color="crimson",
            alpha=0.4,
            display_mode=OscilloscopePlot.MODE_BOTH,
        )

    plot.render()
    return plot
>>>>>>> REPLACE
```

src/pyosc/waveform/analysis.py
```python
<<<<<<< SEARCH
def process_file(
    name: str,
    cfg: Dict[str, Any],
    sidecar: Optional[str] = None,
    crop: Optional[List[int]] = None,
    yscale_mode: str = "snr",
    show_plots: bool = True,
) -> None:
    """
    Process a single waveform file for event detection.

    Parameters
    ----------
    name : str
        Filename of the waveform data.
    cfg : Dict[str, Any]
        Configuration dictionary with SAMPLING_INTERVAL key.
    sidecar : str, optional
        XML sidecar filename.
    crop : List[int], optional
        Crop indices [start, end].
    yscale_mode : str, default="snr"
        Y-axis scaling mode for event plotter.
    show_plots : bool, default=True
        Whether to show plots interactively.
    """
    start_time = time.time()
    logger.info(f"Processing {name} with parameters:")

    # Stage 1: Load Data
    t, x = load_data(name, cfg, sidecar, crop)

    # Calculate parameters
    smooth_n, min_event_n = calculate_smoothing_parameters(cfg)

    # Stage 2: Background Calculation
    bg_initial = calculate_initial_background(t, x, smooth_n)
    global_noise = estimate_noise(x, bg_initial)

    # Stage 3: Initial Event Detection
    bg_time = time.time()
    logger.debug(f"Background calculation took {bg_time - start_time:.3f}s")
    events_initial = detect_initial_events(
        t, x, bg_initial, global_noise, cfg, min_event_n
    )

    # Stage 4: Clean Background Calculation
    bg_clean = calculate_clean_background(t, x, events_initial, smooth_n, bg_initial)

    # Analyze thresholds
    detection_threshold, keep_threshold = analyze_thresholds(
        x, bg_clean, global_noise, cfg
    )

    # Stage 5: Final Event Detection
    events = detect_final_events(t, x, bg_clean, global_noise, cfg, min_event_n)
    detection_time = time.time()
    logger.debug(f"Event detection took {detection_time - bg_time:.3f}s")

    # Stage 7: Event Analysis
    analyze_events(t, x, bg_clean, events, global_noise, cfg)

    logger.debug(f"Total processing time: {detection_time - start_time:.3f}s")

    # Stage 6: Visualization
    plot = create_oscilloscope_plot(
        t,
        x,
        bg_initial,
        bg_clean,
        events,
        detection_threshold,
        keep_threshold,
        name,
        cfg,
        smooth_n,
        global_noise=global_noise,
    )
    
    # Save plots
    data_dir = cfg.get("DATA_PATH", "")
    analysis_dir = data_dir[:-1] if data_dir.endswith("/") else data_dir
    analysis_dir += "_analysis/"
    if not os.path.exists(analysis_dir):
        os.makedirs(analysis_dir)
    plot.save(analysis_dir + f"osc_{name}.png")

    # Create event plotter
    event_plotter = EventPlotter(
        plot, events, bg_clean=bg_clean, global_noise=global_noise, y_scale_mode=yscale_mode
    )
    event_plotter.plot_events_grid(max_events=16)
    event_plotter.save(analysis_dir + f"events_{name}_events.png")
    
    if show_plots:
        plt.show(block=True)
=======
def process_file(
    name: str,
    sampling_interval: float,
    data_path: str,
    smooth_win_t: Optional[float] = None,
    smooth_win_f: Optional[float] = None,
    detection_snr: float = 3.0,
    min_event_keep_snr: float = 6.0,
    min_event_t: float = 0.75e-6,
    widen_frac: float = 10.0,
    signal_polarity: int = -1,
    max_plot_points: int = 10000,
    envelope_mode_limit: float = 10e-3,
    sidecar: Optional[str] = None,
    crop: Optional[List[int]] = None,
    yscale_mode: str = "snr",
    show_plots: bool = True,
) -> None:
    """
    Process a single waveform file for event detection.

    Parameters
    ----------
    name : str
        Filename of the waveform data.
    sampling_interval : float
        Sampling interval in seconds.
    data_path : str
        Path to data directory.
    smooth_win_t : Optional[float], default=None
        Smoothing window in seconds.
    smooth_win_f : Optional[float], default=None
        Smoothing window in Hz.
    detection_snr : float, default=3.0
        Detection SNR threshold.
    min_event_keep_snr : float, default=6.0
        Minimum event keep SNR threshold.
    min_event_t : float, default=0.75e-6
        Minimum event duration in seconds.
    widen_frac : float, default=10.0
        Fraction to widen detected events.
    signal_polarity : int, default=-1
        Signal polarity (-1 for negative, +1 for positive).
    max_plot_points : int, default=10000
        Maximum plot points for decimation.
    envelope_mode_limit : float, default=10e-3
        Envelope mode limit.
    sidecar : str, optional
        XML sidecar filename.
    crop : List[int], optional
        Crop indices [start, end].
    yscale_mode : str, default="snr"
        Y-axis scaling mode for event plotter.
    show_plots : bool, default=True
        Whether to show plots interactively.
    """
    start_time = time.time()
    logger.info(f"Processing {name} with parameters:")

    # Stage 1: Load Data
    t, x = load_data(name, sampling_interval, data_path, sidecar, crop)

    # Calculate parameters
    smooth_n, min_event_n = calculate_smoothing_parameters(
        sampling_interval,
        smooth_win_t,
        smooth_win_f,
        min_event_t,
        detection_snr,
        min_event_keep_snr,
        widen_frac,
        signal_polarity,
    )

    # Stage 2: Background Calculation
    bg_initial = calculate_initial_background(t, x, smooth_n)
    global_noise = estimate_noise(x, bg_initial)

    # Stage 3: Initial Event Detection
    bg_time = time.time()
    logger.debug(f"Background calculation took {bg_time - start_time:.3f}s")
    events_initial = detect_initial_events(
        t,
        x,
        bg_initial,
        global_noise,
        detection_snr,
        min_event_keep_snr,
        widen_frac,
        signal_polarity,
        min_event_n,
    )

    # Stage 4: Clean Background Calculation
    bg_clean = calculate_clean_background(t, x, events_initial, smooth_n, bg_initial)

    # Analyze thresholds
    detection_threshold, keep_threshold = analyze_thresholds(
        x, bg_clean, global_noise, detection_snr, min_event_keep_snr, signal_polarity
    )

    # Stage 5: Final Event Detection
    events = detect_final_events(
        t,
        x,
        bg_clean,
        global_noise,
        detection_snr,
        min_event_keep_snr,
        widen_frac,
        signal_polarity,
        min_event_n,
    )
    detection_time = time.time()
    logger.debug(f"Event detection took {detection_time - bg_time:.3f}s")

    # Stage 7: Event Analysis
    analyze_events(t, x, bg_clean, events, global_noise, signal_polarity)

    logger.debug(f"Total processing time: {detection_time - start_time:.3f}s")

    # Stage 6: Visualization
    plot = create_oscilloscope_plot(
        t,
        x,
        bg_initial,
        bg_clean,
        events,
        detection_threshold,
        keep_threshold,
        name,
        detection_snr,
        min_event_keep_snr,
        max_plot_points,
        envelope_mode_limit,
        smooth_n,
        global_noise=global_noise,
    )

    # Save plots
    analysis_dir = data_path[:-1] if data_path.endswith("/") else data_path
    analysis_dir += "_analysis/"
    if not os.path.exists(analysis_dir):
        os.makedirs(analysis_dir)
    plot.save(analysis_dir + f"osc_{name}.png")

    # Create event plotter
    event_plotter = EventPlotter(
        plot,
        events,
        bg_clean=bg_clean,
        global_noise=global_noise,
        y_scale_mode=yscale_mode,
    )
    event_plotter.plot_events_grid(max_events=16)
    event_plotter.save(analysis_dir + f"events_{name}_events.png")

    if show_plots:
        plt.show(block=True)
>>>>>>> REPLACE
```

Now let's update the script to use the new API:

scripts/run_analysis.py
```python
<<<<<<< SEARCH
def main() -> None:
    """
    Main function to process all measurements.
    """
    cfg = CONFIG.copy()  # Create a copy to avoid modifying the original

    # Configure logging
    configure_logging(cfg.get("LOG_LEVEL", "INFO"))

    for measurement in cfg["MEASUREMENTS"]:
        name = measurement["data"]
        sidecar = measurement.get("sidecar")
        if sidecar:
            params = get_waveform_params(
                name, data_path=cfg["DATA_PATH"], xml_filename=sidecar
            )
            cfg["SAMPLING_INTERVAL"] = params["sampling_interval"]
        else:
            params = get_waveform_params(name, data_path=cfg["DATA_PATH"])
            cfg["SAMPLING_INTERVAL"] = params["sampling_interval"]
        process_file(
            name, 
            cfg, 
            sidecar=sidecar, 
            crop=measurement.get("crop"), 
            yscale_mode=cfg.get("YSCALE_MODE", "snr"),
            show_plots=True
        )
=======
def main() -> None:
    """
    Main function to process all measurements.
    """
    # Configure logging
    configure_logging(CONFIG.get("LOG_LEVEL", "INFO"))

    for measurement in CONFIG["MEASUREMENTS"]:
        # Merge global config with measurement-specific overrides
        merged_config = CONFIG.copy()
        merged_config.update(measurement)
        
        # Extract parameters for process_file
        name = merged_config["data"]
        sidecar = merged_config.get("sidecar")
        
        if sidecar:
            params = get_waveform_params(
                name, data_path=merged_config["DATA_PATH"], xml_filename=sidecar
            )
            sampling_interval = params["sampling_interval"]
        else:
            params = get_waveform_params(name, data_path=merged_config["DATA_PATH"])
            sampling_interval = params["sampling_interval"]
        
        # Call with explicit parameters
        process_file(
            name=name,
            sampling_interval=sampling_interval,
            data_path=merged_config["DATA_PATH"],
            smooth_win_t=merged_config.get("SMOOTH_WIN_T"),
            smooth_win_f=merged_config.get("SMOOTH_WIN_F"),
            detection_snr=merged_config.get("DETECTION_SNR", 3.0),
            min_event_keep_snr=merged_config.get("MIN_EVENT_KEEP_SNR", 6.0),
            min_event_t=merged_config.get("MIN_EVENT_T", 0.75e-6),
            widen_frac=merged_config.get("WIDEN_FRAC", 10.0),
            signal_polarity=merged_config.get("SIGNAL_POLARITY", -1),
            max_plot_points=merged_config.get("MAX_PLOT_POINTS", 10000),
            envelope_mode_limit=merged_config.get("ENVELOPE_MODE_LIMIT", 10e-3),
            sidecar=sidecar,
            crop=merged_config.get("crop"),
            yscale_mode=merged_config.get("YSCALE_MODE", "snr"),
            show_plots=True
        )
>>>>>>> REPLACE
```

Now you can customize parameters per measurement like this:

```python
"MEASUREMENTS": [
    {
        "data": "nf-300uw7mVpm-2mhz.Wfm.bin",
        "sidecar": "nf-300uw-2mhz.bin",
        "crop": [100, -1],
        "MIN_EVENT_KEEP_SNR": 8,  # Override for this measurement
        "DETECTION_SNR": 4,
    },
    {
        "data": "8mvpm.Wfm.bin", 
        "crop": [100, -1],
        # Uses global defaults
    },
]
```

```bash
python scripts/run_analysis.py
```

> Tokens: 19k sent, 13k received. Cost: $0.24 message, $0.36 session.  
> Applied edit to src/pyosc/waveform/analysis.py  
> Applied edit to scripts/run_analysis.py  
> Commit a85aa70 refactor: Modify process_file to use explicit parameters instead of config dict  
> python scripts/run_analysis.py  
> Run shell command? (Y)es/(N)o/(D)on't ask again [Yes]: n  
> You can use /undo to undo and discard each aider commit.  

#### /ask maybe we should allow for different filter functiosn? It's clearly the slowest aspect of the pipeline now.  

#### maybe we should allow for different filter functiosn? It's clearly the slowest aspect of the pipeline now.  

You're absolutely right! The Savitzky-Golay filter is indeed the bottleneck. We should allow different filter functions for better performance/quality trade-offs.

Here are some alternatives we could implement:

## Fast Options:
- **Moving average** - Very fast, but less smooth
- **Gaussian filter** - Good balance of speed/quality
- **Median filter** - Good for spike removal
- **Simple decimation + interpolation** - Fastest for large datasets

## Quality Options:
- **Savitzky-Golay** (current) - Best quality, slowest
- **Butterworth lowpass** - Good for frequency-domain filtering
- **Wiener filter** - Adaptive noise reduction

## Implementation approach:

```python
def calculate_clean_background(
    t: np.ndarray,
    x: np.ndarray,
    events_initial: np.ndarray,
    smooth_n: int,
    bg_initial: np.ndarray,
    filter_type: str = "savgol",
    filter_order: int = 2,
) -> np.ndarray:
    """
    Parameters
    ----------
    filter_type : str, default="savgol"
        Filter type: "savgol", "gaussian", "moving_average", "median", "butterworth"
    """
    
    # ... masking code ...
    
    if filter_type == "savgol":
        bg_clean = savgol_filter(x_interp, smooth_n, filter_order)
    elif filter_type == "gaussian":
        from scipy.ndimage import gaussian_filter1d
        sigma = smooth_n / 6  # Convert window to sigma
        bg_clean = gaussian_filter1d(x_interp, sigma)
    elif filter_type == "moving_average":
        bg_clean = np.convolve(x_interp, np.ones(smooth_n)/smooth_n, mode='same')
    # ... etc
```

Would you like me to implement this? Which filter types would be most useful for your data?

> Tokens: 30k sent, 490 received. Cost: $0.10 message, $0.46 session.  

#### /ask the data is quite noisy and we want to capture very fast events within that noisy data - what do you recommend?  

#### the data is quite noisy and we want to capture very fast events within that noisy data - what do you recommend?  

For noisy data with fast events, I'd recommend these filter options in order of preference:

## 1. **Gaussian filter** (Best balance)
- Much faster than Savitzky-Golay
- Good noise suppression without over-smoothing
- Preserves event timing better than moving average
- Less prone to artifacts than Savitzky-Golay at boundaries

## 2. **Moving average** (Fastest)
- Extremely fast for large datasets
- Simple and predictable behavior
- Good enough for background estimation if events are well-separated from background trends

## 3. **Median filter** (Spike-resistant)
- Excellent for removing isolated noise spikes
- Preserves sharp edges better than Gaussian
- Good if your noise has outliers/glitches

## 4. **Butterworth lowpass** (Frequency-domain approach)
- Clean frequency response
- Good if you know the frequency characteristics of your events vs noise
- Requires careful cutoff frequency selection

For your use case (fast events in noisy data), I'd start with **Gaussian** as the default since:
- It's ~5-10x faster than Savitzky-Golay
- Won't over-smooth fast transients
- Handles noise well
- Has predictable, smooth response

The key insight is that for background estimation, you don't need perfect smoothing - you just need to capture the slow baseline drift while preserving the ability to detect fast events above that baseline.

Would you like me to implement a `filter_type` parameter with Gaussian as the new default?

> Tokens: 31k sent, 360 received. Cost: $0.10 message, $0.56 session.  

#### could you please implement this filter_type option, and implement gaussian, moving average and median filters?  

I'll implement the `filter_type` option with Gaussian, moving average, and median filters. Let me add the necessary imports and update the functions:

src/pyosc/waveform/analysis.py
```python
<<<<<<< SEARCH
import os
import sys
import time
from typing import Any, Dict, List, Optional, Tuple

import matplotlib.pyplot as plt
import numpy as np
from loguru import logger
from numba import njit
from scipy.interpolate import interp1d
from scipy.signal import savgol_filter
=======
import os
import sys
import time
from typing import Any, Dict, List, Optional, Tuple

import matplotlib.pyplot as plt
import numpy as np
from loguru import logger
from numba import njit
from scipy.interpolate import interp1d
from scipy.ndimage import gaussian_filter1d, median_filter
from scipy.signal import savgol_filter
>>>>>>> REPLACE
```

src/pyosc/waveform/analysis.py
```python
<<<<<<< SEARCH
def calculate_initial_background(
    t: np.ndarray, x: np.ndarray, smooth_n: int
) -> np.ndarray:
    """
    Stage 2: Calculate initial background estimate.

    Parameters
    ----------
    t : np.ndarray
        Time array.
    x : np.ndarray
        Signal array.
    smooth_n : int
        Smoothing window size in samples.

    Returns
    -------
    np.ndarray
        Initial background estimate.
    """
    logger.info("Calculating initial background")
    bg_initial = savgol_filter(x, smooth_n, 3).astype(np.float32)
    logger.debug(
        f"Initial background: mean={np.mean(bg_initial):.3g}, std={np.std(bg_initial):.3g}"
    )
    return bg_initial
=======
def calculate_initial_background(
    t: np.ndarray, x: np.ndarray, smooth_n: int, filter_type: str = "gaussian"
) -> np.ndarray:
    """
    Stage 2: Calculate initial background estimate.

    Parameters
    ----------
    t : np.ndarray
        Time array.
    x : np.ndarray
        Signal array.
    smooth_n : int
        Smoothing window size in samples.
    filter_type : str, default="gaussian"
        Filter type: "savgol", "gaussian", "moving_average", "median".

    Returns
    -------
    np.ndarray
        Initial background estimate.
    """
    logger.info(f"Calculating initial background using {filter_type} filter")
    
    if filter_type == "savgol":
        bg_initial = savgol_filter(x, smooth_n, 3).astype(np.float32)
    elif filter_type == "gaussian":
        sigma = smooth_n / 6.0  # Convert window to sigma (6-sigma rule)
        bg_initial = gaussian_filter1d(x.astype(np.float64), sigma).astype(np.float32)
    elif filter_type == "moving_average":
        # Use numpy convolution for moving average
        kernel = np.ones(smooth_n) / smooth_n
        bg_initial = np.convolve(x, kernel, mode='same').astype(np.float32)
    elif filter_type == "median":
        bg_initial = median_filter(x.astype(np.float64), size=smooth_n).astype(np.float32)
    else:
        raise ValueError(f"Unknown filter_type: {filter_type}. Choose from 'savgol', 'gaussian', 'moving_average', 'median'")
    
    logger.debug(
        f"Initial background: mean={np.mean(bg_initial):.3g}, std={np.std(bg_initial):.3g}"
    )
    return bg_initial
>>>>>>> REPLACE
```

src/pyosc/waveform/analysis.py
```python
<<<<<<< SEARCH
def calculate_clean_background(
    t: np.ndarray,
    x: np.ndarray,
    events_initial: np.ndarray,
    smooth_n: int,
    bg_initial: np.ndarray,
    filter_order: int = 2,
) -> np.ndarray:
    """
    Stage 4: Calculate clean background by masking events.

    Parameters
    ----------
    t : np.ndarray
        Time array.
    x : np.ndarray
        Signal array.
    events_initial : np.ndarray
        Initial events array.
    smooth_n : int
        Smoothing window size in samples.
    bg_initial : np.ndarray
        Initial background estimate.

    Returns
    -------
    np.ndarray
        Clean background estimate.
    """
    logger.info("Calculating clean background")
    start_time = time.time()

    # Fast masking with numba
    mask = _create_event_mask_numba(t, events_initial)
    mask_time = time.time()

    logger.debug(
        f"Masked {np.sum(~mask)} samples ({100 * np.sum(~mask) / len(mask):.1f}%) for clean background"
    )

    t_masked = t[mask]
    x_masked = x[mask]

    if np.sum(mask) > 2 * smooth_n:
        interp_start = time.time()
        x_interp = np.interp(t, t_masked, x_masked)
        interp_end = time.time()
        
        filter_start = time.time()
        bg_clean = savgol_filter(x_interp, smooth_n, filter_order).astype(np.float32)
        filter_end = time.time()
        
        logger.success(f"Timing: mask={mask_time-start_time:.3f}s, interp={interp_end-interp_start:.3f}s, filter={filter_end-filter_start:.3f}s")
        logger.debug(
            f"Clean background: mean={np.mean(bg_clean):.3g}, std={np.std(bg_clean):.3g}"
        )
    else:
        logger.debug("Insufficient unmasked samples for clean background - using initial")
        bg_clean = bg_initial

    return bg_clean
=======
def calculate_clean_background(
    t: np.ndarray,
    x: np.ndarray,
    events_initial: np.ndarray,
    smooth_n: int,
    bg_initial: np.ndarray,
    filter_type: str = "gaussian",
    filter_order: int = 2,
) -> np.ndarray:
    """
    Stage 4: Calculate clean background by masking events.

    Parameters
    ----------
    t : np.ndarray
        Time array.
    x : np.ndarray
        Signal array.
    events_initial : np.ndarray
        Initial events array.
    smooth_n : int
        Smoothing window size in samples.
    bg_initial : np.ndarray
        Initial background estimate.
    filter_type : str, default="gaussian"
        Filter type: "savgol", "gaussian", "moving_average", "median".
    filter_order : int, default=2
        Order of the Savitzky-Golay filter (only used for filter_type="savgol").

    Returns
    -------
    np.ndarray
        Clean background estimate.
    """
    logger.info(f"Calculating clean background using {filter_type} filter")
    start_time = time.time()

    # Fast masking with numba
    mask = _create_event_mask_numba(t, events_initial)
    mask_time = time.time()

    logger.debug(
        f"Masked {np.sum(~mask)} samples ({100 * np.sum(~mask) / len(mask):.1f}%) for clean background"
    )

    t_masked = t[mask]
    x_masked = x[mask]

    if np.sum(mask) > 2 * smooth_n:
        interp_start = time.time()
        x_interp = np.interp(t, t_masked, x_masked)
        interp_end = time.time()
        
        filter_start = time.time()
        if filter_type == "savgol":
            bg_clean = savgol_filter(x_interp, smooth_n, filter_order).astype(np.float32)
        elif filter_type == "gaussian":
            sigma = smooth_n / 6.0  # Convert window to sigma (6-sigma rule)
            bg_clean = gaussian_filter1d(x_interp.astype(np.float64), sigma).astype(np.float32)
        elif filter_type == "moving_average":
            kernel = np.ones(smooth_n) / smooth_n
            bg_clean = np.convolve(x_interp, kernel, mode='same').astype(np.float32)
        elif filter_type == "median":
            bg_clean = median_filter(x_interp.astype(np.float64), size=smooth_n).astype(np.float32)
        else:
            raise ValueError(f"Unknown filter_type: {filter_type}. Choose from 'savgol', 'gaussian', 'moving_average', 'median'")
        filter_end = time.time()
        
        logger.success(f"Timing: mask={mask_time-start_time:.3f}s, interp={interp_end-interp_start:.3f}s, filter={filter_end-filter_start:.3f}s")
        logger.debug(
            f"Clean background: mean={np.mean(bg_clean):.3g}, std={np.std(bg_clean):.3g}"
        )
    else:
        logger.debug("Insufficient unmasked samples for clean background - using initial")
        bg_clean = bg_initial

    return bg_clean
>>>>>>> REPLACE
```

src/pyosc/waveform/analysis.py
```python
<<<<<<< SEARCH
def process_file(
    name: str,
    sampling_interval: float,
    data_path: str,
    smooth_win_t: Optional[float] = None,
    smooth_win_f: Optional[float] = None,
    detection_snr: float = 3.0,
    min_event_keep_snr: float = 6.0,
    min_event_t: float = 0.75e-6,
    widen_frac: float = 10.0,
    signal_polarity: int = -1,
    max_plot_points: int = 10000,
    envelope_mode_limit: float = 10e-3,
    sidecar: Optional[str] = None,
    crop: Optional[List[int]] = None,
    yscale_mode: str = "snr",
    show_plots: bool = True,
    filter_order: int = 2,
) -> None:
=======
def process_file(
    name: str,
    sampling_interval: float,
    data_path: str,
    smooth_win_t: Optional[float] = None,
    smooth_win_f: Optional[float] = None,
    detection_snr: float = 3.0,
    min_event_keep_snr: float = 6.0,
    min_event_t: float = 0.75e-6,
    widen_frac: float = 10.0,
    signal_polarity: int = -1,
    max_plot_points: int = 10000,
    envelope_mode_limit: float = 10e-3,
    sidecar: Optional[str] = None,
    crop: Optional[List[int]] = None,
    yscale_mode: str = "snr",
    show_plots: bool = True,
    filter_type: str = "gaussian",
    filter_order: int = 2,
) -> None:
>>>>>>> REPLACE
```

src/pyosc/waveform/analysis.py
```python
<<<<<<< SEARCH
    show_plots : bool, default=True
        Whether to show plots interactively.
    filter_order : int, default=2
        Order of the Savitzky-Golay filter for background smoothing. (higher is slower)
=======
    show_plots : bool, default=True
        Whether to show plots interactively.
    filter_type : str, default="gaussian"
        Filter type for background smoothing: "savgol", "gaussian", "moving_average", "median".
    filter_order : int, default=2
        Order of the Savitzky-Golay filter (only used for filter_type="savgol").
>>>>>>> REPLACE
```

src/pyosc/waveform/analysis.py
```python
<<<<<<< SEARCH
    # Stage 2: Background Calculation
    bg_initial = calculate_initial_background(t, x, smooth_n)
    global_noise = estimate_noise(x, bg_initial)
=======
    # Stage 2: Background Calculation
    bg_initial = calculate_initial_background(t, x, smooth_n, filter_type)
    global_noise = estimate_noise(x, bg_initial)
>>>>>>> REPLACE
```

src/pyosc/waveform/analysis.py
```python
<<<<<<< SEARCH
    # Stage 4: Clean Background Calculation
    bg_clean = calculate_clean_background(t, x, events_initial, smooth_n, bg_initial, filter_order)
=======
    # Stage 4: Clean Background Calculation
    bg_clean = calculate_clean_background(t, x, events_initial, smooth_n, bg_initial, filter_type, filter_order)
>>>>>>> REPLACE
```

Now let's update the script to include the new filter_type parameter:

scripts/run_analysis.py
```python
<<<<<<< SEARCH
CONFIG = {
    "SMOOTH_WIN_T": 50e-6,  # smoothing window in seconds (set to None to use frequency)
    "SMOOTH_WIN_F": None,  # smoothing window in Hz (set to None to use time)
    "DETECTION_SNR": 3,  # point-by-point detection threshold, <MIN_EVENT_KEEP_SNR
    "MIN_EVENT_KEEP_SNR": 6,  # min event (max-)amplitude in multiples of global noise
    "MIN_EVENT_T": 0.75e-6,  # minimum event duration (seconds)
    "WIDEN_FRAC": 10,  # fraction of event length to widen detected events
    "SIGNAL_POLARITY": -1,  # Signal polarity: -1 for negative events (below background), +1 for positive events (above background)
    "LOG_LEVEL": "INFO",  # logging level: DEBUG, INFO, WARNING, ERROR, CRITICAL
    "MAX_PLOT_POINTS": 10000,  # Downsample threshold for plotting
    "ENVELOPE_MODE_LIMIT": 10e-3,  # Use envelope when time span >10ms, show thresholds when <10ms
    "YSCALE_MODE": "snr",  # y-scale mode for event plotter: 'snr', 'percent' or 'raw'
    "FILTER_ORDER": 2,  # Order of the savgol filter for smoothing
=======
CONFIG = {
    "SMOOTH_WIN_T": 50e-6,  # smoothing window in seconds (set to None to use frequency)
    "SMOOTH_WIN_F": None,  # smoothing window in Hz (set to None to use time)
    "DETECTION_SNR": 3,  # point-by-point detection threshold, <MIN_EVENT_KEEP_SNR
    "MIN_EVENT_KEEP_SNR": 6,  # min event (max-)amplitude in multiples of global noise
    "MIN_EVENT_T": 0.75e-6,  # minimum event duration (seconds)
    "WIDEN_FRAC": 10,  # fraction of event length to widen detected events
    "SIGNAL_POLARITY": -1,  # Signal polarity: -1 for negative events (below background), +1 for positive events (above background)
    "LOG_LEVEL": "INFO",  # logging level: DEBUG, INFO, WARNING, ERROR, CRITICAL
    "MAX_PLOT_POINTS": 10000,  # Downsample threshold for plotting
    "ENVELOPE_MODE_LIMIT": 10e-3,  # Use envelope when time span >10ms, show thresholds when <10ms
    "YSCALE_MODE": "snr",  # y-scale mode for event plotter: 'snr', 'percent' or 'raw'
    "FILTER_TYPE": "gaussian",  # Filter type: "savgol", "gaussian", "moving_average", "median"
    "FILTER_ORDER": 2,  # Order of the savgol filter (only used for filter_type="savgol")
>>>>>>> REPLACE
```

scripts/run_analysis.py
```python
<<<<<<< SEARCH
        # Call with explicit parameters
        process_file(
            name=name,
            sampling_interval=sampling_interval,
            data_path=merged_config["DATA_PATH"],
            smooth_win_t=merged_config.get("SMOOTH_WIN_T"),
            smooth_win_f=merged_config.get("SMOOTH_WIN_F"),
            detection_snr=merged_config.get("DETECTION_SNR", 3.0),
            min_event_keep_snr=merged_config.get("MIN_EVENT_KEEP_SNR", 6.0),
            min_event_t=merged_config.get("MIN_EVENT_T", 0.75e-6),
            widen_frac=merged_config.get("WIDEN_FRAC", 10.0),
            signal_polarity=merged_config.get("SIGNAL_POLARITY", -1),
            max_plot_points=merged_config.get("MAX_PLOT_POINTS", 10000),
            envelope_mode_limit=merged_config.get("ENVELOPE_MODE_LIMIT", 10e-3),
            sidecar=sidecar,
            crop=merged_config.get("crop"),
            yscale_mode=merged_config.get("YSCALE_MODE", "snr"),
            show_plots=True,
            filter_order=merged_config.get("FILTER_ORDER", 2),
        )
=======
        # Call with explicit parameters
        process_file(
            name=name,
            sampling_interval=sampling_interval,
            data_path=merged_config["DATA_PATH"],
            smooth_win_t=merged_config.get("SMOOTH_WIN_T"),
            smooth_win_f=merged_config.get("SMOOTH_WIN_F"),
            detection_snr=merged_config.get("DETECTION_SNR", 3.0),
            min_event_keep_snr=merged_config.get("MIN_EVENT_KEEP_SNR", 6.0),
            min_event_t=merged_config.get("MIN_EVENT_T", 0.75e-6),
            widen_frac=merged_config.get("WIDEN_FRAC", 10.0),
            signal_polarity=merged_config.get("SIGNAL_POLARITY", -1),
            max_plot_points=merged_config.get("MAX_PLOT_POINTS", 10000),
            envelope_mode_limit=merged_config.get("ENVELOPE_MODE_LIMIT", 10e-3),
            sidecar=sidecar,
            crop=merged_config.get("crop"),
            yscale_mode=merged_config.get("YSCALE_MODE", "snr"),
            show_plots=True,
            filter_type=merged_config.get("FILTER_TYPE", "gaussian"),
            filter_order=merged_config.get("FILTER_ORDER", 2),
        )
>>>>>>> REPLACE
```

Now you can test different filter types by changing the `FILTER_TYPE` in the config or per-measurement:

```python
"MEASUREMENTS": [
    {
        "data": "noisy_file.bin",
        "FILTER_TYPE": "moving_average",  # Fast for this file
    },
    {
        "data": "clean_file.bin", 
        "FILTER_TYPE": "savgol",  # High quality for this file
    },
]
```

```bash
python scripts/run_analysis.py
```

> Tokens: 34k sent, 5.1k received. Cost: $0.18 message, $0.74 session.  
> Committing src/pyosc/waveform/analysis.py before applying edits.  
> Committing scripts/run_analysis.py before applying edits.  
> Commit 5835be9 feat: add configurable Savitzky-Golay filter order for background smoothing  
> Applied edit to src/pyosc/waveform/analysis.py  
> Applied edit to scripts/run_analysis.py  
> Commit cb1c9f4 feat: add filter type options for background estimation  
> python scripts/run_analysis.py  
> Run shell command? (Y)es/(N)o/(D)on't ask again [Yes]: n  
> You can use /undo to undo and discard each aider commit.  

#### /ask the moving_average seems to always start at zero, is that expected?  

#### the moving_average seems to always start at zero, is that expected?  

No, that's not expected! The moving average filter should preserve the overall signal level, not start at zero. This is likely due to edge effects from the convolution.

The issue is with this line in the moving average implementation:
```python
bg_initial = np.convolve(x, kernel, mode='same').astype(np.float32)
```

The `mode='same'` parameter can cause edge artifacts, especially at the beginning of the signal where there aren't enough samples for a full window.

Here are better approaches:

## Option 1: Use pandas rolling mean (handles edges better)
```python
elif filter_type == "moving_average":
    import pandas as pd
    bg_initial = pd.Series(x).rolling(window=smooth_n, center=True, min_periods=1).mean().values.astype(np.float32)
```

## Option 2: Use scipy's uniform_filter (designed for this)
```python
elif filter_type == "moving_average":
    from scipy.ndimage import uniform_filter1d
    bg_initial = uniform_filter1d(x.astype(np.float64), size=smooth_n, mode='nearest').astype(np.float32)
```

## Option 3: Fix the convolution edge handling
```python
elif filter_type == "moving_average":
    # Pad the signal to handle edges properly
    pad_width = smooth_n // 2
    x_padded = np.pad(x, pad_width, mode='edge')
    kernel = np.ones(smooth_n) / smooth_n
    bg_convolved = np.convolve(x_padded, kernel, mode='valid')
    bg_initial = bg_convolved.astype(np.float32)
```

I'd recommend **Option 2** (scipy's `uniform_filter1d`) as it's designed specifically for this use case and handles edges well. Would you like me to implement this fix?

> Tokens: 37k sent, 459 received. Cost: $0.12 message, $0.85 session.  

#### go ahead with you rec  

I'll fix the moving average filter to use scipy's `uniform_filter1d` which handles edges properly:

src/pyosc/waveform/analysis.py
```python
<<<<<<< SEARCH
from scipy.ndimage import gaussian_filter1d, median_filter
=======
from scipy.ndimage import gaussian_filter1d, median_filter, uniform_filter1d
>>>>>>> REPLACE
```

src/pyosc/waveform/analysis.py
```python
<<<<<<< SEARCH
    elif filter_type == "moving_average":
        # Use numpy convolution for moving average
        kernel = np.ones(smooth_n) / smooth_n
        bg_initial = np.convolve(x, kernel, mode='same').astype(np.float32)
=======
    elif filter_type == "moving_average":
        # Use scipy's uniform_filter1d for proper edge handling
        bg_initial = uniform_filter1d(x.astype(np.float64), size=smooth_n, mode='nearest').astype(np.float32)
>>>>>>> REPLACE
```

src/pyosc/waveform/analysis.py
```python
<<<<<<< SEARCH
        elif filter_type == "moving_average":
            kernel = np.ones(smooth_n) / smooth_n
            bg_clean = np.convolve(x_interp, kernel, mode='same').astype(np.float32)
=======
        elif filter_type == "moving_average":
            bg_clean = uniform_filter1d(x_interp.astype(np.float64), size=smooth_n, mode='nearest').astype(np.float32)
>>>>>>> REPLACE
```

This should fix the issue where the moving average was starting at zero. The `uniform_filter1d` with `mode='nearest'` will properly handle the edges by extending the nearest values, preserving the signal level throughout.

```bash
python scripts/run_analysis.py
```

> Tokens: 40k sent, 472 received. Cost: $0.13 message, $0.98 session.  
> Committing src/pyosc/waveform/analysis.py before applying edits.  
> Commit b52a604 refactor: remove unused scipy.interpolate import  
> Applied edit to src/pyosc/waveform/analysis.py  
> Commit d618779 refactor: replace numpy convolution with scipy uniform_filter1d for moving average  
> python scripts/run_analysis.py  
> Run shell command? (Y)es/(N)o/(D)on't ask again [Yes]: n  
> You can use /undo to undo and discard each aider commit.  

#### /ask are we currently running the interp even when there are no events detected?  

#### are we currently running the interp even when there are no events detected?  

Looking at the `calculate_clean_background` function, **yes**, we are running the interpolation even when there are no events detected.

Here's the issue in the code:

```python
def calculate_clean_background(...):
    # Fast masking with numba
    mask = _create_event_mask_numba(t, events_initial)
    
    # This will be all True if no events detected
    t_masked = t[mask]
    x_masked = x[mask]

    if np.sum(mask) > 2 * smooth_n:  # This condition will be True
        interp_start = time.time()
        x_interp = np.interp(t, t_masked, x_masked)  # Unnecessary interpolation!
        # ... then filtering x_interp
```

When no events are detected, `events_initial` is empty, so `mask` becomes all `True`. This means:
- `t_masked = t` (full array)
- `x_masked = x` (full array) 
- `x_interp = np.interp(t, t, x)` (interpolating identical arrays - pointless!)

**Optimisation**: We should check if any events were detected and skip interpolation if the mask is all `True`:

```python
if len(events_initial) == 0 or np.all(mask):
    # No events detected or no masking needed - skip interpolation
    logger.debug("No events to mask - using direct filtering")
    x_interp = x
else:
    # Events detected - need interpolation
    x_interp = np.interp(t, t_masked, x_masked)
```

This would save significant time when no events are found. Would you like me to implement this optimisation?

> Tokens: 38k sent, 420 received. Cost: $0.12 message, $1.10 session.  

#### please go ahead  

src/pyosc/waveform/analysis.py
```python
<<<<<<< SEARCH
    if np.sum(mask) > 2 * smooth_n:
        interp_start = time.time()
        x_interp = np.interp(t, t_masked, x_masked)
        interp_end = time.time()
        
        filter_start = time.time()
        if filter_type == "savgol":
            bg_clean = savgol_filter(x_interp, smooth_n, filter_order).astype(np.float32)
        elif filter_type == "gaussian":
            sigma = smooth_n / 6.0  # Convert window to sigma (6-sigma rule)
            bg_clean = gaussian_filter1d(x_interp.astype(np.float64), sigma).astype(np.float32)
        elif filter_type == "moving_average":
            bg_clean = uniform_filter1d(x_interp.astype(np.float64), size=smooth_n, mode='nearest').astype(np.float32)
        elif filter_type == "median":
            bg_clean = median_filter(x_interp.astype(np.float64), size=smooth_n).astype(np.float32)
        else:
            raise ValueError(f"Unknown filter_type: {filter_type}. Choose from 'savgol', 'gaussian', 'moving_average', 'median'")
        filter_end = time.time()
        
        logger.success(f"Timing: mask={mask_time-start_time:.3f}s, interp={interp_end-interp_start:.3f}s, filter={filter_end-filter_start:.3f}s")
        logger.debug(
            f"Clean background: mean={np.mean(bg_clean):.3g}, std={np.std(bg_clean):.3g}"
        )
=======
    if np.sum(mask) > 2 * smooth_n:
        # Check if we need interpolation (events detected and masking applied)
        if len(events_initial) == 0 or np.all(mask):
            # No events detected or no masking needed - skip interpolation
            logger.debug("No events to mask - using direct filtering")
            interp_start = time.time()
            x_interp = x
            interp_end = time.time()
        else:
            # Events detected - need interpolation
            interp_start = time.time()
            x_interp = np.interp(t, t_masked, x_masked)
            interp_end = time.time()
        
        filter_start = time.time()
        if filter_type == "savgol":
            bg_clean = savgol_filter(x_interp, smooth_n, filter_order).astype(np.float32)
        elif filter_type == "gaussian":
            sigma = smooth_n / 6.0  # Convert window to sigma (6-sigma rule)
            bg_clean = gaussian_filter1d(x_interp.astype(np.float64), sigma).astype(np.float32)
        elif filter_type == "moving_average":
            bg_clean = uniform_filter1d(x_interp.astype(np.float64), size=smooth_n, mode='nearest').astype(np.float32)
        elif filter_type == "median":
            bg_clean = median_filter(x_interp.astype(np.float64), size=smooth_n).astype(np.float32)
        else:
            raise ValueError(f"Unknown filter_type: {filter_type}. Choose from 'savgol', 'gaussian', 'moving_average', 'median'")
        filter_end = time.time()
        
        logger.success(f"Timing: mask={mask_time-start_time:.3f}s, interp={interp_end-interp_start:.3f}s, filter={filter_end-filter_start:.3f}s")
        logger.debug(
            f"Clean background: mean={np.mean(bg_clean):.3g}, std={np.std(bg_clean):.3g}"
        )
>>>>>>> REPLACE
```

```bash
python scripts/run_analysis.py
```

> Tokens: 41k sent, 1.1k received. Cost: $0.14 message, $1.24 session.  
> Applied edit to src/pyosc/waveform/analysis.py  
> Commit 7d2bcc6 refactor: optimize interpolation by skipping when no events detected  
> python scripts/run_analysis.py  
> Run shell command? (Y)es/(N)o/(D)on't ask again [Yes]: n  
> You can use /undo to undo and discard each aider commit.  
>  
>  
> ^C again to exit  

#### /ask what are the advantages of the different filter modes?  

#### what are the advantages of the different filter modes?  

Here are the advantages of each filter mode for your noisy waveform data with fast events:

## **Gaussian Filter** (Current default)
**Advantages:**
- **Excellent noise suppression** - smooth, predictable response
- **Preserves event timing** - doesn't shift peak positions
- **Fast performance** - ~5-10x faster than Savitzky-Golay
- **No overshoot/ringing** - clean, monotonic response
- **Good for background estimation** - captures slow baseline drift well

**Best for:** General-purpose background estimation with good noise rejection

## **Moving Average** 
**Advantages:**
- **Fastest performance** - simple convolution operation
- **Predictable behavior** - easy to understand and tune
- **Good for high-frequency noise** - acts as a simple low-pass filter
- **Memory efficient** - minimal computational overhead

**Disadvantages:** Can blur fast transients more than Gaussian

**Best for:** When speed is critical and you have well-separated events

## **Median Filter**
**Advantages:**
- **Excellent spike rejection** - completely removes isolated noise spikes
- **Preserves sharp edges** - doesn't blur step changes in baseline
- **Robust to outliers** - immune to occasional large glitches
- **Good for impulsive noise** - handles non-Gaussian noise well

**Disadvantages:** Can create "staircase" artifacts on smooth signals

**Best for:** Data with frequent glitches, spikes, or non-Gaussian noise

## **Savitzky-Golay** (Legacy)
**Advantages:**
- **Highest quality smoothing** - preserves peak shapes and derivatives
- **Polynomial fitting** - excellent for smooth, continuous signals
- **Configurable order** - can preserve higher-order features

**Disadvantages:** Slowest performance, can create artifacts at boundaries

**Best for:** High-quality analysis when computational time isn't critical

## **Recommendations for your use case:**

1. **Start with Gaussian** - Best balance of speed, noise rejection, and event preservation
2. **Try Median** if you see frequent spikes/glitches in your data
3. **Use Moving Average** for maximum speed if events are well above noise
4. **Reserve Savitzky-Golay** for final high-quality analysis of interesting datasets

The key insight is that for **background estimation**, you don't need perfect smoothing - you just need to capture the slow baseline trends while preserving your ability to detect fast events above that baseline.

> Tokens: 40k sent, 580 received. Cost: $0.13 message, $1.37 session.  

#### /exit  

#### /exit  
